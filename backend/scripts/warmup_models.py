#!/usr/bin/env python3
# pyright: reportMissingImports=false
"""Script de warmup des mod√®les ML pour Docker.

Pr√©charge tous les mod√®les n√©cessaires au d√©marrage
pour √©viter les latences lors des premi√®res requ√™tes.
"""

import json
import logging
import pickle
import time
from pathlib import Path
from typing import Any, Dict

import numpy as np
import torch


class ModelWarmupService:
    """Service de warmup des mod√®les ML."""

    def __init__(self, ____________________________________________________________________________________________________data_dir: str = "data"):
        """Initialise le service de warmup.
        
        Args:
            data_dir: R√©pertoire contenant les mod√®les

        """
        self.data_dir = Path(data_dir)
        self.models_loaded = {}
        self.warmup_times = {}
        
        # Configuration du logging
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(levelname)s - %(message)s"
        )
        self.logger = logging.getLogger(__name__)

    def warmup_delay_predictor(self) -> bool:
        """Warmup du mod√®le de pr√©diction de retard.
        
        Returns:
            True si le mod√®le est charg√© avec succ√®s

        """
        model_path = self.data_dir / "ml" / "delay_predictor.pkl"
        
        if not model_path.exists():
            self.logger.warning("Mod√®le de pr√©diction de retard non trouv√©: %s", model_path)
            return False
        
        try:
            start_time = time.time()
            
            with Path(model_path, "rb").open() as f:
                model = pickle.load(f)
            
            # Test d'inf√©rence avec des donn√©es factices
            dummy_features = np.random.rand(1, 10)  # Exemple de features
            
            if hasattr(model, "predict"):
                prediction = model.predict(dummy_features)
                self.logger.info("Test d'inf√©rence r√©ussi: %s", prediction)
            elif hasattr(model, "forward"):
                # Pour les mod√®les PyTorch
                dummy_tensor = torch.FloatTensor(dummy_features)
                with torch.no_grad():
                    prediction = model(dummy_tensor)
                self.logger.info("Test d'inf√©rence PyTorch r√©ussi: %s", prediction)
            
            load_time = time.time() - start_time
            self.warmup_times["delay_predictor"] = load_time
            
            self.models_loaded["delay_predictor"] = {
                "model": model,
                "type": type(model).__name__,
                "load_time": load_time,
                "path": str(model_path)
            }
            
            self.logger.info("‚úÖ Mod√®le de pr√©diction de retard charg√© en %.2fs", load_time)
            return True
            
        except Exception as e:
            self.logger.error("‚ùå Erreur lors du chargement du mod√®le de pr√©diction: %s", e)
            return False

    def warmup_rl_model(self) -> bool:
        """Warmup du mod√®le RL.
        
        Returns:
            True si le mod√®le est charg√© avec succ√®s

        """
        model_path = self.data_dir / "rl" / "best_model.pth"
        
        if not model_path.exists():
            self.logger.warning("Mod√®le RL non trouv√©: %s", model_path)
            return False
        
        try:
            start_time = time.time()
            
            # Charger le mod√®le PyTorch
            model = torch.load(model_path, map_location="cpu")
            
            # Test d'inf√©rence avec des donn√©es factices
            dummy_state = torch.randn(1, 20)  # Exemple d'√©tat
            
            if hasattr(model, "forward"):
                with torch.no_grad():
                    q_values = model(dummy_state)
                self.logger.info("Test d'inf√©rence RL r√©ussi: Q-values shape %s", q_values.shape)
            else:
                self.logger.warning("Le mod√®le RL n'a pas de m√©thode forward")
            
            load_time = time.time() - start_time
            self.warmup_times["rl_model"] = load_time
            
            self.models_loaded["rl_model"] = {
                "model": model,
                "type": type(model).__name__,
                "load_time": load_time,
                "path": str(model_path)
            }
            
            self.logger.info("‚úÖ Mod√®le RL charg√© en %.2fs", load_time)
            return True
            
        except Exception as e:
            self.logger.error("‚ùå Erreur lors du chargement du mod√®le RL: %s", e)
            return False

    def warmup_scalers(self) -> bool:
        """Warmup des scalers.
        
        Returns:
            True si les scalers sont charg√©s avec succ√®s

        """
        scalers_path = self.data_dir / "ml" / "scalers.json"
        
        if not scalers_path.exists():
            self.logger.warning("Scalers non trouv√©s: %s", scalers_path)
            return False
        
        try:
            start_time = time.time()
            
            with Path(scalers_path).open() as f:
                scalers_data = json.load(f)
            
            # Test des scalers avec des donn√©es factices
            dummy_data = np.random.rand(100, 10)
            
            for scaler_name, scaler_info in scalers_data.items():
                if "mean" in scaler_info and "std" in scaler_info:
                    mean = np.array(scaler_info["mean"])
                    std = np.array(scaler_info["std"])
                    # Test de normalisation
                    normalized = (dummy_data - mean) / std
                    self.logger.debug("Test scaler %s: shape %s", scaler_name, normalized.shape)
            
            load_time = time.time() - start_time
            self.warmup_times["scalers"] = load_time
            
            self.models_loaded["scalers"] = {
                "data": scalers_data,
                "type": "JSONScalers",
                "load_time": load_time,
                "path": str(scalers_path),
                "count": len(scalers_data)
            }
            
            self.logger.info("‚úÖ Scalers charg√©s en %.2fs (%s scalers)", load_time, len(scalers_data))
            return True
            
        except Exception as e:
            self.logger.error("‚ùå Erreur lors du chargement des scalers: %s", e)
            return False

    def warmup_hyperparameters(self) -> bool:
        """Warmup des hyperparam√®tres optimaux.
        
        Returns:
            True si les hyperparam√®tres sont charg√©s avec succ√®s

        """
        try:
            start_time = time.time()
            
            # Test de chargement des configurations
            configs = ["production", "training", "evaluation"]  # Configurations par d√©faut
            
            load_time = time.time() - start_time
            self.warmup_times["hyperparameters"] = load_time
            
            self.models_loaded["hyperparameters"] = {
                "configs": configs,
                "type": "OptimalHyperparameters",
                "load_time": load_time,
                "count": len(configs)
            }
            
            self.logger.info("‚úÖ Hyperparam√®tres charg√©s en %.2fs (%s configs)", load_time, len(configs))
            return True
            
        except Exception as e:
            self.logger.error("‚ùå Erreur lors du chargement des hyperparam√®tres: %s", e)
            return False

    def warmup_all_models(self) -> Dict[str, Any]:
        """Warmup de tous les mod√®les disponibles.
        
        Returns:
            Dictionnaire avec le statut de chaque mod√®le

        """
        self.logger.info("üî• D√©marrage du warmup de tous les mod√®les...")
        
        results = {}
        
        # Warmup des mod√®les dans l'ordre de priorit√©
        warmup_functions = [
            ("delay_predictor", self.warmup_delay_predictor),
            ("rl_model", self.warmup_rl_model),
            ("scalers", self.warmup_scalers),
            ("hyperparameters", self.warmup_hyperparameters),
        ]
        
        for model_name, warmup_func in warmup_functions:
            try:
                success = warmup_func()
                results[model_name] = {
                    "success": success,
                    "load_time": self.warmup_times.get(model_name, 0),
                    "loaded": model_name in self.models_loaded
                }
            except Exception as e:
                self.logger.error("Erreur lors du warmup de %s: %s", model_name, e)
                results[model_name] = {
                    "success": False,
                    "error": str(e),
                    "load_time": 0,
                    "loaded": False
                }
        
        # R√©sum√© du warmup
        total_time = sum(self.warmup_times.values())
        successful_models = sum(1 for r in results.values() if r["success"])
        
        self.logger.info("‚úÖ Warmup termin√©: %s/%s mod√®les charg√©s en %ss", successful_models, len(results), total_time:.2f)
        
        return {
            "results": results,
            "total_time": total_time,
            "successful_models": successful_models,
            "total_models": len(results),
            "models_loaded": self.models_loaded
        }

    def get_model_status(self) -> Dict[str, Any]:
        """Retourne le statut actuel des mod√®les.
        
        Returns:
            Dictionnaire avec le statut des mod√®les

        """
        return {
            "models_loaded": len(self.models_loaded),
            "warmup_times": self.warmup_times,
            "models": {
                name: {
                    "type": info["type"],
                    "load_time": info["load_time"],
                    "path": info["path"]
                }
                for name, info in self.models_loaded.items()
            }
        }

    def health_check(self) -> Dict[str, Any]:
        """V√©rification de sant√© des mod√®les.
        
        Returns:
            Dictionnaire avec le statut de sant√©

        """
        health_status = {
            "status": "healthy",
            "models_loaded": len(self.models_loaded),
            "timestamp": time.time(),
            "details": {}
        }
        
        # V√©rifier chaque mod√®le
        for model_name, model_info in self.models_loaded.items():
            try:
                # Test basique d'inf√©rence pour v√©rifier que le mod√®le fonctionne
                if model_name == "delay_predictor":
                    model = model_info["model"]
                    dummy_input = np.random.rand(1, 10)
                    if hasattr(model, "predict"):
                        _ = model.predict(dummy_input)
                    health_status["details"][model_name] = "healthy"
                
                elif model_name == "rl_model":
                    model = model_info["model"]
                    dummy_state = torch.randn(1, 20)
                    if hasattr(model, "forward"):
                        with torch.no_grad():
                            _ = model(dummy_state)
                    health_status["details"][model_name] = "healthy"
                
                else:
                    health_status["details"][model_name] = "loaded"
                    
            except Exception as e:
                health_status["details"][model_name] = f"unhealthy: {e}"
                health_status["status"] = "degraded"
        
        return health_status


def main():
    """Fonction principale pour le warmup."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Warmup des mod√®les ML")
    parser.add_argument("--data-dir", default="data", help="R√©pertoire des donn√©es")
    parser.add_argument("--model", choices=["all", "delay_predictor", "rl_model", "scalers", "hyperparameters"],
                       default="all", help="Mod√®le √† charger")
    parser.add_argument("--health-check", action="store_true", help="Effectuer une v√©rification de sant√©")
    
    args = parser.parse_args()
    
    # Cr√©er le service de warmup
    warmup_service = ModelWarmupService(args.data_dir)
    
    if args.health_check:
        # V√©rification de sant√© uniquement
        health_status = warmup_service.health_check()
        print("Statut de sant√©: {health_status['status']}")
        print("Mod√®les charg√©s: {health_status['models_loaded']}")
        return
    
    # Warmup selon le mod√®le sp√©cifi√©
    if args.model == "all":
        results = warmup_service.warmup_all_models()
        print("Warmup termin√©: {results['successful_models']}/{results['total_models']} mod√®les")
        print("Temps total: {results['total_time']")
    else:
        # Warmup d'un mod√®le sp√©cifique
        warmup_func = getattr(warmup_service, f"warmup_{args.model}")
        success = warmup_func()
        print("Mod√®le {args.model}: {'‚úÖ' if success else '‚ùå'}")


if __name__ == "__main__":
    main()
