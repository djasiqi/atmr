#!/usr/bin/env python3
# pyright: reportMissingImports=false
"""Script de dÃ©ploiement pour l'Ã‰tape 10 - Couverture de tests â‰¥ 70%.

Ce script orchestre le dÃ©ploiement de tous les tests crÃ©Ã©s
et valide que la couverture de tests atteint l'objectif.
"""

import json
import subprocess
import sys
from datetime import UTC, datetime
from pathlib import Path

# Ajouter le rÃ©pertoire backend au path Python
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

def deploy_test_files():
    """DÃ©ploie tous les fichiers de test."""
    print("ğŸ“¦ DÃ©ploiement des fichiers de test")
    
    test_files = [
        "tests/rl/test_per_comprehensive.py",
        "tests/rl/test_action_masking_comprehensive.py",
        "tests/rl/test_reward_shaping_comprehensive.py",
        "tests/rl/test_integration_comprehensive.py",
        "tests/test_alerts_comprehensive.py",
        "tests/test_shadow_mode_comprehensive.py",
        "tests/test_docker_production_comprehensive.py"
    ]
    
    deployed_files = []
    failed_files = []
    
    for test_file in test_files:
        file_path = Path(backend_dir) / test_file
        if file_path.exists():
            deployed_files.append(test_file)
            print("  âœ… {test_file} (dÃ©ployÃ©)")
        else:
            failed_files.append(test_file)
            print("  âŒ {test_file} (Ã©chec du dÃ©ploiement)")
    
    return deployed_files, failed_files

def deploy_test_scripts():
    """DÃ©ploie les scripts de test."""
    print("\nğŸ”§ DÃ©ploiement des scripts de test")
    
    test_scripts = [
        "scripts/run_comprehensive_test_coverage.py",
        "scripts/validate_step10_test_coverage.py",
        "scripts/analyze_test_coverage.py",
        "scripts/run_step10_test_coverage.py"
    ]
    
    deployed_scripts = []
    failed_scripts = []
    
    for script in test_scripts:
        script_path = Path(backend_dir) / script
        if script_path.exists():
            deployed_scripts.append(script)
            print("  âœ… {script} (dÃ©ployÃ©)")
        else:
            failed_scripts.append(script)
            print("  âŒ {script} (Ã©chec du dÃ©ploiement)")
    
    return deployed_scripts, failed_scripts

def validate_test_environment():
    """Valide l'environnement de test."""
    print("\nğŸŒ Validation de l'environnement de test")
    
    # VÃ©rifier Python
    python_version = sys.version_info
    print("  Python: {python_version.major}.{python_version.minor}.{python_version.micro}")
    
    # VÃ©rifier les modules requis
    required_modules = ["pytest", "numpy", "torch", "unittest"]
    available_modules = []
    missing_modules = []
    
    for module in required_modules:
        try:
            __import__(module)
            available_modules.append(module)
            print("  âœ… {module}")
        except ImportError:
            missing_modules.append(module)
            print("  âŒ {module} (manquant)")
    
    return available_modules, missing_modules

def run_test_suite():
    """ExÃ©cute la suite de tests complÃ¨te."""
    print("\nğŸ§ª ExÃ©cution de la suite de tests")
    
    # Essayer d'exÃ©cuter pytest si disponible
    try:
        result = subprocess.run(
            ["pytest", "--version"],
            check=False, capture_output=True,
            text=True,
            timeout=10
        )
        
        if result.returncode == 0:
            print("  âœ… pytest disponible: {result.stdout.strip()}")
            
            # ExÃ©cuter les tests avec couverture
            print("  ğŸ¯ ExÃ©cution des tests avec couverture...")
            
            coverage_result = subprocess.run(
                ["pytest", "tests/", "--cov=backend", "--cov-report=html", "--cov-report=term"],
                check=False, capture_output=True,
                text=True,
                timeout=0.300
            )
            
            if coverage_result.returncode == 0:
                print("  âœ… Tests exÃ©cutÃ©s avec succÃ¨s")
                return True, coverage_result.stdout
            print("  âŒ Erreur lors de l'exÃ©cution des tests: {coverage_result.stderr}")
            return False, coverage_result.stderr
        print("  âŒ pytest non disponible: {result.stderr}")
        return False, result.stderr
            
    except subprocess.TimeoutExpired:
        print("  â° Timeout lors de l'exÃ©cution des tests")
        return False, "Timeout"
    except FileNotFoundError:
        print("  âŒ pytest non trouvÃ© dans le PATH")
        return False, "pytest not found"
    except Exception as e:
        print("  ğŸ’¥ Erreur inattendue: {e}")
        return False, str(e)

def run_manual_tests():
    """ExÃ©cute les tests manuellement."""
    print("\nğŸ”§ ExÃ©cution manuelle des tests")
    
    # Importer et exÃ©cuter les tests principaux
    test_modules = [
        "tests.rl.test_per_comprehensive",
        "tests.rl.test_action_masking_comprehensive",
        "tests.rl.test_reward_shaping_comprehensive",
        "tests.rl.test_integration_comprehensive",
        "tests.test_alerts_comprehensive",
        "tests.test_shadow_mode_comprehensive",
        "tests.test_docker_production_comprehensive"
    ]
    
    test_results = []
    
    for module in test_modules:
        try:
            print("  ğŸ§ª ExÃ©cution de {module}...")
            
            # Importer le module
            spec = __import__(module, fromlist=[""])
            
            # ExÃ©cuter les tests
            if hasattr(spec, "run_tests"):
                passed, total = spec.run_tests()
                test_results.append({
                    "module": module,
                    "passed": passed,
                    "total": total,
                    "success_rate": (passed / total * 100) if total > 0 else 0
                })
                print("    âœ… {passed}/{total} tests rÃ©ussis")
            else:
                print("    âš ï¸ Pas de fonction run_tests trouvÃ©e")
                test_results.append({
                    "module": module,
                    "passed": 0,
                    "total": 0,
                    "success_rate": 0
                })
                
        except Exception as e:
            print("    âŒ Erreur: {e}")
            test_results.append({
                "module": module,
                "passed": 0,
                "total": 0,
                "success_rate": 0,
                "error": str(e)
            })
    
    return test_results

def generate_coverage_report(test_results):
    """GÃ©nÃ¨re un rapport de couverture."""
    print("\nğŸ“Š GÃ©nÃ©ration du rapport de couverture")
    
    # Calculer les statistiques globales
    total_tests = sum(result["total"] for result in test_results)
    total_passed = sum(result["passed"] for result in test_results)
    global_success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
    
    # Analyser les modules RL
    rl_modules = [result for result in test_results if "rl" in result["module"]]
    rl_tests = sum(result["total"] for result in rl_modules)
    rl_passed = sum(result["passed"] for result in rl_modules)
    rl_success_rate = (rl_passed / rl_tests * 100) if rl_tests > 0 else 0
    
    # GÃ©nÃ©rer le rapport
    return {
        "timestamp": datetime.now(UTC).isoformat(),
        "step": "Ã‰tape 10 - Couverture de tests â‰¥ 70%",
        "summary": {
            "total_tests": total_tests,
            "total_passed": total_passed,
            "global_success_rate": global_success_rate,
            "rl_tests": rl_tests,
            "rl_passed": rl_passed,
            "rl_success_rate": rl_success_rate,
            "target_met": global_success_rate >= 70
        },
        "test_results": test_results,
        "recommendations": generate_recommendations(test_results, global_success_rate)
    }
    

def generate_recommendations(test_results, global_success_rate):
    """GÃ©nÃ¨re des recommandations."""
    recommendations = []
    
    if global_success_rate < 70:
        recommendations.append({
            "type": "critical",
            "message": f"Couverture globale insuffisante: {global_success_rate",
            "action": "Augmenter le nombre de tests et amÃ©liorer leur qualitÃ©"
        })
    
    failed_modules = [result for result in test_results if result["total"] > 0 and result["passed"] < result["total"]]
    if failed_modules:
        recommendations.append({
            "type": "warning",
            "message": f"Modules avec tests Ã©chouÃ©s: {len(failed_modules)}",
            "action": "Corriger les tests Ã©chouÃ©s"
        })
    
    modules_without_tests = [result for result in test_results if result["total"] == 0]
    if modules_without_tests:
        recommendations.append({
            "type": "info",
            "message": f"Modules sans tests: {len(modules_without_tests)}",
            "action": "CrÃ©er des tests pour ces modules"
        })
    
    return recommendations

def save_deployment_report(report, filename="step10_deployment_report.json"):
    """Sauvegarde le rapport de dÃ©ploiement."""
    report_path = Path(__file__).parent / filename
    
    with Path(report_path, "w", encoding="utf-8").open() as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    print("ğŸ“„ Rapport de dÃ©ploiement sauvegardÃ©: {report_path}")
    return report_path

def print_deployment_summary(report):
    """Affiche un rÃ©sumÃ© du dÃ©ploiement."""
    print("\n" + "="*60)
    print("ğŸ“Š RÃ‰SUMÃ‰ DU DÃ‰PLOIEMENT - Ã‰TAPE 10")
    print("="*60)
    
    summary = report["summary"]
    
    print("Tests totaux: {summary['total_tests']}")
    print("Tests rÃ©ussis: {summary['total_passed']}")
    print("Couverture globale: {summary['global_success_rate']")
    print("Tests RL: {summary['rl_tests']}")
    print("Tests RL rÃ©ussis: {summary['rl_passed']}")
    print("Couverture RL: {summary['rl_success_rate']")
    print("Objectif atteint: {'âœ…' if summary['target_met'] else 'âŒ'}")
    
    print("\nğŸ“‹ RÃ©sultats par module:")
    for result in report["test_results"]:
        status_emoji = "âœ…" if result["passed"] == result["total"] else "âš ï¸" if result["passed"] > 0 else "âŒ"
        print("  {status_emoji} {result['module']}: {result['passed']}/{result['total']} ({result['success_rate']")
    
    print("\nğŸ’¡ Recommandations:")
    for rec in report["recommendations"]:
        type_emoji = {
            "critical": "ğŸš¨",
            "warning": "âš ï¸",
            "info": "â„¹ï¸"
        }.get(rec["type"], "ğŸ“")
        
        print("  {type_emoji} {rec['message']}")
        print("     Action: {rec['action']}")
    
    print("="*60)

def main():
    """Fonction principale de dÃ©ploiement."""
    print("ğŸš€ DÃ©marrage du dÃ©ploiement de l'Ã‰tape 10")
    print("ğŸ“… {datetime.now(UTC).strftime('%Y-%m-%d %H:%M:%S')} UTC")
    
    # DÃ©ployer les fichiers de test
    _deployed_files, _failed_files = deploy_test_files()
    
    # DÃ©ployer les scripts de test
    _deployed_scripts, _failed_scripts = deploy_test_scripts()
    
    # Valider l'environnement
    _available_modules, _missing_modules = validate_test_environment()
    
    # ExÃ©cuter la suite de tests
    pytest_success, _pytest_output = run_test_suite()
    
    # ExÃ©cuter les tests manuellement si pytest a Ã©chouÃ©
    if not pytest_success:
        print("\nğŸ”„ Fallback vers l'exÃ©cution manuelle des tests")
        test_results = run_manual_tests()
    else:
        # Parser les rÃ©sultats de pytest
        test_results = []
        # (Dans un environnement rÃ©el, on parserait la sortie de pytest)
    
    # GÃ©nÃ©rer le rapport de couverture
    report = generate_coverage_report(test_results)
    
    # Sauvegarder le rapport
    save_deployment_report(report)
    
    # Afficher le rÃ©sumÃ©
    print_deployment_summary(report)
    
    # DÃ©terminer le code de sortie
    if report["summary"]["target_met"]:
        print("\nğŸ‰ DÃ©ploiement rÃ©ussi - Objectif de couverture atteint!")
        return 0
    print("\nâš ï¸ DÃ©ploiement partiel - Des amÃ©liorations sont nÃ©cessaires")
    return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
