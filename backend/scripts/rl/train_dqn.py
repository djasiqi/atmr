#!/usr/bin/env python3
# ruff: noqa: T201, DTZ005
# pyright: reportMissingImports=false
"""
Script d'entra√Ænement DQN pour le dispatch autonome.

Entra√Æne un agent DQN sur l'environnement de dispatch pendant N √©pisodes,
avec monitoring TensorBoard, √©valuation p√©riodique, et sauvegarde automatique.

Usage:
    python scripts/rl/train_dqn.py --episodes 1000 --learning-rate 0.001

Auteur: ATMR Project - RL Team
Date: Octobre 2025
Semaine: 16 (Jours 6-7)
"""
import argparse
import os
import sys
from datetime import datetime
from pathlib import Path

import numpy as np

# Ajouter le chemin backend au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent.parent))

# Importer apr√®s avoir ajust√© le path
try:
    from torch.utils.tensorboard import SummaryWriter
except ImportError:
    print("‚ùå TensorBoard non install√©. Installer avec: pip install tensorboard")
    sys.exit(1)

from services.rl.dispatch_env import DispatchEnv
from services.rl.dqn_agent import DQNAgent


def evaluate_agent(agent: DQNAgent, env: DispatchEnv, episodes: int = 10) -> dict:
    """
    √âvalue l'agent sans exploration (greedy pur).

    Args:
        agent: Agent DQN √† √©valuer
        env: Environnement de dispatch
        episodes: Nombre d'√©pisodes d'√©valuation

    Returns:
        Dictionnaire avec m√©triques d'√©valuation
    """
    print(f"\nüìä √âvaluation sur {episodes} √©pisodes...")

    rewards = []
    steps_list = []
    assignments_list = []
    late_pickups_list = []

    for _ in range(episodes):
        state, _ = env.reset()
        episode_reward = 0.0
        done = False
        steps = 0

        while not done and steps < 200:
            # Greedy (pas d'exploration)
            action = agent.select_action(state, training=False)
            state, reward, done, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1

            if done or truncated:
                break

        rewards.append(episode_reward)
        steps_list.append(steps)

        # Extraire stats si disponibles
        if 'episode_stats' in info:
            stats = info['episode_stats']
            assignments_list.append(stats.get('assignments', 0))
            late_pickups_list.append(stats.get('late_pickups', 0))

    avg_reward = np.mean(rewards)
    std_reward = np.std(rewards)
    avg_steps = np.mean(steps_list)

    print(f"   Reward moyen: {avg_reward:.1f} ¬± {std_reward:.1f}")
    print(f"   Steps moyen: {avg_steps:.1f}")

    if assignments_list:
        avg_assignments = np.mean(assignments_list)
        avg_late = np.mean(late_pickups_list)
        print(f"   Assignments: {avg_assignments:.1f}")
        print(f"   Late pickups: {avg_late:.1f}")

    return {
        'avg_reward': avg_reward,
        'std_reward': std_reward,
        'min_reward': np.min(rewards),
        'max_reward': np.max(rewards),
        'avg_steps': avg_steps,
        'avg_assignments': np.mean(assignments_list) if assignments_list else 0,
        'avg_late_pickups': np.mean(late_pickups_list) if late_pickups_list else 0
    }


def train_dqn(
    episodes: int = 1000,
    max_steps: int = 100,
    learning_rate: float = 0.001,
    gamma: float = 0.99,
    epsilon_decay: float = 0.995,
    batch_size: int = 64,
    save_interval: int = 100,
    eval_interval: int = 50,
    num_drivers: int = 10,
    max_bookings: int = 20,
    simulation_hours: int = 2
):
    """
    Entra√Æne un agent DQN sur l'environnement de dispatch.

    Args:
        episodes: Nombre d'√©pisodes d'entra√Ænement
        max_steps: Steps maximum par √©pisode
        learning_rate: Taux d'apprentissage
        gamma: Discount factor
        epsilon_decay: D√©croissance de epsilon
        batch_size: Taille du batch
        save_interval: Fr√©quence de sauvegarde (episodes)
        eval_interval: Fr√©quence d'√©valuation (episodes)
        num_drivers: Nombre de drivers dans l'environnement
        max_bookings: Nombre maximum de bookings
        simulation_hours: Dur√©e de simulation par √©pisode (heures)
    """
    print("="*70)
    print("üöÄ ENTRA√éNEMENT AGENT DQN - DISPATCH AUTONOME")
    print("="*70)

    # Cr√©er dossiers n√©cessaires
    os.makedirs("data/rl/models", exist_ok=True)
    os.makedirs("data/rl/tensorboard", exist_ok=True)
    os.makedirs("data/rl/logs", exist_ok=True)

    # Cr√©er environnement
    print("\nüì¶ Cr√©ation environnement...")
    env = DispatchEnv(
        num_drivers=num_drivers,
        max_bookings=max_bookings,
        simulation_hours=simulation_hours
    )
    print("   ‚úÖ Environnement cr√©√©:")
    print(f"      Drivers: {num_drivers}")
    print(f"      Max bookings: {max_bookings}")
    print(f"      Simulation: {simulation_hours}h")
    print(f"      State dim: {env.observation_space.shape[0]}")
    print(f"      Action dim: {env.action_space.n}")

    # Cr√©er agent
    print("\nü§ñ Cr√©ation agent DQN...")
    agent = DQNAgent(
        state_dim=env.observation_space.shape[0],
        action_dim=env.action_space.n,
        learning_rate=learning_rate,
        gamma=gamma,
        epsilon_decay=epsilon_decay,
        batch_size=batch_size
    )

    # TensorBoard writer
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = f"data/rl/tensorboard/dqn_{timestamp}"
    writer = SummaryWriter(log_dir)
    print(f"   ‚úÖ TensorBoard logs: {log_dir}")

    # M√©triques de tracking
    episode_rewards = []
    episode_lengths = []
    episode_losses = []
    best_avg_reward = -float('inf')
    recent_rewards = []  # Pour moyenne mobile

    print("\nüìä Configuration:")
    print(f"   Episodes: {episodes}")
    print(f"   Learning rate: {learning_rate}")
    print(f"   Gamma: {gamma}")
    print(f"   Epsilon decay: {epsilon_decay}")
    print(f"   Batch size: {batch_size}")
    print(f"   Device: {agent.device}")

    print("\nüèÅ D√©but de l'entra√Ænement...\n")
    print("-"*70)

    # Boucle d'entra√Ænement principale
    for episode in range(episodes):
        state, _ = env.reset()
        episode_reward = 0.0
        episode_loss = 0.0
        loss_count = 0
        steps = 0
        done = False

        # √âpisode complet
        while not done and steps < max_steps:
            # S√©lectionner action (avec exploration)
            action = agent.select_action(state, training=True)

            # Step dans l'environnement
            next_state, reward, done, truncated, info = env.step(action)

            # Stocker transition
            agent.store_transition(state, action, next_state, reward, done or truncated)

            # Entra√Æner si assez de donn√©es
            if len(agent.memory) >= agent.batch_size:
                loss = agent.train_step()
                episode_loss += loss
                loss_count += 1

            # Mise √† jour
            state = next_state
            episode_reward += reward
            steps += 1

        # Fin de l'√©pisode
        agent.decay_epsilon()
        agent.episode_count += 1

        # Update target network p√©riodiquement
        if (episode + 1) % agent.target_update_freq == 0:
            agent.update_target_network()

        # Tracking
        episode_rewards.append(episode_reward)
        episode_lengths.append(steps)
        recent_rewards.append(episode_reward)

        # Garder seulement les 100 derniers pour moyenne mobile
        if len(recent_rewards) > 100:
            recent_rewards.pop(0)

        avg_loss = episode_loss / loss_count if loss_count > 0 else 0.0
        episode_losses.append(avg_loss)

        # TensorBoard logging
        writer.add_scalar('Training/Reward', episode_reward, episode)
        writer.add_scalar('Training/Epsilon', agent.epsilon, episode)
        writer.add_scalar('Training/Loss', avg_loss, episode)
        writer.add_scalar('Training/Steps', steps, episode)
        writer.add_scalar('Training/BufferSize', len(agent.memory), episode)

        # Moyenne mobile
        if len(recent_rewards) >= 10:
            avg_reward_10 = np.mean(recent_rewards[-10:])
            writer.add_scalar('Training/AvgReward10', avg_reward_10, episode)

        if len(recent_rewards) >= 100:
            avg_reward_100 = np.mean(recent_rewards)
            writer.add_scalar('Training/AvgReward100', avg_reward_100, episode)

        # Print progress tous les 10 episodes
        if (episode + 1) % 10 == 0:
            avg_reward_10 = np.mean(recent_rewards[-10:]) if len(recent_rewards) >= 10 else episode_reward
            print(f"Episode {episode+1:4d}/{episodes} | "
                  f"Reward: {episode_reward:7.1f} | "
                  f"Avg(10): {avg_reward_10:7.1f} | "
                  f"Œµ: {agent.epsilon:.3f} | "
                  f"Loss: {avg_loss:.4f} | "
                  f"Steps: {steps:3d}")

        # √âvaluation p√©riodique
        if (episode + 1) % eval_interval == 0:
            eval_results = evaluate_agent(agent, env, episodes=10)

            # TensorBoard
            writer.add_scalar('Evaluation/AvgReward', eval_results['avg_reward'], episode)
            writer.add_scalar('Evaluation/StdReward', eval_results['std_reward'], episode)
            writer.add_scalar('Evaluation/AvgSteps', eval_results['avg_steps'], episode)

            print(f"\n{'='*70}")
            print(f"üìà √âVALUATION (Episode {episode+1})")
            print(f"   Reward: {eval_results['avg_reward']:.1f} ¬± {eval_results['std_reward']:.1f}")
            print(f"   Range: [{eval_results['min_reward']:.1f}, {eval_results['max_reward']:.1f}]")
            print(f"{'='*70}\n")

            # Sauvegarder meilleur mod√®le
            if eval_results['avg_reward'] > best_avg_reward:
                best_avg_reward = eval_results['avg_reward']
                best_path = "data/rl/models/dqn_best.pth"
                agent.save(best_path)
                print(f"   ‚úÖ Nouveau meilleur mod√®le: {best_avg_reward:.1f} ‚Üí {best_path}")

        # Checkpoints p√©riodiques
        if (episode + 1) % save_interval == 0:
            avg_recent = np.mean(recent_rewards[-10:]) if len(recent_rewards) >= 10 else episode_reward
            checkpoint_path = agent.save_checkpoint(episode + 1, float(avg_recent))
            print(f"   üíæ Checkpoint sauvegard√©: {checkpoint_path}")

    # Fin du training
    print("\n" + "="*70)
    print("‚úÖ ENTRA√éNEMENT TERMIN√â!")
    print("="*70)

    # Statistiques finales
    print("\nüìä Statistiques finales:")
    print(f"   Episodes entra√Æn√©s: {episodes}")
    print(f"   Training steps: {agent.training_step}")
    print(f"   Meilleur reward (eval): {best_avg_reward:.1f}")
    print(f"   Epsilon final: {agent.epsilon:.4f}")
    print(f"   Buffer size: {len(agent.memory)}")

    # Moyenne des 100 derniers √©pisodes
    if len(episode_rewards) >= 100:
        avg_last_100 = np.mean(episode_rewards[-100:])
        print(f"   Avg reward (100 derniers): {avg_last_100:.1f}")

    # Sauvegarder mod√®le final
    final_path = "data/rl/models/dqn_final.pth"
    agent.save(final_path)
    print(f"\nüíæ Mod√®le final sauvegard√©: {final_path}")

    # Fermer TensorBoard
    writer.close()
    print(f"üìä TensorBoard logs: {log_dir}")
    print(f"   Lancer avec: tensorboard --logdir={log_dir}")

    # √âvaluation finale
    print("\nüéØ √âvaluation finale (100 √©pisodes)...")
    final_eval = evaluate_agent(agent, env, episodes=100)

    print(f"\n{'='*70}")
    print("üìà R√âSULTATS FINAUX")
    print("="*70)
    print(f"Reward moyen: {final_eval['avg_reward']:.1f} ¬± {final_eval['std_reward']:.1f}")
    print(f"Range: [{final_eval['min_reward']:.1f}, {final_eval['max_reward']:.1f}]")
    print(f"Steps moyen: {final_eval['avg_steps']:.1f}")
    if final_eval['avg_assignments'] > 0:
        print(f"Assignments: {final_eval['avg_assignments']:.1f}")
        print(f"Late pickups: {final_eval['avg_late_pickups']:.1f}")
    print(f"{'='*70}")

    # Sauvegarder m√©triques finales
    import json
    metrics_path = f"data/rl/logs/metrics_{timestamp}.json"
    metrics = {
        'timestamp': timestamp,
        'episodes': episodes,
        'learning_rate': learning_rate,
        'gamma': gamma,
        'epsilon_decay': epsilon_decay,
        'batch_size': batch_size,
        'final_epsilon': agent.epsilon,
        'training_steps': agent.training_step,
        'best_eval_reward': best_avg_reward,
        'final_eval': final_eval,
        'episode_rewards': episode_rewards[-100:],  # 100 derniers
    }

    with open(metrics_path, 'w') as f:
        json.dump(metrics, f, indent=2)

    print(f"\nüíæ M√©triques sauvegard√©es: {metrics_path}")

    return agent, final_eval


def main():
    """Point d'entr√©e principal."""
    parser = argparse.ArgumentParser(
        description="Entra√Æner un agent DQN pour le dispatch autonome"
    )

    # Param√®tres d'entra√Ænement
    parser.add_argument('--episodes', type=int, default=1000,
                        help='Nombre d\'√©pisodes d\'entra√Ænement (d√©faut: 1000)')
    parser.add_argument('--max-steps', type=int, default=100,
                        help='Steps maximum par √©pisode (d√©faut: 100)')
    parser.add_argument('--learning-rate', type=float, default=0.001,
                        help='Taux d\'apprentissage (d√©faut: 0.001)')
    parser.add_argument('--gamma', type=float, default=0.99,
                        help='Discount factor (d√©faut: 0.99)')
    parser.add_argument('--epsilon-decay', type=float, default=0.995,
                        help='D√©croissance epsilon (d√©faut: 0.995)')
    parser.add_argument('--batch-size', type=int, default=64,
                        help='Taille du batch (d√©faut: 64)')
    parser.add_argument('--save-interval', type=int, default=100,
                        help='Fr√©quence de sauvegarde en episodes (d√©faut: 100)')
    parser.add_argument('--eval-interval', type=int, default=50,
                        help='Fr√©quence d\'√©valuation en episodes (d√©faut: 50)')

    # Param√®tres environnement
    parser.add_argument('--num-drivers', type=int, default=10,
                        help='Nombre de drivers (d√©faut: 10)')
    parser.add_argument('--max-bookings', type=int, default=20,
                        help='Nombre maximum de bookings (d√©faut: 20)')
    parser.add_argument('--simulation-hours', type=int, default=2,
                        help='Dur√©e simulation en heures (d√©faut: 2)')

    args = parser.parse_args()

    # Lancer l'entra√Ænement
    try:
        train_dqn(
            episodes=args.episodes,
            max_steps=args.max_steps,
            learning_rate=args.learning_rate,
            gamma=args.gamma,
            epsilon_decay=args.epsilon_decay,
            batch_size=args.batch_size,
            save_interval=args.save_interval,
            eval_interval=args.eval_interval,
            num_drivers=args.num_drivers,
            max_bookings=args.max_bookings,
            simulation_hours=args.simulation_hours
        )
        print("\nüéâ Training termin√© avec succ√®s!")
        return 0

    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Entra√Ænement interrompu par l'utilisateur.")
        return 1

    except Exception as e:
        print(f"\n‚ùå Erreur pendant l'entra√Ænement: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
