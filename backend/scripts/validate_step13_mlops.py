#!/usr/bin/env python3
# pyright: reportMissingImports=false
"""Script de validation pour l'√âtape 13 - MLOps : registre mod√®les & promotion contr√¥l√©e.

Ce script valide que le syst√®me MLOps fonctionne correctement avec
tra√ßabilit√©, promotion contr√¥l√©e et rollback.
"""

import json
import sys
import tempfile
import traceback
from datetime import UTC, datetime
from pathlib import Path

from torch import nn

# Ajouter le r√©pertoire backend au path Python
backend_dir = Path(__file__).parent.parent
sys.path.insert(0, str(backend_dir))

def test_model_registry_import():
    """Teste l'importation des modules MLOps."""
    print("\nüß™ Test d'importation des modules MLOps")
    print("-" * 50)
    
    try:
        print("  ‚úÖ Import ModelRegistry: SUCC√àS")
        print("  ‚úÖ Import ModelMetadata: SUCC√àS")
        print("  ‚úÖ Import ModelPromotionValidator: SUCC√àS")
        print("  ‚úÖ Import TrainingMetadataSchema: SUCC√àS")
        print("  ‚úÖ Import create_model_registry: SUCC√àS")
        print("  ‚úÖ Import create_training_metadata: SUCC√àS")
        
        return True
        
    except Exception:
        print("  ‚ùå Import modules MLOps: √âCHEC - {e}")
        print("     Traceback: {traceback.format_exc()}")
        return False

def test_model_registry_creation():
    """Teste la cr√©ation du registre de mod√®les."""
    print("\nüß™ Test cr√©ation du registre de mod√®les")
    print("-" * 50)
    
    try:
        from services.ml.model_registry import create_model_registry
        
        with tempfile.TemporaryDirectory() as temp_dir:
            registry_path = Path(temp_dir)
            registry = create_model_registry(registry_path)
            
            assert registry.registry_path == registry_path
            assert registry.models_path.exists()
            assert registry.metadata_path.exists()
            assert registry.current_path.exists()
            assert registry.registry_file.exists()
            
            print("  ‚úÖ Cr√©ation du registre: SUCC√àS")
            print("  ‚úÖ R√©pertoires cr√©√©s: SUCC√àS")
            print("  ‚úÖ Fichier de registre: SUCC√àS")
            
            return True
            
    except Exception:
        print("  ‚ùå Cr√©ation du registre: √âCHEC - {e}")
        print("     Traceback: {traceback.format_exc()}")
        return False

def test_model_metadata_schema():
    """Teste le sch√©ma de m√©tadonn√©es."""
    print("\nüß™ Test sch√©ma de m√©tadonn√©es")
    print("-" * 50)
    
    try:
        from services.ml.training_metadata_schema import TrainingMetadataSchema, create_training_metadata
        
        # Test cr√©ation du template
        template = TrainingMetadataSchema.create_metadata_template()
        assert "model_info" in template
        assert "architecture_config" in template
        assert "training_config" in template
        assert "features_config" in template
        assert "scalers_config" in template
        print("  ‚úÖ Template de m√©tadonn√©es: SUCC√àS")
        
        # Test validation
        is_valid, issues = TrainingMetadataSchema.validate_metadata(template)
        assert is_valid
        assert len(issues) == 0
        print("  ‚úÖ Validation des m√©tadonn√©es: SUCC√àS")
        
        # Test cr√©ation de m√©tadonn√©es personnalis√©es
        metadata = create_training_metadata(
            model_name="test_model",
            model_arch="dueling_dqn",
            version="v1.00"
        )
        assert metadata["model_info"]["model_name"] == "test_model"
        assert metadata["model_info"]["model_arch"] == "dueling_dqn"
        print("  ‚úÖ Cr√©ation de m√©tadonn√©es personnalis√©es: SUCC√àS")
        
        return True
        
    except Exception:
        print("  ‚ùå Test sch√©ma de m√©tadonn√©es: √âCHEC - {e}")
        print("     Traceback: {traceback.format_exc()}")
        return False

def test_model_registration():
    """Teste l'enregistrement de mod√®les."""
    print("\nüß™ Test enregistrement de mod√®les")
    print("-" * 50)
    
    try:
        from services.ml.model_registry import ModelMetadata, create_model_registry
        
        with tempfile.TemporaryDirectory() as temp_dir:
            registry_path = Path(temp_dir)
            registry = create_model_registry(registry_path)
            
            # Cr√©er un mod√®le de test
            model = nn.Linear(10, 5)
            
            # Cr√©er les m√©tadonn√©es
            metadata = ModelMetadata(
                model_name="test_model",
                model_arch="dueling_dqn",
                version="v1.00",
                created_at=datetime.now(UTC),
                training_config={"learning_rate": 0.0001},
                performance_metrics={"accuracy": 0.85, "punctuality_rate": 0.88},
                features_config={"state_features": 15},
                scalers_config={"state_scaler": "StandardScaler"}
            )
            
            # Enregistrer le mod√®le
            model_path = registry.register_model(model, metadata)
            
            assert model_path.exists()
            assert model_path.suffix == ".pth"
            print("  ‚úÖ Enregistrement du mod√®le: SUCC√àS")
            
            # V√©rifier les versions
            versions = registry.get_model_versions("test_model", "dueling_dqn")
            assert len(versions) == 1
            assert versions[0]["version"] == "v1.00"
            print("  ‚úÖ R√©cup√©ration des versions: SUCC√àS")
            
            return True
            
    except Exception:
        print("  ‚ùå Test enregistrement de mod√®les: √âCHEC - {e}")
        print("     Traceback: {traceback.format_exc()}")
        return False

def test_model_promotion():
    """Teste la promotion de mod√®les."""
    print("\nüß™ Test promotion de mod√®les")
    print("-" * 50)
    
    try:
        
        with tempfile.TemporaryDirectory() as temp_dir:
            registry_path = Path(temp_dir)
            registry = create_model_registry(registry_path)
            
            # Cr√©er un mod√®le de test
            model = nn.Linear(10, 5)
            
            # Cr√©er les m√©tadonn√©es avec de bonnes m√©triques
            metadata = ModelMetadata(
                model_name="test_model",
                model_arch="dueling_dqn",
                version="v1.00",
                created_at=datetime.now(UTC),
                training_config={"learning_rate": 0.0001},
                performance_metrics={
                    "punctuality_rate": 0.88,
                    "avg_distance": 12.5,
                    "avg_delay": 3.2
                },
                features_config={"state_features": 15},
                scalers_config={"state_scaler": "StandardScaler"}
            )
            
            # Enregistrer le mod√®le
            registry.register_model(model, metadata)
            
            # Promouvoir le mod√®le
            kpi_thresholds = {
                "punctuality_rate": 0.85,
                "avg_distance": 15.0,
                "avg_delay": 5.0
            }
            
            success = registry.promote_model(
                "test_model", "dueling_dqn", "v1.00", kpi_thresholds
            )
            
            assert success
            print("  ‚úÖ Promotion r√©ussie: SUCC√àS")
            
            # V√©rifier la promotion
            current_model = registry.get_current_model("test_model", "dueling_dqn")
            assert current_model is not None
            assert current_model["version"] == "v1.00"
            print("  ‚úÖ V√©rification de la promotion: SUCC√àS")
            
            # V√©rifier le lien symbolique
            current_model_path = registry.current_path / "test_model_dueling_dqn.pth"
            assert current_model_path.exists()
            print("  ‚úÖ Lien symbolique cr√©√©: SUCC√àS")
            
            return True
            
    except Exception:
        print("  ‚ùå Test promotion de mod√®les: √âCHEC - {e}")
        print("     Traceback: {traceback.format_exc()}")
        return False

def test_model_rollback():
    """Teste le rollback de mod√®les."""
    print("\nüß™ Test rollback de mod√®les")
    print("-" * 50)
    
    try:
        
        with tempfile.TemporaryDirectory() as temp_dir:
            registry_path = Path(temp_dir)
            registry = create_model_registry(registry_path)
            
            # Cr√©er un mod√®le de test
            model = nn.Linear(10, 5)
            
            # Enregistrer plusieurs versions
            versions = ["v1.00", "v1.10", "v1.20"]
            for version in versions:
                metadata = ModelMetadata(
                    model_name="test_model",
                    model_arch="dueling_dqn",
                    version=version,
                    created_at=datetime.now(UTC),
                    training_config={"learning_rate": 0.0001},
                    performance_metrics={"accuracy": 0.85},
                    features_config={"state_features": 15},
                    scalers_config={"state_scaler": "StandardScaler"}
                )
                registry.register_model(model, metadata)
            
            print("  ‚úÖ Enregistrement de plusieurs versions: SUCC√àS")
            
            # Promouvoir la derni√®re version
            registry.promote_model("test_model", "dueling_dqn", "v1.20", {}, force=True)
            print("  ‚úÖ Promotion de la derni√®re version: SUCC√àS")
            
            # Rollback vers la premi√®re version
            success = registry.rollback_model("test_model", "dueling_dqn", "v1.00")
            assert success
            print("  ‚úÖ Rollback r√©ussi: SUCC√àS")
            
            # V√©rifier le rollback
            current_model = registry.get_current_model("test_model", "dueling_dqn")
            assert current_model["version"] == "v1.00"
            print("  ‚úÖ V√©rification du rollback: SUCC√àS")
            
            return True
            
    except Exception:
        print("  ‚ùå Test rollback de mod√®les: √âCHEC - {e}")
        print("     Traceback: {traceback.format_exc()}")
        return False

def test_kpi_validation():
    """Teste la validation des KPIs."""
    print("\nüß™ Test validation des KPIs")
    print("-" * 50)
    
    try:
        
        with tempfile.TemporaryDirectory() as temp_dir:
            registry_path = Path(temp_dir)
            registry = create_model_registry(registry_path)
            
            # Cr√©er un mod√®le de test
            model = nn.Linear(10, 5)
            
            # Test avec des m√©triques faibles (doit √©chouer)
            metadata_weak = ModelMetadata(
                model_name="test_model",
                model_arch="dueling_dqn",
                version="v1.00",
                created_at=datetime.now(UTC),
                training_config={"learning_rate": 0.0001},
                performance_metrics={
                    "punctuality_rate": 0.7,  # Faible
                    "avg_distance": 20.0,    # √âlev√©
                    "avg_delay": 8.0         # √âlev√©
                },
                features_config={"state_features": 15},
                scalers_config={"state_scaler": "StandardScaler"}
            )
            
            registry.register_model(model, metadata_weak)
            
            kpi_thresholds = {
                "punctuality_rate": 0.85,
                "avg_distance": 15.0,
                "avg_delay": 5.0
            }
            
            success = registry.promote_model(
                "test_model", "dueling_dqn", "v1.00", kpi_thresholds
            )
            
            assert not success  # Doit √©chouer
            print("  ‚úÖ Validation KPI faible (√©chec attendu): SUCC√àS")
            
            # Test avec des m√©triques bonnes (doit r√©ussir)
            metadata_good = ModelMetadata(
                model_name="test_model2",
                model_arch="dueling_dqn",
                version="v1.00",
                created_at=datetime.now(UTC),
                training_config={"learning_rate": 0.0001},
                performance_metrics={
                    "punctuality_rate": 0.88,  # Bon
                    "avg_distance": 12.0,      # Bon
                    "avg_delay": 3.0          # Bon
                },
                features_config={"state_features": 15},
                scalers_config={"state_scaler": "StandardScaler"}
            )
            
            registry.register_model(model, metadata_good)
            
            success = registry.promote_model(
                "test_model2", "dueling_dqn", "v1.00", kpi_thresholds
            )
            
            assert success  # Doit r√©ussir
            print("  ‚úÖ Validation KPI bon (succ√®s attendu): SUCC√àS")
            
            return True
            
    except Exception:
        print("  ‚ùå Test validation des KPIs: √âCHEC - {e}")
        print("     Traceback: {traceback.format_exc()}")
        return False

def test_training_scripts():
    """Teste les scripts de training."""
    print("\nüß™ Test scripts de training")
    print("-" * 50)
    
    try:
        # Test import des scripts
        from scripts.ml.train_model import MLTrainingOrchestrator
        from scripts.rl.rl_train_offline import RLTrainingOrchestrator
        
        print("  ‚úÖ Import MLTrainingOrchestrator: SUCC√àS")
        print("  ‚úÖ Import RLTrainingOrchestrator: SUCC√àS")
        
        # Test cr√©ation des orchestrateurs
        with tempfile.TemporaryDirectory() as temp_dir:
            registry_path = Path(temp_dir)
            
            ml_orchestrator = MLTrainingOrchestrator(registry_path)
            assert ml_orchestrator.registry_path == registry_path
            print("  ‚úÖ Cr√©ation MLTrainingOrchestrator: SUCC√àS")
            
            rl_orchestrator = RLTrainingOrchestrator(registry_path)
            assert rl_orchestrator.registry_path == registry_path
            print("  ‚úÖ Cr√©ation RLTrainingOrchestrator: SUCC√àS")
        
        return True
        
    except Exception:
        print("  ‚ùå Test scripts de training: √âCHEC - {e}")
        print("     Traceback: {traceback.format_exc()}")
        return False

def test_evaluation_file_update():
    """Teste la mise √† jour du fichier d'√©valuation."""
    print("\nüß™ Test mise √† jour fichier d'√©valuation")
    print("-" * 50)
    
    try:
        
        with tempfile.TemporaryDirectory() as temp_dir:
            registry_path = Path(temp_dir)
            registry = create_model_registry(registry_path)
            
            # Cr√©er un mod√®le de test
            model = nn.Linear(10, 5)
            
            # Cr√©er les m√©tadonn√©es
            metadata = ModelMetadata(
                model_name="test_model",
                model_arch="dueling_dqn",
                version="v1.00",
                created_at=datetime.now(UTC),
                training_config={"learning_rate": 0.0001},
                performance_metrics={
                    "punctuality_rate": 0.88,
                    "avg_distance": 12.5,
                    "avg_delay": 3.2
                },
                features_config={"state_features": 15},
                scalers_config={"state_scaler": "StandardScaler"}
            )
            
            # Enregistrer et promouvoir le mod√®le
            registry.register_model(model, metadata)
            registry.promote_model("test_model", "dueling_dqn", "v1.00", {}, force=True)
            
            # Simuler la mise √† jour du fichier d'√©valuation
            evaluation_file = registry_path / "evaluation_optimized_final.json"
            current_model = registry.get_current_model("test_model", "dueling_dqn")
            
            evaluation_data = {
                "timestamp": datetime.now(UTC).isoformat(),
                "model_version": "v1.00",
                "model_architecture": "dueling_dqn",
                "performance_metrics": current_model["performance_metrics"],
                "model_path": current_model["model_path"],
                "metadata_path": current_model["metadata_path"],
                "promotion_date": current_model["promoted_at"]
            }
            
            with Path(evaluation_file, "w", encoding="utf-8").open() as f:
                json.dump(evaluation_data, f, indent=2, ensure_ascii=False)
            
            assert evaluation_file.exists()
            print("  ‚úÖ Cr√©ation du fichier d'√©valuation: SUCC√àS")
            
            # V√©rifier le contenu
            with Path(evaluation_file, encoding="utf-8").open() as f:
                loaded_data = json.load(f)
            
            assert loaded_data["model_version"] == "v1.00"
            assert loaded_data["model_architecture"] == "dueling_dqn"
            print("  ‚úÖ V√©rification du contenu: SUCC√àS")
            
            return True
            
    except Exception:
        print("  ‚ùå Test mise √† jour fichier d'√©valuation: √âCHEC - {e}")
        print("     Traceback: {traceback.format_exc()}")
        return False

def run_comprehensive_validation():
    """Ex√©cute la validation compl√®te de l'√âtape 13."""
    print("üöÄ VALIDATION COMPL√àTE DE L'√âTAPE 13 - MLOPS")
    print("=" * 70)
    print("üìÖ Date: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M:%S')} UTC")
    print("üê≥ Environnement: Docker Container")
    print("üêç Python: {sys.version}")
    print()
    
    # Liste des tests √† ex√©cuter
    tests = [
        {
            "name": "Importation des modules MLOps",
            "function": test_model_registry_import
        },
        {
            "name": "Cr√©ation du registre de mod√®les",
            "function": test_model_registry_creation
        },
        {
            "name": "Sch√©ma de m√©tadonn√©es",
            "function": test_model_metadata_schema
        },
        {
            "name": "Enregistrement de mod√®les",
            "function": test_model_registration
        },
        {
            "name": "Promotion de mod√®les",
            "function": test_model_promotion
        },
        {
            "name": "Rollback de mod√®les",
            "function": test_model_rollback
        },
        {
            "name": "Validation des KPIs",
            "function": test_kpi_validation
        },
        {
            "name": "Scripts de training",
            "function": test_training_scripts
        },
        {
            "name": "Mise √† jour fichier d'√©valuation",
            "function": test_evaluation_file_update
        }
    ]
    
    results = []
    total_tests = len(tests)
    successful_tests = 0
    
    # Ex√©cuter chaque test
    for test in tests:
        print("\nüìã Test: {test['name']}")
        success = test["function"]()
        
        results.append({
            "name": test["name"],
            "success": success
        })
        
        if success:
            successful_tests += 1
    
    # G√©n√©rer le rapport final
    print("\n" + "=" * 70)
    print("üìä RAPPORT FINAL DE VALIDATION - √âTAPE 13")
    print("=" * 70)
    
    print("Total des tests: {total_tests}")
    print("Tests r√©ussis: {successful_tests}")
    print("Tests √©chou√©s: {total_tests - successful_tests}")
    print("Taux de succ√®s: {(successful_tests / total_tests * 100)")
    
    print("\nüìã D√©tail des r√©sultats:")
    for result in results:
        "‚úÖ" if result["success"] else "‚ùå"
        print("  {status_emoji} {result['name']}")
        print("     Statut: {'SUCC√àS' if result['success'] else '√âCHEC'}")
        print()
    
    # Conclusion
    if successful_tests == total_tests:
        print("üéâ VALIDATION COMPL√àTE R√âUSSIE!")
        print("‚úÖ Le syst√®me MLOps fonctionne parfaitement")
        print("‚úÖ Le registre de mod√®les est op√©rationnel")
        print("‚úÖ La promotion contr√¥l√©e fonctionne")
        print("‚úÖ Le rollback est fonctionnel")
        print("‚úÖ La validation des KPIs est efficace")
        print("‚úÖ Les scripts de training sont int√©gr√©s")
        print("‚úÖ L'√âtape 13 est pr√™te pour la production")
    else:
        print("‚ö†Ô∏è VALIDATION PARTIELLE")
        print("‚úÖ Certains composants fonctionnent")
        print("‚ö†Ô∏è Certains tests ont √©chou√©")
        print("üîç V√©rifier les erreurs ci-dessus")
    
    return successful_tests >= total_tests * 0.8  # 80% de succ√®s acceptable

def main():
    """Fonction principale."""
    try:
        success = run_comprehensive_validation()
        
        if success:
            print("\nüéâ VALIDATION R√âUSSIE!")
            print("‚úÖ L'√âtape 13 - MLOps est valid√©e")
            return 0
        print("\n‚ö†Ô∏è VALIDATION PARTIELLE")
        print("‚ùå Certains aspects n√©cessitent attention")
        return 1
            
    except Exception:
        print("\nüö® ERREUR CRITIQUE: {e}")
        print("Traceback: {traceback.format_exc()}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
