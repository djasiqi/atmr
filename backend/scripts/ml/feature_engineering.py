"""
Script de feature engineering avancÃ© pour le modÃ¨le ML de prÃ©diction de retards.

CrÃ©e des features dÃ©rivÃ©es, interactions, et normalise les donnÃ©es.

Usage:
    python scripts/ml/feature_engineering.py [--input data/ml/training_data.csv] [--output data/ml/]
"""
# ruff: noqa: T201
# pyright: reportArgumentType=false, reportAttributeAccessIssue=false, reportGeneralTypeIssues=false, reportReturnType=false
# print() est intentionnel dans les scripts ML
# Pandas/sklearn ont des types complexes, ignorer warnings stricts

import argparse
import json
import sys
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler  # type: ignore[import-untyped]


def create_interaction_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    CrÃ©e des features d'interaction entre variables importantes.

    Interactions crÃ©Ã©es :
    - distance Ã— traffic_density : Impact combinÃ© distance + trafic
    - distance Ã— weather_factor : Impact combinÃ© distance + mÃ©tÃ©o
    - traffic_density Ã— weather_factor : Conditions dÃ©favorables combinÃ©es
    - is_medical Ã— distance : Urgence mÃ©dicale longue distance
    - is_urgent Ã— traffic_density : Urgence en heure de pointe
    """
    print("\n" + "="*70)
    print("ğŸ”— CRÃ‰ATION DES FEATURES D'INTERACTION")
    print("="*70)

    df_new = df.copy()

    # Interaction 1: Distance Ã— Trafic (effet combinÃ© majeur)
    if 'distance_km' in df.columns and 'traffic_density' in df.columns:
        df_new['distance_x_traffic'] = df['distance_km'] * df['traffic_density']
        print("âœ… distance_x_traffic = distance Ã— traffic")

    # Interaction 2: Distance Ã— MÃ©tÃ©o
    if 'distance_km' in df.columns and 'weather_factor' in df.columns:
        df_new['distance_x_weather'] = df['distance_km'] * df['weather_factor']
        print("âœ… distance_x_weather = distance Ã— weather")

    # Interaction 3: Trafic Ã— MÃ©tÃ©o (conditions dÃ©favorables)
    if 'traffic_density' in df.columns and 'weather_factor' in df.columns:
        df_new['traffic_x_weather'] = df['traffic_density'] * df['weather_factor']
        print("âœ… traffic_x_weather = traffic Ã— weather")

    # Interaction 4: MÃ©dical Ã— Distance (urgence longue distance)
    if 'is_medical' in df.columns and 'distance_km' in df.columns:
        df_new['medical_x_distance'] = df['is_medical'] * df['distance_km']
        print("âœ… medical_x_distance = is_medical Ã— distance")

    # Interaction 5: Urgent Ã— Trafic (urgence en pointe)
    if 'is_urgent' in df.columns and 'traffic_density' in df.columns:
        df_new['urgent_x_traffic'] = df['is_urgent'] * df['traffic_density']
        print("âœ… urgent_x_traffic = is_urgent Ã— traffic")

    n_new_features = len(df_new.columns) - len(df.columns)
    print(f"\nâœ… {n_new_features} features d'interaction crÃ©Ã©es")

    return df_new


def create_temporal_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    CrÃ©e des features temporelles avancÃ©es.

    Features crÃ©Ã©es :
    - is_rush_hour : Binaire heures de pointe (7-9h, 17-19h)
    - is_weekend : Binaire weekend (samedi-dimanche)
    - hour_sin, hour_cos : Encodage cyclique de l'heure
    - day_sin, day_cos : Encodage cyclique du jour
    - is_morning_peak : Binaire pic matin (7-9h)
    - is_evening_peak : Binaire pic soir (17-19h)
    """
    print("\n" + "="*70)
    print("â° CRÃ‰ATION DES FEATURES TEMPORELLES")
    print("="*70)

    df_new = df.copy()

    # Heures de pointe (7-9h et 17-19h)
    if 'time_of_day' in df.columns:
        df_new['is_rush_hour'] = df['time_of_day'].apply(
            lambda h: 1.0 if h in [7, 8, 17, 18] else 0.0
        )
        print("âœ… is_rush_hour (7-9h, 17-19h)")

        df_new['is_morning_peak'] = df['time_of_day'].apply(
            lambda h: 1.0 if h in [7, 8] else 0.0
        )
        print("âœ… is_morning_peak (7-9h)")

        df_new['is_evening_peak'] = df['time_of_day'].apply(
            lambda h: 1.0 if h in [17, 18] else 0.0
        )
        print("âœ… is_evening_peak (17-19h)")

        # Encodage cyclique de l'heure (Ã©vite discontinuitÃ© 23h â†’ 0h)
        df_new['hour_sin'] = np.sin(2 * np.pi * df['time_of_day'] / 24)
        df_new['hour_cos'] = np.cos(2 * np.pi * df['time_of_day'] / 24)
        print("âœ… hour_sin, hour_cos (encodage cyclique)")

    # Weekend (samedi-dimanche)
    if 'day_of_week' in df.columns:
        df_new['is_weekend'] = df['day_of_week'].apply(
            lambda d: 1.0 if d >= 5 else 0.0
        )
        print("âœ… is_weekend (samedi-dimanche)")

        # Encodage cyclique du jour (Ã©vite discontinuitÃ© dimanche â†’ lundi)
        df_new['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)
        df_new['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)
        print("âœ… day_sin, day_cos (encodage cyclique)")

    # Midi (12-14h)
    if 'time_of_day' in df.columns:
        df_new['is_lunch_time'] = df['time_of_day'].apply(
            lambda h: 1.0 if h in [12, 13] else 0.0
        )
        print("âœ… is_lunch_time (12-14h)")

    n_new_features = len(df_new.columns) - len(df.columns)
    print(f"\nâœ… {n_new_features} features temporelles crÃ©Ã©es")

    return df_new


def create_aggregated_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    CrÃ©e des features agrÃ©gÃ©es basÃ©es sur l'historique et les patterns.

    Features crÃ©Ã©es :
    - delay_by_hour : Retard moyen par heure
    - delay_by_day : Retard moyen par jour
    - delay_by_driver_exp : Retard moyen par niveau d'expÃ©rience driver
    - distance_category : CatÃ©gorie de distance (courte/moyenne/longue)
    - traffic_level : Niveau de trafic (faible/moyen/Ã©levÃ©)
    """
    print("\n" + "="*70)
    print("ğŸ“Š CRÃ‰ATION DES FEATURES AGRÃ‰GÃ‰ES")
    print("="*70)

    df_new = df.copy()
    target = 'actual_delay_minutes'

    # Retard moyen par heure
    if 'time_of_day' in df.columns and target in df.columns:
        hour_delays = df.groupby('time_of_day')[target].mean()
        df_new['delay_by_hour'] = df['time_of_day'].map(hour_delays)  # type: ignore[arg-type]
        print("âœ… delay_by_hour (retard moyen par heure)")

    # Retard moyen par jour
    if 'day_of_week' in df.columns and target in df.columns:
        day_delays = df.groupby('day_of_week')[target].mean()
        df_new['delay_by_day'] = df['day_of_week'].map(day_delays)  # type: ignore[arg-type]
        print("âœ… delay_by_day (retard moyen par jour)")

    # CatÃ©gorie d'expÃ©rience driver
    if 'driver_total_bookings' in df.columns:
        df_new['driver_experience_level'] = pd.cut(
            df['driver_total_bookings'],
            bins=[0, 50, 200, float('inf')],
            labels=[0, 1, 2]  # 0=novice, 1=intermÃ©diaire, 2=expert
        ).astype(float)  # type: ignore[attr-defined]
        print("âœ… driver_experience_level (novice/inter/expert)")

        # Retard moyen par niveau d'expÃ©rience
        if target in df.columns:
            exp_delays = df.groupby(
                pd.cut(df['driver_total_bookings'], bins=[0, 50, 200, float('inf')]), observed=True
            )[target].mean()
            df_new['delay_by_driver_exp'] = pd.cut(
                df['driver_total_bookings'],
                bins=[0, 50, 200, float('inf')]
            ).map(exp_delays)  # type: ignore[attr-defined,arg-type]
            print("âœ… delay_by_driver_exp (retard par niveau exp)")

    # CatÃ©gorie de distance
    if 'distance_km' in df.columns:
        df_new['distance_category'] = pd.cut(
            df['distance_km'],
            bins=[0, 5, 10, 20, float('inf')],
            labels=[0, 1, 2, 3]  # 0=courte, 1=moyenne, 2=longue, 3=trÃ¨s longue
        ).astype(float)  # type: ignore[attr-defined]
        print("âœ… distance_category (courte/moyenne/longue)")

    # Niveau de trafic
    if 'traffic_density' in df.columns:
        df_new['traffic_level'] = pd.cut(
            df['traffic_density'],
            bins=[0, 0.4, 0.7, 1.0],
            labels=[0, 1, 2]  # 0=faible, 1=moyen, 2=Ã©levÃ©
        ).astype(float)  # type: ignore[attr-defined]
        print("âœ… traffic_level (faible/moyen/Ã©levÃ©)")

    n_new_features = len(df_new.columns) - len(df.columns)
    print(f"\nâœ… {n_new_features} features agrÃ©gÃ©es crÃ©Ã©es")

    return df_new


def create_polynomial_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    CrÃ©e des features polynomiales pour capturer relations non-linÃ©aires.

    Features crÃ©Ã©es :
    - distance_squared : Distance au carrÃ© (relation quadratique)
    - traffic_squared : Trafic au carrÃ©
    - driver_exp_log : Log de l'expÃ©rience (rendements dÃ©croissants)
    """
    print("\n" + "="*70)
    print("ğŸ“ CRÃ‰ATION DES FEATURES POLYNOMIALES")
    print("="*70)

    df_new = df.copy()

    # Distance au carrÃ© (relation quadratique possible)
    if 'distance_km' in df.columns:
        df_new['distance_squared'] = df['distance_km'] ** 2
        print("âœ… distance_squared = distanceÂ²")

    # Trafic au carrÃ©
    if 'traffic_density' in df.columns:
        df_new['traffic_squared'] = df['traffic_density'] ** 2
        print("âœ… traffic_squared = trafficÂ²")

    # Log de l'expÃ©rience driver (rendements dÃ©croissants)
    if 'driver_total_bookings' in df.columns:
        df_new['driver_exp_log'] = np.log1p(df['driver_total_bookings'])  # log(1+x)
        print("âœ… driver_exp_log = log(1 + exp)")

    n_new_features = len(df_new.columns) - len(df.columns)
    print(f"\nâœ… {n_new_features} features polynomiales crÃ©Ã©es")

    return df_new


def normalize_features(
    df: pd.DataFrame,
    exclude_cols: list[str] | None = None
) -> tuple[pd.DataFrame, dict]:
    """
    Normalise les features continues avec StandardScaler et MinMaxScaler.

    Args:
        df: DataFrame Ã  normaliser
        exclude_cols: Colonnes Ã  exclure de la normalisation

    Returns:
        Tuple (DataFrame normalisÃ©, dict des scalers)
    """
    print("\n" + "="*70)
    print("ğŸ“ NORMALISATION DES FEATURES")
    print("="*70)

    if exclude_cols is None:
        exclude_cols = [
            'booking_id', 'driver_id', 'assignment_id', 'company_id',
            'actual_delay_minutes'  # Target
        ]

    df_new = df.copy()
    scalers = {}

    # SÃ©parer features numÃ©riques continues et binaires
    numeric_cols = df_new.select_dtypes(include=[np.number]).columns.tolist()
    numeric_cols = [col for col in numeric_cols if col not in exclude_cols]

    # Features binaires (0/1) - ne pas normaliser
    binary_cols = [col for col in numeric_cols if df_new[col].isin([0.0, 1.0]).all()]  # type: ignore[arg-type]

    # Features continues - normaliser avec StandardScaler
    continuous_cols = [col for col in numeric_cols if col not in binary_cols]

    if len(continuous_cols) > 0:
        print(f"\nğŸ”§ StandardScaler sur {len(continuous_cols)} features continues :")
        for col in continuous_cols[:5]:  # Afficher les 5 premiÃ¨res
            print(f"   - {col}")
        if len(continuous_cols) > 5:
            print(f"   ... et {len(continuous_cols) - 5} autres")

        scaler = StandardScaler()
        df_new[continuous_cols] = scaler.fit_transform(df_new[continuous_cols])
        scalers['standard'] = {
            'scaler': scaler,
            'columns': continuous_cols
        }

    if len(binary_cols) > 0:
        print(f"\nâœ… {len(binary_cols)} features binaires conservÃ©es sans normalisation")

    print(f"\nâœ… Normalisation terminÃ©e : {len(continuous_cols)} features normalisÃ©es")

    return df_new, scalers


def split_train_test(
    df: pd.DataFrame,
    test_size: float = 0.2,
    random_state: int = 42
) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    Split le dataset en train/test avec stratification.

    Args:
        df: DataFrame complet
        test_size: Proportion du test set (dÃ©faut: 0.2 = 20%)
        random_state: Seed pour reproductibilitÃ©

    Returns:
        Tuple (train_df, test_df)
    """
    print("\n" + "="*70)
    print("âœ‚ï¸ SPLIT TRAIN/TEST")
    print("="*70)

    # Stratifier sur les bins de retard pour avoir distribution similaire
    target = 'actual_delay_minutes'
    if target in df.columns:
        # CrÃ©er bins pour stratification (3 bins pour Ã©viter classes trop petites)
        try:
            bins = pd.cut(df[target], bins=3, labels=False, duplicates='drop')

            train_df, test_df = train_test_split(
                df,
                test_size=test_size,
                random_state=random_state,
                stratify=bins
            )
        except ValueError:
            # Si stratification Ã©choue, split sans stratification
            print("âš ï¸ Stratification impossible, split simple")
            train_df, test_df = train_test_split(
                df,
                test_size=test_size,
                random_state=random_state
            )
    else:
        train_df, test_df = train_test_split(
            df,
            test_size=test_size,
            random_state=random_state
        )

    print(f"âœ… Train set : {len(train_df)} Ã©chantillons ({(1-test_size)*100:.0f}%)")
    print(f"âœ… Test set  : {len(test_df)} Ã©chantillons ({test_size*100:.0f}%)")

    # VÃ©rifier distribution du target
    if target in df.columns:
        print("\nğŸ“Š Distribution du target :")
        print(f"   Train - Moyenne : {train_df[target].mean():.2f} min")
        print(f"   Test  - Moyenne : {test_df[target].mean():.2f} min")
        print(f"   DiffÃ©rence      : {abs(train_df[target].mean() - test_df[target].mean()):.2f} min")

    return train_df, test_df


def generate_feature_report(
    original_df: pd.DataFrame,
    engineered_df: pd.DataFrame,
    output_dir: Path
) -> None:
    """GÃ©nÃ¨re un rapport dÃ©taillÃ© du feature engineering."""
    print("\n" + "="*70)
    print("ğŸ“ GÃ‰NÃ‰RATION DU RAPPORT")
    print("="*70)

    report_path = output_dir / 'FEATURE_ENGINEERING_REPORT.md'

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ğŸ”§ RAPPORT DE FEATURE ENGINEERING\n\n")

        # RÃ©sumÃ©
        f.write("## ğŸ“Š RÃ‰SUMÃ‰\n\n")
        f.write(f"- **Features originales** : {len(original_df.columns)}\n")
        f.write(f"- **Features aprÃ¨s engineering** : {len(engineered_df.columns)}\n")
        f.write(f"- **Nouvelles features crÃ©Ã©es** : {len(engineered_df.columns) - len(original_df.columns)}\n\n")

        # Liste des nouvelles features
        new_features = [col for col in engineered_df.columns if col not in original_df.columns]

        f.write("## ğŸ†• NOUVELLES FEATURES CRÃ‰Ã‰ES\n\n")

        # Par catÃ©gorie
        interaction_features = [f for f in new_features if '_x_' in f]
        temporal_features = [f for f in new_features if any(x in f for x in ['is_', 'hour_', 'day_'])]
        aggregated_features = [f for f in new_features if any(x in f for x in ['delay_by_', '_level', '_category'])]
        polynomial_features = [f for f in new_features if any(x in f for x in ['squared', '_log'])]

        f.write("### Interactions\n\n")
        for feat in interaction_features:
            f.write(f"- `{feat}`\n")

        f.write("\n### Temporelles\n\n")
        for feat in temporal_features:
            f.write(f"- `{feat}`\n")

        f.write("\n### AgrÃ©gÃ©es\n\n")
        for feat in aggregated_features:
            f.write(f"- `{feat}`\n")

        f.write("\n### Polynomiales\n\n")
        for feat in polynomial_features:
            f.write(f"- `{feat}`\n")

        f.write("\n---\n\n")
        f.write("**Rapport gÃ©nÃ©rÃ© automatiquement par `feature_engineering.py`**\n")

    print(f"âœ… Rapport sauvegardÃ© : {report_path}")


def main():
    """Point d'entrÃ©e principal."""
    parser = argparse.ArgumentParser(description="Feature engineering pour ML")
    parser.add_argument("--input", type=str, default="data/ml/training_data.csv",
                       help="Fichier CSV d'entrÃ©e")
    parser.add_argument("--output", type=str, default="data/ml/",
                       help="Dossier de sortie")
    parser.add_argument("--test-size", type=float, default=0.2,
                       help="Proportion du test set (dÃ©faut: 0.2)")

    args = parser.parse_args()

    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    print("\n" + "="*70)
    print("ğŸ”§ FEATURE ENGINEERING AVANCÃ‰")
    print("="*70)
    print(f"Input  : {args.input}")
    print(f"Output : {args.output}")
    print("="*70)

    try:
        # 1. Charger donnÃ©es
        print("\nğŸ“‚ Chargement des donnÃ©es...")
        df_original = pd.read_csv(args.input)
        print(f"âœ… {len(df_original)} Ã©chantillons Ã— {len(df_original.columns)} features")

        # 2. Feature engineering
        df = df_original.copy()

        df = create_interaction_features(df)
        df = create_temporal_features(df)
        df = create_aggregated_features(df)
        df = create_polynomial_features(df)

        print(f"\nâœ… Total features : {len(df_original.columns)} â†’ {len(df.columns)} (+{len(df.columns) - len(df_original.columns)})")

        # 3. Split train/test AVANT normalisation (pour Ã©viter data leakage)
        train_df, test_df = split_train_test(df, test_size=args.test_size)

        # 4. Normalisation (fit sur train, transform sur train et test)
        print("\nğŸ”§ Normalisation du train set...")
        train_normalized, scalers = normalize_features(train_df)

        print("\nğŸ”§ Normalisation du test set (avec scalers du train)...")
        test_normalized = test_df.copy()
        if 'standard' in scalers:
            scaler = scalers['standard']['scaler']
            cols = scalers['standard']['columns']
            test_normalized[cols] = scaler.transform(test_df[cols])

        # 5. Sauvegarder
        print("\nğŸ’¾ Sauvegarde des fichiers...")

        # Dataset complet (avant split)
        full_path = output_dir / 'training_data_engineered.csv'
        df.to_csv(full_path, index=False)
        print(f"âœ… Dataset complet : {full_path}")

        # Train/test normalisÃ©s
        train_path = output_dir / 'train_data.csv'
        test_path = output_dir / 'test_data.csv'
        train_normalized.to_csv(train_path, index=False)
        test_normalized.to_csv(test_path, index=False)
        print(f"âœ… Train set : {train_path}")
        print(f"âœ… Test set  : {test_path}")

        # Scalers
        scalers_path = output_dir / 'scalers.json'
        scalers_data = {
            'standard_scaler': {
                'columns': scalers['standard']['columns'],
                'mean': scalers['standard']['scaler'].mean_.tolist(),
                'scale': scalers['standard']['scaler'].scale_.tolist(),
            }
        } if 'standard' in scalers else {}

        with open(scalers_path, 'w') as f:
            json.dump(scalers_data, f, indent=2)
        print(f"âœ… Scalers : {scalers_path}")

        # Rapport
        generate_feature_report(df_original, df, output_dir)

        # MÃ©tadonnÃ©es
        metadata = {
            "original_features": len(df_original.columns),
            "engineered_features": len(df.columns),
            "new_features": len(df.columns) - len(df_original.columns),
            "train_samples": len(train_normalized),
            "test_samples": len(test_normalized),
            "test_size": args.test_size,
            "normalized": True,
        }

        metadata_path = output_dir / 'feature_engineering_metadata.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        print(f"âœ… MÃ©tadonnÃ©es : {metadata_path}")

        print("\n" + "="*70)
        print("âœ… FEATURE ENGINEERING TERMINÃ‰ AVEC SUCCÃˆS !")
        print("="*70)
        print("\nğŸ“Š RÃ©sumÃ© :")
        print(f"   Features  : {len(df_original.columns)} â†’ {len(df.columns)} (+{len(df.columns) - len(df_original.columns)})")
        print(f"   Train set : {len(train_normalized)} ({(1-args.test_size)*100:.0f}%)")
        print(f"   Test set  : {len(test_normalized)} ({args.test_size*100:.0f}%)")
        print("="*70)

    except Exception as e:
        print(f"\nâŒ ERREUR : {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

