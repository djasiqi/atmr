"""
Script d'entraÃ®nement du modÃ¨le ML de prÃ©diction de retards.

EntraÃ®ne un RandomForestRegressor et Ã©value ses performances.

Usage:
    python scripts/ml/train_model.py [--train data/ml/train_data.csv] [--test data/ml/test_data.csv]
"""
# ruff: noqa: T201, N803
# pyright: reportArgumentType=false, reportReturnType=false, reportOperatorIssue=false
# print() est intentionnel dans les scripts ML
# X_train, X_test = convention ML (ignorer N803)

import argparse
import json
import pickle
import sys
import time
from pathlib import Path

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score


def load_datasets(train_path: str, test_path: str) -> tuple[pd.DataFrame, pd.DataFrame]:
    """Charge les datasets train et test."""
    print("\n" + "="*70)
    print("ğŸ“‚ CHARGEMENT DES DATASETS")
    print("="*70)

    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    print(f"âœ… Train: {train_df.shape}")
    print(f"âœ… Test:  {test_df.shape}")

    return train_df, test_df


def prepare_features_and_target(
    df: pd.DataFrame,
    target_col: str = 'actual_delay_minutes',
    exclude_cols: list[str] | None = None
) -> tuple[pd.DataFrame, pd.Series]:
    """
    SÃ©pare les features et le target.

    Args:
        df: DataFrame complet
        target_col: Nom de la colonne target
        exclude_cols: Colonnes Ã  exclure des features (IDs, etc.)

    Returns:
        Tuple (X features, y target)
    """
    if exclude_cols is None:
        exclude_cols = [
            'booking_id', 'driver_id', 'assignment_id', 'company_id',
            target_col
        ]

    # Features = toutes colonnes sauf target et IDs
    feature_cols = [col for col in df.columns if col not in exclude_cols]

    X = df[feature_cols]
    y = df[target_col]

    print("\nğŸ“Š Features prÃ©parÃ©es:")
    print(f"   Features: {X.shape[1]}")
    print(f"   Ã‰chantillons: {X.shape[0]}")
    print(f"   Target: {target_col}")

    return X, y  # type: ignore[return-value]


def train_random_forest(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    n_estimators: int = 100,
    max_depth: int | None = None,
    random_state: int = 42
) -> RandomForestRegressor:
    """
    EntraÃ®ne un Random Forest Regressor.

    Args:
        X_train: Features d'entraÃ®nement
        y_train: Target d'entraÃ®nement
        n_estimators: Nombre d'arbres (dÃ©faut: 100)
        max_depth: Profondeur max des arbres (None = illimitÃ©)
        random_state: Seed pour reproductibilitÃ©

    Returns:
        ModÃ¨le entraÃ®nÃ©
    """
    print("\n" + "="*70)
    print("ğŸŒ³ ENTRAÃNEMENT RANDOM FOREST")
    print("="*70)
    print("ParamÃ¨tres:")
    print(f"   n_estimators: {n_estimators}")
    print(f"   max_depth: {max_depth or 'IllimitÃ©'}")
    print(f"   random_state: {random_state}")
    print("="*70)

    model = RandomForestRegressor(
        n_estimators=n_estimators,
        max_depth=max_depth,
        random_state=random_state,
        n_jobs=-1,  # Utiliser tous les CPUs
        verbose=0
    )

    print("\nâ±ï¸ EntraÃ®nement en cours...")
    start_time = time.time()

    model.fit(X_train, y_train)

    elapsed = time.time() - start_time
    print(f"âœ… EntraÃ®nement terminÃ© en {elapsed:.2f}s")

    return model


def evaluate_model(
    model: RandomForestRegressor,
    X_train: pd.DataFrame,
    y_train: pd.Series,
    X_test: pd.DataFrame,
    y_test: pd.Series
) -> dict:
    """
    Ã‰value les performances du modÃ¨le.

    MÃ©triques calculÃ©es:
    - MAE (Mean Absolute Error)
    - RMSE (Root Mean Squared Error)
    - RÂ² score
    - Temps de prÃ©diction

    Returns:
        Dict avec toutes les mÃ©triques
    """
    print("\n" + "="*70)
    print("ğŸ“Š Ã‰VALUATION DU MODÃˆLE")
    print("="*70)

    # PrÃ©dictions
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    # MÃ©triques Train
    train_mae = mean_absolute_error(y_train, y_train_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))
    train_r2 = r2_score(y_train, y_train_pred)

    # MÃ©triques Test
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))
    test_r2 = r2_score(y_test, y_test_pred)

    # Temps de prÃ©diction (moyenne sur 100 prÃ©dictions)
    sample = X_test.iloc[:100] if len(X_test) >= 100 else X_test
    start = time.time()
    for _ in range(100):
        model.predict(sample)
    avg_pred_time = (time.time() - start) / 100 * 1000  # en ms

    print("\nğŸ¯ MÃ‰TRIQUES TRAIN SET:")
    print(f"   MAE  : {train_mae:.2f} min")
    print(f"   RMSE : {train_rmse:.2f} min")
    print(f"   RÂ²   : {train_r2:.4f}")

    print("\nğŸ¯ MÃ‰TRIQUES TEST SET:")
    print(f"   MAE  : {test_mae:.2f} min {'âœ…' if test_mae < 5.0 else 'âš ï¸'} (cible: < 5 min)")
    print(f"   RMSE : {test_rmse:.2f} min")
    print(f"   RÂ²   : {test_r2:.4f} {'âœ…' if test_r2 > 0.6 else 'âš ï¸'} (cible: > 0.6)")

    print("\nâš¡ PERFORMANCE:")
    print(f"   Temps prÃ©diction: {avg_pred_time:.2f}ms {'âœ…' if avg_pred_time < 100 else 'âš ï¸'} (cible: < 100ms)")

    # Overfitting check
    overfitting = train_r2 - test_r2
    print("\nğŸ” OVERFITTING CHECK:")
    print(f"   Diff RÂ² (train - test): {overfitting:.4f}")
    if overfitting > 0.15:
        print("   âš ï¸ Overfitting dÃ©tectÃ© (diff > 0.15)")
    else:
        print("   âœ… Pas d'overfitting significatif")

    return {
        "train": {
            "mae": float(train_mae),
            "rmse": float(train_rmse),
            "r2": float(train_r2),
        },
        "test": {
            "mae": float(test_mae),
            "rmse": float(test_rmse),
            "r2": float(test_r2),
        },
        "prediction_time_ms": float(avg_pred_time),
        "overfitting": float(overfitting),
    }


def cross_validate_model(
    model: RandomForestRegressor,
    X: pd.DataFrame,
    y: pd.Series,
    cv: int = 5
) -> dict:
    """
    Validation croisÃ©e pour estimer la robustesse du modÃ¨le.

    Args:
        model: ModÃ¨le Ã  valider
        X: Features
        y: Target
        cv: Nombre de folds (dÃ©faut: 5)

    Returns:
        Dict avec scores CV
    """
    print("\n" + "="*70)
    print(f"ğŸ”„ VALIDATION CROISÃ‰E ({cv}-FOLD CV)")
    print("="*70)

    print("\nâ±ï¸ Cross-validation en cours...")

    # Scorer sur MAE (nÃ©gatif par convention sklearn)
    cv_mae_scores = -cross_val_score(
        model, X, y,
        cv=cv,
        scoring='neg_mean_absolute_error',
        n_jobs=-1
    )

    # Scorer sur RÂ²
    cv_r2_scores = cross_val_score(
        model, X, y,
        cv=cv,
        scoring='r2',
        n_jobs=-1
    )

    print(f"\nğŸ“Š RÃ©sultats {cv}-Fold CV:")
    print("\n   MAE:")
    print(f"      Moyenne : {cv_mae_scores.mean():.2f} min")
    print(f"      Std     : Â±{cv_mae_scores.std():.2f} min")
    print(f"      Min/Max : {cv_mae_scores.min():.2f} / {cv_mae_scores.max():.2f} min")

    print("\n   RÂ²:")
    print(f"      Moyenne : {cv_r2_scores.mean():.4f}")
    print(f"      Std     : Â±{cv_r2_scores.std():.4f}")
    print(f"      Min/Max : {cv_r2_scores.min():.4f} / {cv_r2_scores.max():.4f}")

    # StabilitÃ©
    cv_stability = cv_r2_scores.std()
    print("\nğŸ” STABILITÃ‰:")
    print(f"   Std RÂ² = {cv_stability:.4f}")
    if cv_stability < 0.05:
        print("   âœ… ModÃ¨le trÃ¨s stable (std < 0.05)")
    elif cv_stability < 0.10:
        print("   âœ… ModÃ¨le stable (std < 0.10)")
    else:
        print("   âš ï¸ ModÃ¨le instable (std > 0.10)")

    return {
        "cv_mae_mean": float(cv_mae_scores.mean()),
        "cv_mae_std": float(cv_mae_scores.std()),
        "cv_r2_mean": float(cv_r2_scores.mean()),
        "cv_r2_std": float(cv_r2_scores.std()),
        "stability": float(cv_stability),
    }


def analyze_feature_importance(
    model: RandomForestRegressor,
    feature_names: list[str],
    top_n: int = 15
) -> pd.DataFrame:
    """
    Analyse l'importance des features.

    Args:
        model: ModÃ¨le entraÃ®nÃ©
        feature_names: Noms des features
        top_n: Nombre de top features Ã  afficher

    Returns:
        DataFrame avec importances triÃ©es
    """
    print("\n" + "="*70)
    print(f"ğŸ¯ IMPORTANCE DES FEATURES (TOP {top_n})")
    print("="*70)

    importances = model.feature_importances_
    feature_importance = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)

    print(f"\n   {'Rang':<5} {'Feature':<30} {'Importance':<12} {'Cumul %'}")
    print("   " + "-"*65)

    cumul = 0.0
    for i, row in feature_importance.head(top_n).iterrows():  # type: ignore[attr-defined]
        cumul += row['importance']
        bar = "â–ˆ" * int(row['importance'] * 50)
        idx = int(i) + 1 if isinstance(i, (int, float, np.integer)) else 1  # type: ignore[arg-type]
        print(f"   {idx:<5} {row['feature']:<30} {row['importance']:.4f}  {bar:10s} {cumul*100:.1f}%")

    print(f"\nâœ… Top {top_n} features expliquent {cumul*100:.1f}% de la variance")

    return feature_importance


def save_model(
    model: RandomForestRegressor,
    feature_names: list[str],
    metrics: dict,
    output_path: str
) -> None:
    """
    Sauvegarde le modÃ¨le et ses mÃ©tadonnÃ©es.

    Args:
        model: ModÃ¨le entraÃ®nÃ©
        feature_names: Liste des features utilisÃ©es
        metrics: MÃ©triques de performance
        output_path: Chemin de sauvegarde
    """
    print("\n" + "="*70)
    print("ğŸ’¾ SAUVEGARDE DU MODÃˆLE")
    print("="*70)

    output_file = Path(output_path)
    output_file.parent.mkdir(parents=True, exist_ok=True)

    # DonnÃ©es Ã  sauvegarder
    model_data = {
        "model": model,
        "feature_names": feature_names,
        "metrics": metrics,
        "n_features": len(feature_names),
        "trained_at": pd.Timestamp.now().isoformat(),
    }

    # Sauvegarder en pickle
    with open(output_file, 'wb') as f:
        pickle.dump(model_data, f)

    file_size = output_file.stat().st_size / 1024  # en KB

    print(f"âœ… ModÃ¨le sauvegardÃ©: {output_file}")
    print(f"   Taille: {file_size:.1f} KB")
    print(f"   Features: {len(feature_names)}")
    print(f"   MAE (test): {metrics['test']['mae']:.2f} min")
    print(f"   RÂ² (test): {metrics['test']['r2']:.4f}")


def generate_training_report(
    metrics: dict,
    cv_results: dict,
    feature_importance: pd.DataFrame,
    output_dir: Path
) -> None:
    """GÃ©nÃ¨re un rapport d'entraÃ®nement complet."""
    print("\n" + "="*70)
    print("ğŸ“ GÃ‰NÃ‰RATION DU RAPPORT")
    print("="*70)

    report_path = output_dir / 'TRAINING_REPORT.md'

    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# ğŸ¤– RAPPORT D'ENTRAÃNEMENT DU MODÃˆLE ML\n\n")

        # MÃ©triques
        f.write("## ğŸ“Š MÃ‰TRIQUES DE PERFORMANCE\n\n")
        f.write("### Test Set\n\n")
        f.write(f"- **MAE** : {metrics['test']['mae']:.2f} min")
        f.write(" âœ…\n" if metrics['test']['mae'] < 5.0 else " âš ï¸\n")
        f.write(f"- **RMSE** : {metrics['test']['rmse']:.2f} min\n")
        f.write(f"- **RÂ²** : {metrics['test']['r2']:.4f}")
        f.write(" âœ…\n" if metrics['test']['r2'] > 0.6 else " âš ï¸\n")
        f.write(f"- **Temps prÃ©diction** : {metrics['prediction_time_ms']:.2f}ms")
        f.write(" âœ…\n" if metrics['prediction_time_ms'] < 100 else " âš ï¸\n")

        # Validation croisÃ©e
        f.write("\n### Validation CroisÃ©e (5-Fold)\n\n")
        f.write(f"- **MAE (CV)** : {cv_results['cv_mae_mean']:.2f} Â± {cv_results['cv_mae_std']:.2f} min\n")
        f.write(f"- **RÂ² (CV)** : {cv_results['cv_r2_mean']:.4f} Â± {cv_results['cv_r2_std']:.4f}\n")
        f.write(f"- **StabilitÃ©** : {cv_results['stability']:.4f}")
        f.write(" âœ…\n" if cv_results['stability'] < 0.10 else " âš ï¸\n")

        # Overfitting
        f.write("\n### Overfitting Check\n\n")
        f.write(f"- **Diff RÂ² (train - test)** : {metrics['overfitting']:.4f}\n")
        if metrics['overfitting'] > 0.15:
            f.write("- âš ï¸ **Overfitting dÃ©tectÃ©**\n")
        else:
            f.write("- âœ… **Pas d'overfitting significatif**\n")

        # Top features
        f.write("\n## ğŸ¯ TOP 10 FEATURES\n\n")
        f.write("| Rang | Feature | Importance |\n")
        f.write("|------|---------|------------|\n")

        for i, row in feature_importance.head(10).iterrows():  # type: ignore[attr-defined]
            idx = int(i) + 1 if isinstance(i, (int, float, np.integer)) else 1  # type: ignore[arg-type]
            f.write(f"| {idx} | `{row['feature']}` | {row['importance']:.4f} |\n")

        f.write("\n---\n\n")
        f.write("**Rapport gÃ©nÃ©rÃ© automatiquement par `train_model.py`**\n")

    print(f"âœ… Rapport sauvegardÃ©: {report_path}")


def main():
    """Point d'entrÃ©e principal."""
    parser = argparse.ArgumentParser(description="EntraÃ®nement modÃ¨le ML")
    parser.add_argument("--train", type=str, default="data/ml/train_data.csv",
                       help="Fichier CSV train")
    parser.add_argument("--test", type=str, default="data/ml/test_data.csv",
                       help="Fichier CSV test")
    parser.add_argument("--output", type=str, default="data/ml/models/delay_predictor.pkl",
                       help="Fichier de sortie du modÃ¨le")
    parser.add_argument("--n-estimators", type=int, default=100,
                       help="Nombre d'arbres (dÃ©faut: 100)")
    parser.add_argument("--max-depth", type=int, default=None,
                       help="Profondeur max (dÃ©faut: illimitÃ©)")

    args = parser.parse_args()

    print("\n" + "="*70)
    print("ğŸ¤– ENTRAÃNEMENT MODÃˆLE ML - PRÃ‰DICTION DE RETARDS")
    print("="*70)
    print(f"Train  : {args.train}")
    print(f"Test   : {args.test}")
    print(f"Output : {args.output}")
    print("="*70)

    try:
        # 1. Charger datasets
        train_df, test_df = load_datasets(args.train, args.test)

        # 2. PrÃ©parer features et target
        X_train, y_train = prepare_features_and_target(train_df)
        X_test, y_test = prepare_features_and_target(test_df)

        # 3. EntraÃ®ner modÃ¨le
        model = train_random_forest(
            X_train, y_train,
            n_estimators=args.n_estimators,
            max_depth=args.max_depth
        )

        # 4. Ã‰valuer
        metrics = evaluate_model(model, X_train, y_train, X_test, y_test)

        # 5. Validation croisÃ©e
        cv_results = cross_validate_model(model, X_train, y_train, cv=5)

        # 6. Feature importance
        feature_importance = analyze_feature_importance(model, X_train.columns.tolist(), top_n=15)

        # 7. Sauvegarder modÃ¨le
        save_model(model, X_train.columns.tolist(), metrics, args.output)

        # 8. Rapport
        output_dir = Path(args.output).parent
        generate_training_report(metrics, cv_results, feature_importance, output_dir)

        # 9. MÃ©tadonnÃ©es
        metadata = {
            "train_samples": len(X_train),
            "test_samples": len(X_test),
            "n_features": len(X_train.columns),
            "metrics": metrics,
            "cv_results": cv_results,
            "top_features": feature_importance.head(10).to_dict('records'),
        }

        metadata_path = output_dir / 'training_metadata.json'
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)

        print(f"âœ… MÃ©tadonnÃ©es: {metadata_path}")

        # RÃ©sumÃ© final
        print("\n" + "="*70)
        print("âœ… ENTRAÃNEMENT TERMINÃ‰ AVEC SUCCÃˆS !")
        print("="*70)
        print("\nğŸ¯ Performance Test Set:")
        print(f"   MAE  : {metrics['test']['mae']:.2f} min {'âœ…' if metrics['test']['mae'] < 5.0 else 'âŒ'}")
        print(f"   RÂ²   : {metrics['test']['r2']:.4f} {'âœ…' if metrics['test']['r2'] > 0.6 else 'âŒ'}")
        print(f"   Temps: {metrics['prediction_time_ms']:.2f}ms {'âœ…' if metrics['prediction_time_ms'] < 100 else 'âŒ'}")

        print("\nğŸ“Š Validation CroisÃ©e:")
        print(f"   MAE (CV): {cv_results['cv_mae_mean']:.2f} Â± {cv_results['cv_mae_std']:.2f} min")
        print(f"   RÂ² (CV) : {cv_results['cv_r2_mean']:.4f} Â± {cv_results['cv_r2_std']:.4f}")

        print("\n" + "="*70)

        # Check objectifs
        success = (
            metrics['test']['mae'] < 5.0 and
            metrics['test']['r2'] > 0.6 and
            metrics['prediction_time_ms'] < 100
        )

        if success:
            print("ğŸ‰ TOUS LES OBJECTIFS ATTEINTS !")
            print("="*70 + "\n")
            sys.exit(0)
        else:
            print("âš ï¸ Certains objectifs non atteints")
            print("   â†’ ConsidÃ©rer fine-tuning hyperparamÃ¨tres")
            print("="*70 + "\n")
            sys.exit(0)

    except Exception as e:
        print(f"\nâŒ ERREUR : {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

