#!/usr/bin/env python3
"""
Tests complets pour le syst√®me Safety Guards.

Teste la d√©tection d'anomalies, les rollbacks automatiques,
et l'int√©gration avec le syst√®me de dispatch RL.

Auteur: ATMR Project - RL Team
Date: 21 octobre 2025
"""

import logging
from datetime import UTC, datetime, timedelta
from unittest.mock import Mock, patch

import pytest

# Import conditionnel pour √©viter les erreurs si les modules ne sont pas disponibles
try:
    from services.safety_guards import SafetyGuards, SafetyThresholds, get_safety_guards
except ImportError:
    SafetyGuards = None
    SafetyThresholds = None
    get_safety_guards = None


class TestSafetyThresholds:
    """Tests pour SafetyThresholds."""
    
    def test_default_thresholds(self):
        """Test les seuils par d√©faut."""
        if SafetyThresholds is None:
            pytest.skip("SafetyThresholds non disponible")
        
        thresholds = SafetyThresholds()
        
        assert thresholds.max_delay_minutes == 30.0
        assert thresholds.invalid_action_rate == 0.03
        assert thresholds.min_completion_rate == 0.90
        assert thresholds.max_driver_load == 12
        assert thresholds.min_driver_utilization == 0.60
        assert thresholds.max_avg_distance_km == 25.0
        assert thresholds.min_rl_confidence == 0.70
    
    def test_custom_thresholds(self):
        """Test les seuils personnalis√©s."""
        if SafetyThresholds is None:
            pytest.skip("SafetyThresholds non disponible")
        
        thresholds = SafetyThresholds(
            max_delay_minutes=45.0,
            invalid_action_rate=0.05,
            min_completion_rate=0.95
        )
        
        assert thresholds.max_delay_minutes == 45.0
        assert thresholds.invalid_action_rate == 0.05
        assert thresholds.min_completion_rate == 0.95


class TestSafetyGuards:
    """Tests pour SafetyGuards."""
    
    @pytest.fixture
    def safety_guards(self):
        """Cr√©e une instance de SafetyGuards pour les tests."""
        if SafetyGuards is None:
            pytest.skip("SafetyGuards non disponible")
        
        return SafetyGuards()
    
    @pytest.fixture
    def safe_dispatch_result(self):
        """R√©sultat de dispatch s√ªr."""
        return {
            "max_delay_minutes": 15.0,
            "avg_delay_minutes": 8.0,
            "completion_rate": 0.95,
            "invalid_action_rate": 0.01,
            "driver_loads": [3, 4, 5, 2, 6],
            "avg_distance_km": 12.0,
            "max_distance_km": 20.0,
            "total_distance_km": 60.0
        }
    
    @pytest.fixture
    def unsafe_dispatch_result(self):
        """R√©sultat de dispatch dangereux."""
        return {
            "max_delay_minutes": 45.0,  # Dangereux: > 30 min
            "avg_delay_minutes": 25.0,
            "completion_rate": 0.80,  # Dangereux: < 0.90
            "invalid_action_rate": 0.05,  # Dangereux: > 0.03
            "driver_loads": [15, 2, 1],  # Dangereux: max > 12
            "avg_distance_km": 30.0,  # Dangereux: > 25 km
            "max_distance_km": 60.0,  # Dangereux: > 50 km
            "total_distance_km": 150.0
        }
    
    @pytest.fixture
    def rl_metadata_safe(self):
        """M√©tadonn√©es RL s√ªres."""
        return {
            "confidence": 0.85,
            "uncertainty": 0.15,
            "decision_time_ms": 35,
            "q_value_variance": 0.1,
            "episode_length": 100
        }
    
    @pytest.fixture
    def rl_metadata_unsafe(self):
        """M√©tadonn√©es RL dangereuses."""
        return {
            "confidence": 0.60,  # Dangereux: < 0.70
            "uncertainty": 0.40,  # Dangereux: > 0.25
            "decision_time_ms": 150,  # Dangereux: > 100 ms
            "q_value_variance": 0.30,  # Dangereux: > 0.20
            "episode_length": 30  # Dangereux: < 50
        }
    
    def test_check_safe_dispatch(self, ____________________________________________________________________________________________________safety_guards, safe_dispatch_result, rl_metadata_safe):
        """Test la v√©rification d'un dispatch s√ªr."""
        is_safe, result = safety_guards.check_dispatch_result(
            safe_dispatch_result, rl_metadata_safe
        )
        
        assert is_safe is True
        assert result["is_safe"] is True
        assert result["violation_count"] == 0
        assert "timestamp" in result
    
    def test_check_unsafe_dispatch(self, ____________________________________________________________________________________________________safety_guards, unsafe_dispatch_result, rl_metadata_unsafe):
        """Test la v√©rification d'un dispatch dangereux."""
        is_safe, result = safety_guards.check_dispatch_result(
            unsafe_dispatch_result, rl_metadata_unsafe
        )
        
        assert is_safe is False
        assert result["is_safe"] is False
        assert result["violation_count"] > 0
        assert "timestamp" in result
        
        # V√©rifier que des violations sp√©cifiques sont d√©tect√©es
        checks = result["checks"]
        assert checks["max_delay_ok"] is False
        assert checks["completion_rate_ok"] is False
        assert checks["invalid_actions_ok"] is False
        assert checks["driver_load_ok"] is False
        assert checks["rl_confidence_ok"] is False
    
    def test_check_without_rl_metadata(self, ____________________________________________________________________________________________________safety_guards, safe_dispatch_result):
        """Test la v√©rification sans m√©tadonn√©es RL."""
        is_safe, result = safety_guards.check_dispatch_result(
            safe_dispatch_result, None
        )
        
        assert is_safe is True
        assert result["is_safe"] is True
    
    def test_extract_metrics(self, safety_guards):
        """Test l'extraction des m√©triques."""
        dispatch_result = {
            "max_delay_minutes": 20.0,
            "completion_rate": 0.92,
            "driver_loads": [4, 5, 3]
        }
        
        rl_metadata = {
            "confidence": 0.80,
            "uncertainty": 0.20,
            "decision_time_ms": 40
        }
        
        metrics = safety_guards._extract_metrics(dispatch_result, rl_metadata)
        
        assert metrics["max_delay_minutes"] == 20.0
        assert metrics["completion_rate"] == 0.92
        assert metrics["rl_confidence"] == 0.80
        assert metrics["decision_time_ms"] == 40
        assert metrics["max_driver_load"] == 5
        assert metrics["avg_driver_load"] == 4.0
    
    def test_perform_safety_checks(self, safety_guards):
        """Test l'ex√©cution des checks de s√©curit√©."""
        # M√©triques s√ªres
        safe_metrics = {
            "max_delay_minutes": 15.0,
            "invalid_action_rate": 0.01,
            "completion_rate": 0.95,
            "max_driver_load": 8,
            "avg_driver_load": 6.0,
            "max_distance_km": 20.0,
            "avg_distance_km": 15.0,
            "rl_confidence": 0.85,
            "rl_uncertainty": 0.15,
            "decision_time_ms": 35,
            "episode_length": 100
        }
        
        checks = safety_guards._perform_safety_checks(safe_metrics)
        
        # Tous les checks doivent passer
        assert all(checks.values())
        assert checks["max_delay_ok"] is True
        assert checks["completion_rate_ok"] is True
        assert checks["driver_load_ok"] is True
        assert checks["rl_confidence_ok"] is True
    
    def test_violation_recording(self, ____________________________________________________________________________________________________safety_guards, unsafe_dispatch_result, rl_metadata_unsafe):
        """Test l'enregistrement des violations."""
        # V√©rifier que l'historique est vide au d√©but
        assert len(safety_guards.violation_history) == 0
        
        # Effectuer un check dangereux
        safety_guards.check_dispatch_result(unsafe_dispatch_result, rl_metadata_unsafe)
        
        # V√©rifier qu'une violation a √©t√© enregistr√©e
        assert len(safety_guards.violation_history) == 1
        
        violation = safety_guards.violation_history[0]
        assert "timestamp" in violation
        assert "violations" in violation
        assert "metrics" in violation
        assert "severity" in violation
        assert len(violation["violations"]) > 0
    
    def test_severity_calculation(self, safety_guards):
        """Test le calcul de la s√©v√©rit√©."""
        # Test avec peu de violations
        few_violations = {"check1": True, "check2": False, "check3": True}
        severity = safety_guards._calculate_severity(few_violations, {})
        assert severity == "LOW"
        
        # Test avec plusieurs violations
        many_violations = {"check1": False, "check2": False, "check3": False, "check4": False, "check5": False}
        severity = safety_guards._calculate_severity(many_violations, {})
        assert severity == "CRITICAL"
    
    def test_should_rollback(self, safety_guards):
        """Test la logique de rollback."""
        # Au d√©but, pas de rollback
        assert safety_guards.should_rollback() is False
        
        # Ajouter des violations r√©centes
        for i in range(5):
            violation = {
                "timestamp": datetime.now(UTC) - timedelta(minutes=i),
                "violations": ["max_delay_ok"],
                "metrics": {},
                "severity": "HIGH"
            }
            safety_guards.violation_history.append(violation)
        
        # Maintenant, rollback recommand√©
        assert safety_guards.should_rollback() is True
    
    def test_health_status(self, safety_guards):
        """Test le statut de sant√©."""
        status = safety_guards.get_health_status()
        
        assert "status" in status
        assert "total_violations" in status
        assert "recent_violations_24h" in status
        assert "rollback_count" in status
        assert "thresholds" in status
        assert "timestamp" in status
        assert status["status"] == "healthy"
    
    def test_update_thresholds(self, safety_guards):
        """Test la mise √† jour des seuils."""
        new_thresholds = {
            "max_delay_minutes": 45.0,
            "min_completion_rate": 0.95
        }
        
        safety_guards.update_thresholds(new_thresholds)
        
        assert safety_guards.thresholds.max_delay_minutes == 45.0
        assert safety_guards.thresholds.min_completion_rate == 0.95
    
    def test_error_handling(self, safety_guards):
        """Test la gestion d'erreurs."""
        # Test avec des donn√©es invalides
        invalid_result = {"invalid": "data"}
        
        is_safe, result = safety_guards.check_dispatch_result(invalid_result, None)
        
        assert is_safe is False
        assert "error" in result


class TestSafetyGuardsIntegration:
    """Tests d'int√©gration pour Safety Guards."""
    
    def test_get_safety_guards_singleton(self):
        """Test le singleton Safety Guards."""
        if get_safety_guards is None:
            pytest.skip("get_safety_guards non disponible")
        
        guards1 = get_safety_guards()
        guards2 = get_safety_guards()
        
        assert guards1 is guards2
    
    @patch("services.safety_guards.logging")
    def test_logging_integration(self, mock_logging):
        """Test l'int√©gration avec le syst√®me de logging."""
        if SafetyGuards is None:
            pytest.skip("SafetyGuards non disponible")
        
        _guards = SafetyGuards()
        
        # V√©rifier que le logging a √©t√© configur√©
        mock_logging.getLogger.assert_called()
    
    def test_performance_under_load(self, ____________________________________________________________________________________________________safety_guards, safe_dispatch_result, rl_metadata_safe):
        """Test les performances sous charge."""
        import time
        
        start_time = time.time()
        
        # Effectuer 100 checks
        for _ in range(100):
            safety_guards.check_dispatch_result(safe_dispatch_result, rl_metadata_safe)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # V√©rifier que chaque check prend moins de 10ms en moyenne
        avg_time_per_check = total_time / 100
        assert avg_time_per_check < 0.01  # 10ms
    
    def test_memory_usage(self, safety_guards):
        """Test l'utilisation m√©moire."""
        import sys
        
        initial_size = sys.getsizeof(safety_guards.violation_history)
        
        # Ajouter 1000 violations
        for i in range(1000):
            violation = {
                "timestamp": datetime.now(UTC),
                "violations": ["test_violation"],
                "metrics": {"test": i},
                "severity": "LOW"
            }
            safety_guards.violation_history.append(violation)
        
        # V√©rifier que la taille n'explose pas (rotation automatique)
        final_size = sys.getsizeof(safety_guards.violation_history)
        
        # La taille ne devrait pas augmenter de plus de 50% gr√¢ce √† la rotation
        assert final_size < initial_size * 1.5


def run_safety_guards_tests():
    """Ex√©cute tous les tests Safety Guards."""
    print("üõ°Ô∏è Ex√©cution des tests Safety Guards")
    
    # Tests de base
    test_classes = [
        TestSafetyThresholds,
        TestSafetyGuards,
        TestSafetyGuardsIntegration
    ]
    
    total_tests = 0
    passed_tests = 0
    
    for test_class in test_classes:
        print("\nüìã Tests {test_class.__name__}")
        
        # Cr√©er une instance de la classe de test
        test_instance = test_class()
        
        # Ex√©cuter les m√©thodes de test
        for method_name in dir(test_instance):
            if method_name.startswith("test_"):
                total_tests += 1
                try:
                    method = getattr(test_instance, method_name)
                    method()
                    print("  ‚úÖ {method_name}")
                    passed_tests += 1
                except Exception:
                    print("  ‚ùå {method_name}: {e}")
    
    print("\nüìä R√©sultats des tests Safety Guards:")
    print("  Tests ex√©cut√©s: {total_tests}")
    print("  Tests r√©ussis: {passed_tests}")
    print("  Taux de succ√®s: {passed_tests/total_tests*100" if total_tests > 0 else "  Taux de succ√®s: 0%")
    
    return passed_tests, total_tests


if __name__ == "__main__":
    run_safety_guards_tests()
