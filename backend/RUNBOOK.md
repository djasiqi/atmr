# Runbook ‚Äî Gestion des catastrophes

Ce document d√©crit les proc√©dures d'astreinte pour g√©rer les sc√©narios de catastrophe.

## Table des mati√®res

1. [OSRM Down](#osrm-down)
2. [DB Read-Only](#db-read-only)
3. [Pic de charge](#pic-de-charge)
4. [R√©seau flaky](#r√©seau-flaky)
5. [Killswitch](#killswitch)
6. [Healthchecks Kubernetes](#healthchecks-kubernetes)

---

## OSRM Down

### Sympt√¥mes

- Erreurs 502/503 sur les appels OSRM
- Timeouts sur les calculs de distance/temps
- M√©triques `osrm_availability` = 0%

### Actions imm√©diates (0-5 min)

1. **V√©rifier l'√©tat du service**

   ```bash
   docker-compose ps osrm
   docker-compose logs osrm --tail 50
   ```

2. **V√©rifier la connectivit√©**

   ```bash
   curl http://osrm:5000/health
   ```

3. **Si OSRM est down**
   - Le syst√®me doit utiliser le cache OSRM
   - Les nouvelles requ√™tes peuvent √©chouer gracieusement
   - **Ne PAS red√©marrer imm√©diatement** (attendre diagnostic)

### Actions de r√©cup√©ration (5-15 min)

1. **Red√©marrer OSRM**

   ```bash
   docker-compose restart osrm
   ```

2. **V√©rifier que le service remonte**

   ```bash
   # Attendre 30s
   docker-compose logs osrm -f
   ```

3. **V√©rifier que les requ√™tes reprennent**
   ```bash
   # V√©rifier les m√©triques dans Flower ou logs
   curl http://api:5000/api/health
   ```

### RTO (Recovery Time Objective)

- **Objectif**: ‚â§ 30 secondes apr√®s restauration OSRM
- **Acceptable**: ‚â§ 2 minutes

---

## DB Read-Only

### Sympt√¥mes

- Erreurs SQL "read-only" dans les logs
- √âcritures √©chouent (POST/PUT/PATCH)
- Lectures fonctionnent normalement

### Actions imm√©diates (0-5 min)

1. **V√©rifier l'√©tat de la DB**

   ```bash
   docker-compose exec postgres psql -U postgres -d atmr -c "SHOW transaction_read_only;"
   ```

2. **V√©rifier les logs**

   ```bash
   docker-compose logs postgres --tail 50
   ```

3. **Si DB est en read-only**
   - Les lectures continuent (syst√®me partiellement op√©rationnel)
   - Les √©critures sont rejet√©es (erreur HTTP 503)
   - **Activer le mode maintenance** si n√©cessaire (voir Killswitch)

### Actions de r√©cup√©ration (5-15 min)

1. **Passer la DB en read-write**

   ```bash
   docker-compose exec postgres psql -U postgres -d atmr -c "ALTER DATABASE atmr SET default_transaction_read_only = off;"
   ```

2. **V√©rifier que les √©critures reprennent**
   ```bash
   # Tester un endpoint POST/PUT
   curl -X POST http://api:5000/api/test-endpoint
   ```

### RTO

- **Objectif**: ‚â§ 5 minutes
- **Acceptable**: ‚â§ 10 minutes

---

## Pic de charge

### Sympt√¥mes

- Latence √©lev√©e (P95 > 5s)
- Taux d'erreur > 5%
- CPU/Memory proches de 100%
- Files d'attente Celery qui s'accumulent

### Actions imm√©diates (0-5 min)

1. **V√©rifier les m√©triques**

   ```bash
   # V√©rifier les ressources
   docker stats

   # V√©rifier les logs d'erreur
   docker-compose logs api --tail 100 | grep ERROR
   ```

2. **Activer le rate limiting** (si disponible)

   ```bash
   # Via variable d'environnement ou config
   export ENABLE_RATE_LIMITING=true
   docker-compose restart api
   ```

3. **Scaler les services** (si possible)
   ```bash
   docker-compose up -d --scale celery-worker=5
   ```

### Actions de r√©cup√©ration (5-30 min)

1. **Identifier la source du pic**

   - V√©rifier les logs pour patterns
   - V√©rifier les m√©triques d'utilisation

2. **Optimiser**

   - Augmenter les ressources (CPU/Memory)
   - Ajouter des workers Celery
   - Activer le cache si disponible

3. **Si le pic persiste**
   - Activer le mode d√©grad√© (voir Killswitch)
   - R√©duire la fonctionnalit√© non-critique

### RTO

- **Objectif**: Syst√®me reste op√©rationnel avec ‚â• 95% de succ√®s
- **Acceptable**: ‚â• 90% de succ√®s, latence P95 < 10s

---

## R√©seau flaky

### Sympt√¥mes

- Timeouts intermittents
- Erreurs de connexion
- Latence variable
- Perte de paquets

### Actions imm√©diates (0-5 min)

1. **V√©rifier la connectivit√©**

   ```bash
   # Ping test
   ping -c 10 osrm
   ping -c 10 postgres
   ping -c 10 redis
   ```

2. **V√©rifier les m√©triques r√©seau**

   ```bash
   # V√©rifier packet loss
   # V√©rifier latence dans les logs
   docker-compose logs api | grep -i timeout
   ```

3. **V√©rifier les retries**
   - Les services doivent retry automatiquement
   - V√©rifier que les backoffs exponentiels sont actifs

### Actions de r√©cup√©ration (5-30 min)

1. **Si r√©seau local flaky**
   - V√©rifier les liens Docker
   - Red√©marrer les r√©seaux si n√©cessaire
2. **Si r√©seau externe flaky**
   - V√©rifier les providers (OSRM externe, APIs tierces)
   - Activer les timeouts plus longs si n√©cessaire
   - Activer le cache si disponible

### RTO

- **Objectif**: Pas de perte de donn√©es, retries automatiques
- **Acceptable**: D√©gradation gracieuse, quelques erreurs temporaires

---

## Killswitch

Le killswitch permet d'activer rapidement un mode de maintenance ou d√©grad√©.

### Activation du killswitch

1. **Via variable d'environnement**

   ```bash
   export MAINTENANCE_MODE=true
   docker-compose restart api
   ```

2. **Via ChatOps** (voir `chatops/killswitch.py`)
   ```bash
   python -m chatops.killswitch enable --reason "OSRM down, activating maintenance"
   ```

### Mode maintenance

En mode maintenance:

- Toutes les requ√™tes API retournent HTTP 503
- Message: "Service en maintenance - Merci de r√©essayer plus tard"
- Les t√¢ches Celery continuent (si possible)
- Les logs sont conserv√©s

### Mode d√©grad√©

En mode d√©grad√©:

- Fonctionnalit√©s non-critiques d√©sactiv√©es
- Rate limiting agressif
- Cache prioritaire
- Logs simplifi√©s

### D√©sactivation du killswitch

```bash
export MAINTENANCE_MODE=false
docker-compose restart api
```

---

## Healthchecks Kubernetes

### Endpoints disponibles

L'application expose deux endpoints distincts pour Kubernetes :

1. **`/health`** - Liveness probe (simple)

   - Retourne : `{"status": "ok"}`
   - Status code : `200`
   - Usage : V√©rifier que le processus est vivant
   - Ne v√©rifie **pas** les d√©pendances

2. **`/ready`** - Readiness probe (d√©pendances critiques)
   - Retourne : `{"status": "ready", "checks": {"database": "ok", "redis": "ok"}}`
   - Status code : `200` si pr√™t, `503` si non pr√™t
   - Usage : V√©rifier que le pod peut recevoir du trafic
   - V√©rifie : Database + Redis

### Configuration Kubernetes recommand√©e

```yaml
apiVersion: v1
kind: Pod
spec:
  containers:
    - name: api
      livenessProbe:
        httpGet:
          path: /health
          port: 5000
        initialDelaySeconds: 30
        periodSeconds: 10
        timeoutSeconds: 5
        failureThreshold: 3

      readinessProbe:
        httpGet:
          path: /ready
          port: 5000
        initialDelaySeconds: 10
        periodSeconds: 5
        timeoutSeconds: 3
        failureThreshold: 2
        successThreshold: 1
```

### Comportement

- **`/health`** : Toujours retourne `200` si le processus Flask est actif
- **`/ready`** : Retourne `503` si :
  - La base de donn√©es n'est pas accessible
  - Redis n'est pas configur√© ou inaccessible

### D√©pannage

**Probl√®me : Pod en √©tat `NotReady`**

```bash
# V√©rifier manuellement
curl http://localhost:5000/ready

# Voir les checks d√©taill√©s
curl http://localhost:5000/health/detailed
```

**Solutions communes :**

- Si `database: error` ‚Üí V√©rifier connexion DB
- Si `redis: not_configured` ‚Üí V√©rifier variable `REDIS_URL`
- Si `redis: error` ‚Üí V√©rifier que Redis est accessible

### Notes

- `/health` reste simple pour √©viter les red√©marrages inutiles
- `/ready` est strict pour √©viter le trafic vers un pod non fonctionnel
- `/health/detailed` reste disponible pour diagnostic mais n'est pas utilis√© par K8s

---

## Plan de backout

### Vue d'ensemble

Le plan de backout permet de revenir rapidement √† une version fonctionnelle en cas de probl√®me critique apr√®s un d√©ploiement ou une migration.

**RTO (Recovery Time Objective) :** < 5 minutes  
**RPO (Recovery Point Objective) :** 0 (pas de perte de donn√©es)

### ‚ö†Ô∏è Conditions de d√©clenchement

D√©clencher un backout si :

- ‚ùå Erreurs 5xx en masse (> 10% des requ√™tes)
- ‚ùå Base de donn√©es corrompue apr√®s migration
- ‚ùå Service inaccessible
- ‚ùå Violation de SLO critique
- ‚ùå Incident de s√©curit√© d√©tect√©

**Ne PAS faire de backout si :**

- ‚ö†Ô∏è Probl√®me localis√© √† quelques utilisateurs
- ‚ö†Ô∏è Erreurs transitoires (< 1% des requ√™tes)
- ‚ö†Ô∏è Pas de confirmation de cause racine

### 1. Rollback migration Alembic

#### A. Identifier la migration probl√©matique

```bash
# Voir la version actuelle de la DB
cd backend
flask db current

# Voir l'historique complet des migrations
flask db history

# Voir les d√©tails d'une migration sp√©cifique
flask db history --verbose | grep <revision_id>
```

#### B. Rollback d'une migration

```bash
# Rollback d'une seule migration (la derni√®re)
cd backend
export DATABASE_URL="postgresql://user:pass@host:5432/dbname"
export FLASK_CONFIG="production"
export SECRET_KEY="your-secret"
export JWT_SECRET_KEY="your-jwt-secret"

# Downgrade
flask db downgrade -1

# V√©rifier l'√©tat
flask db current
```

#### C. Rollback vers version sp√©cifique

```bash
# Lister toutes les r√©visions disponibles
flask db history | head -20

# Rollback vers une r√©vision sp√©cifique
flask db downgrade <revision_id>

# Exemple: rollback vers la r√©vision avant le dernier changement
flask db downgrade abc123def456
```

#### D. Rollback d'urgence (multiples migrations)

```bash
# Rollback de 3 migrations en une fois
flask db downgrade -3

# ‚ö†Ô∏è ATTENTION: V√©rifier l'int√©grit√© des donn√©es apr√®s un rollback multiple
```

#### E. V√©rifier l'√©tat apr√®s rollback

```bash
# 1. V√©rifier version DB
flask db current

# 2. V√©rifier tables existantes
psql -U atmr -d atmr -c "\dt"

# 3. V√©rifier sant√© de l'API
curl http://localhost:5000/health
curl http://localhost:5000/ready

# 4. V√©rifier logs d'erreur
docker-compose logs api --tail 100 | grep -i error

# 5. Tester une requ√™te critique
curl -H "Authorization: Bearer $TOKEN" http://localhost:5000/api/bookings
```

### 2. Rollback d√©ploiement Docker

#### A. Identifier version pr√©c√©dente

```bash
# Lister toutes les images disponibles
docker images | grep atmr-backend

# Lister les tags disponibles
docker images atmr-backend --format "table {{.Tag}}\t{{.CreatedAt}}"

# Voir l'historique Git pour identifier la version pr√©c√©dente
cd /path/to/atmr
git log --oneline -10
git tag -l "v*" | tail -5
```

#### B. Rollback image Docker (docker-compose)

```bash
# 1. Arr√™ter l'application actuelle
docker-compose down

# 2. Identifier et taguer l'image pr√©c√©dente
docker tag atmr-backend:<previous-tag> atmr-backend:latest
# OU r√©cup√©rer depuis registry
docker pull registry.example.com/atmr-backend:v1.2.3
docker tag registry.example.com/atmr-backend:v1.2.3 atmr-backend:latest

# 3. Red√©marrer avec l'image pr√©c√©dente
docker-compose up -d

# 4. V√©rifier la sant√©
docker-compose ps
curl http://localhost:5000/health
curl http://localhost:5000/ready
```

#### C. Rollback image Docker (Kubernetes)

```bash
# 1. Identifier l'image pr√©c√©dente
kubectl get deployments -n atmr -o jsonpath='{.items[*].spec.template.spec.containers[*].image}'

# 2. Rollback vers version pr√©c√©dente
kubectl set image deployment/atmr-api api=registry.example.com/atmr-backend:v1.2.3 -n atmr

# 3. Ou utiliser rollout history
kubectl rollout history deployment/atmr-api -n atmr
kubectl rollout undo deployment/atmr-api -n atmr

# 4. Surveiller le rollback
kubectl rollout status deployment/atmr-api -n atmr
kubectl get pods -n atmr -w
```

#### D. V√©rifier sant√© apr√®s rollback

```bash
# Healthchecks
curl http://localhost:5000/health
curl http://localhost:5000/ready

# M√©triques Prometheus
curl http://localhost:5000/prometheus/metrics-http | grep http_request_errors_total

# Logs r√©cents
docker-compose logs api --tail 100
# ou
kubectl logs -f deployment/atmr-api -n atmr --tail 100

# Tester requ√™tes critiques
curl -X GET http://localhost:5000/api/bookings \
  -H "Authorization: Bearer $JWT_TOKEN"

# V√©rifier base de donn√©es accessible
docker-compose exec api flask db current
```

### 3. Rollback code (Git)

```bash
# 1. Identifier le commit probl√©matique
git log --oneline -10

# 2. Cr√©er une branche de hotfix depuis le commit pr√©c√©dent
git checkout -b hotfix/rollback-$(date +%Y%m%d)
git reset --hard <previous-commit-hash>

# 3. Push et merge emergency
git push origin hotfix/rollback-$(date +%Y%m%d)
# Merge via PR d'urgence ou directement:
git checkout main
git merge hotfix/rollback-$(date +%Y%m%d) --no-ff

# 4. Red√©ployer
docker-compose build --no-cache
docker-compose up -d
```

### 4. Proc√©dures de test de backout

#### Test mensuel (Recommand√©)

**Fr√©quence:** Le premier mardi de chaque mois  
**Environnement:** Staging uniquement

#### Test rollback migration

```bash
# 1. Appliquer une migration de test
flask db upgrade head

# 2. V√©rifier l'√©tat
flask db current

# 3. Rollback
flask db downgrade -1

# 4. Mesurer le temps (objectif: < 30 secondes)
time flask db downgrade -1

# 5. V√©rifier int√©grit√©
flask db current
psql -U atmr -d atmr -c "SELECT COUNT(*) FROM information_schema.tables;"
```

#### Test rollback d√©ploiement

```bash
# 1. D√©ployer version de test
docker-compose up -d

# 2. V√©rifier fonctionnement
curl http://localhost:5000/health

# 3. Rollback (mesurer le temps)
START=$(date +%s)
docker-compose down
docker-compose up -d
END=$(date +%s)
DURATION=$((END - START))
echo "Temps de rollback: ${DURATION}s (objectif: < 5 min)"

# 4. V√©rifier sant√©
curl http://localhost:5000/ready
```

#### Checklist de validation post-rollback

- [ ] ‚úÖ API r√©pond sur `/health` (200)
- [ ] ‚úÖ API r√©pond sur `/ready` (200)
- [ ] ‚úÖ Base de donn√©es accessible
- [ ] ‚úÖ Redis accessible
- [ ] ‚úÖ Aucune erreur 5xx dans les logs
- [ ] ‚úÖ Tests de smoke passent
- [ ] ‚úÖ M√©triques Prometheus normales
- [ ] ‚úÖ Pas de perte de donn√©es critiques

### 5. Communication post-rollback

Apr√®s un rollback en production :

1. **Notification imm√©diate :**

   - Alert Slack #incidents
   - Page on-call engineer
   - Notifier √©quipe backend

2. **Post-mortem (dans les 24h) :**

   - Documenter cause du probl√®me
   - Analyser pourquoi le rollback √©tait n√©cessaire
   - Identifier am√©liorations pr√©ventives
   - Mettre √† jour ce RUNBOOK si n√©cessaire

3. **Mesures correctives :**
   - Corriger le probl√®me dans la version suivante
   - Am√©liorer tests/validation avant d√©ploiement
   - Documenter changements requis

### 6. RTO (Recovery Time Objective) et m√©triques

**Objectifs :**

- **Rollback migration :** < 30 secondes
- **Rollback d√©ploiement :** < 5 minutes
- **Communication incident :** < 2 minutes

**M√©triques √† surveiller apr√®s rollback :**

```bash
# Taux d'erreur
curl http://localhost:5000/prometheus/metrics-http | grep http_requests_total

# Latence
curl http://localhost:5000/prometheus/metrics-http | grep http_request_duration_seconds

# Disponibilit√© endpoints critiques
curl http://localhost:5000/health
curl http://localhost:5000/api/bookings
```

### 7. Contacts d'urgence

En cas de besoin d'aide pour un rollback :

- **Lead Backend :** [Contact]
- **Lead DevOps/SRE :** [Contact]
- **On-call engineer :** PagerDuty
- **Base de donn√©es :** [Contact DBA]

---

## Sauvegarde et restauration de base de donn√©es

### Vue d'ensemble

Les sauvegardes PostgreSQL sont essentielles pour garantir la r√©cup√©ration en cas de perte de donn√©es.

**RPO (Recovery Point Objective) :** < 15 minutes  
**RTO (Recovery Time Objective) :** < 30 minutes  
**Fr√©quence de backup :** Toutes les heures en production

### Scripts disponibles

- **`scripts/backup_db.sh`** : Cr√©er un backup de la base de donn√©es
- **`scripts/restore_db.sh`** : Restaurer depuis un backup
- **`scripts/test_backup_restore.sh`** : Tester le processus complet

### 1. Cr√©er un backup

#### Utilisation basique

```bash
# Backup dans le r√©pertoire par d√©faut (./backups)
./scripts/backup_db.sh

# Backup dans un r√©pertoire sp√©cifique
./scripts/backup_db.sh /path/to/backups
```

#### Fonctionnalit√©s

- ‚úÖ Cr√©e deux formats : `.dump` (custom, rapide) et `.sql` (texte, lisible)
- ‚úÖ Timestamp automatique dans le nom de fichier
- ‚úÖ Liens symboliques `latest.dump` et `latest.sql`
- ‚úÖ Compatible Docker Compose et installation locale
- ‚úÖ Affiche la taille du backup

#### Exemple de sortie

```
üîÑ Backup base de donn√©es PostgreSQL...
   Database: atmr
   Host: postgres:5432
   Mode: Docker Compose

‚úÖ Backup cr√©√© avec succ√®s!
   üì¶ Format custom: backups/atmr_backup_20250127_143022.dump (45M)
   üìÑ Format SQL: backups/atmr_backup_20250127_143022.sql (67M)
   üîó Liens: backups/latest.dump, backups/latest.sql
```

### 2. Restaurer depuis un backup

#### Utilisation

```bash
# Restauration interactive (demande confirmation)
./scripts/restore_db.sh backups/atmr_backup_20250127_143022.dump

# Restauration forc√©e (sans confirmation)
./scripts/restore_db.sh backups/latest.dump --force

# Utiliser le dernier backup
./scripts/restore_db.sh backups/latest.dump --force
```

#### ‚ö†Ô∏è ATTENTION

La restauration **√©crase compl√®tement** la base de donn√©es actuelle. Assurez-vous de :

1. ‚úÖ Avoir un backup r√©cent avant restauration
2. ‚úÖ V√©rifier que le fichier de backup est valide
3. ‚úÖ Avoir test√© la proc√©dure en staging

#### Formats support√©s

- **`.dump`** : Format custom PostgreSQL (recommand√©, plus rapide)
- **`.sql`** : Format SQL texte (plus lisible, plus lent)

Le script d√©tecte automatiquement le format.

### 3. Tests de backup/restore

#### Test automatique

```bash
# Lancer le test complet
./scripts/test_backup_restore.sh
```

Le script effectue :

1. ‚úÖ Cr√©ation d'un backup
2. ‚úÖ Ajout de donn√©es de test
3. ‚úÖ Restauration depuis le backup
4. ‚úÖ V√©rification que les donn√©es de test ont √©t√© supprim√©es
5. ‚úÖ Calcul des m√©triques RTO/RPO

#### Exemple de sortie

```
==========================================
üß™ TEST BACKUP/RESTORE PostgreSQL
==========================================

üì¶ √âtape 1/4: Cr√©ation du backup...
‚úÖ Backup cr√©√©: backups/atmr_backup_20250127_143022.dump (12s)

üìù √âtape 2/4: Cr√©ation de donn√©es de test...
‚úÖ Donn√©es de test cr√©√©es (timestamp: 1706368222)

üîÑ √âtape 3/4: Restauration depuis le backup...
‚úÖ Restauration termin√©e (15s)

üîç √âtape 4/4: V√©rification de l'int√©grit√©...
‚úÖ Test r√©ussi: donn√©es restaur√©es correctement
   üìä Tables restaur√©es: 42

==========================================
‚úÖ TEST BACKUP/RESTORE R√âUSSI
==========================================

üìä M√©triques:
   ‚è±Ô∏è  Temps de backup: 12s
   ‚è±Ô∏è  Temps de restauration: 15s
   ‚è±Ô∏è  Temps total: 27s

üéØ Objectifs:
   RTO (Restore Time Objective): 15s (objectif: < 30 min ‚úÖ)
   RPO (Recovery Point Objective): ~12s (objectif: < 15 min ‚úÖ)
```

### 4. Backup automatis√© (Production)

#### Crontab (recommand√©)

```bash
# Backup toutes les heures
0 * * * * /path/to/atmr/scripts/backup_db.sh /var/backups/atmr

# Backup quotidien √† 2h du matin
0 2 * * * /path/to/atmr/scripts/backup_db.sh /var/backups/atmr/daily
```

#### Backup vers stockage distant

```bash
# Exemple: Backup + upload vers S3
./scripts/backup_db.sh /tmp/backups
aws s3 cp /tmp/backups/atmr_backup_*.dump s3://atmr-backups/ --recursive

# Nettoyer les anciens backups locaux (garder 7 jours)
find /tmp/backups -name "atmr_backup_*.dump" -mtime +7 -delete
```

### 5. V√©rifications post-restauration

Apr√®s une restauration, v√©rifier :

```bash
# 1. Sant√© de l'API
curl http://localhost:5000/health
curl http://localhost:5000/ready

# 2. Nombre de tables
docker-compose exec postgres psql -U atmr -d atmr -c "\dt" | wc -l

# 3. V√©rifier quelques donn√©es critiques
docker-compose exec postgres psql -U atmr -d atmr -c "SELECT COUNT(*) FROM company;"
docker-compose exec postgres psql -U atmr -d atmr -c "SELECT COUNT(*) FROM booking;"

# 4. V√©rifier logs d'erreur
docker-compose logs api --tail 100 | grep -i error

# 5. Tester une requ√™te API
curl -H "Authorization: Bearer $TOKEN" http://localhost:5000/api/bookings
```

### 6. R√©tention des backups

**Recommandations :**

- **Backups horaires :** Garder 24 heures (24 backups)
- **Backups quotidiens :** Garder 7 jours (7 backups)
- **Backups hebdomadaires :** Garder 4 semaines (4 backups)
- **Backups mensuels :** Garder 12 mois (12 backups)

**Script de nettoyage :**

```bash
#!/bin/bash
# Nettoyer backups > 7 jours
BACKUP_DIR="/var/backups/atmr"
find "$BACKUP_DIR" -name "atmr_backup_*.dump" -mtime +7 -delete
find "$BACKUP_DIR" -name "atmr_backup_*.sql" -mtime +7 -delete
```

### 7. Tests mensuels de restauration

**Fr√©quence :** Le premier mercredi de chaque mois  
**Environnement :** Staging uniquement

```bash
# 1. Lancer le test
./scripts/test_backup_restore.sh

# 2. Documenter les r√©sultats
# - Temps de backup (objectif: < 5 min)
# - Temps de restauration (objectif: < 30 min)
# - Taille du backup
# - Int√©grit√© des donn√©es v√©rifi√©e

# 3. Si test √©choue, investigation imm√©diate
```

### 8. Troubleshooting

**Probl√®me : Backup √©choue**

```bash
# V√©rifier connexion PostgreSQL
docker-compose exec postgres pg_isready -U atmr

# V√©rifier espace disque
df -h

# V√©rifier permissions
ls -la backups/
```

**Probl√®me : Restauration √©choue**

```bash
# V√©rifier format du backup
file backups/atmr_backup_*.dump

# V√©rifier int√©grit√©
pg_restore --list backups/atmr_backup_*.dump | head -20

# V√©rifier espace disque disponible
df -h
```

**Probl√®me : Donn√©es manquantes apr√®s restauration**

1. V√©rifier que le bon backup a √©t√© utilis√©
2. V√©rifier la date/heure du backup
3. V√©rifier les logs de restauration pour erreurs
4. Tester avec un autre backup si disponible

---

## Contacts

- **On-call engineer**: Voir rotation dans PagerDuty/OpsGenie
- **Lead DevOps**: [Contact]
- **CTO**: [Contact]

## Escalade

1. **Niveau 1** (0-15 min): On-call engineer
2. **Niveau 2** (15-30 min): Lead DevOps
3. **Niveau 3** (30+ min): CTO

---

_Derni√®re mise √† jour: 2025-01-28_
