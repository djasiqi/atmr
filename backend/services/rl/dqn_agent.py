# ruff: noqa: T201
# pyright: reportMissingImports=false
"""
Agent DQN (Deep Q-Network) pour le dispatch autonome.

Impl√©mente:
- Epsilon-greedy exploration/exploitation
- Experience replay avec buffer
- Target network pour stabilit√©
- Double DQN pour r√©duire overestimation
- Save/Load de mod√®les

Auteur: ATMR Project - RL Team
Date: Octobre 2025
Semaine: 15 (Jours 3-5)
"""
import os
from pathlib import Path

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

from services.rl.q_network import QNetwork
from services.rl.replay_buffer import ReplayBuffer, Transition


class DQNAgent:
    """
    Agent DQN pour le dispatch de v√©hicules.

    Features:
        - Epsilon-greedy pour √©quilibrer exploration/exploitation
        - Experience Replay (r√©utilise les exp√©riences)
        - Target Network (stabilit√© d'apprentissage)
        - Double DQN (r√©duit surestimation des Q-values)
        - Gradient clipping (√©vite explosions)
        - Checkpoints automatiques

    Hyperparam√®tres:
        - learning_rate: 0.001 (taux d'apprentissage)
        - gamma: 0.99 (discount factor - importance du futur)
        - epsilon: 1.0 ‚Üí 0.01 (exploration ‚Üí exploitation)
        - batch_size: 64 (taille batch pour training)
        - buffer_size: 100k (transitions stock√©es)
    """

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        learning_rate: float = 0.001,
        gamma: float = 0.99,
        epsilon_start: float = 1.0,
        epsilon_end: float = 0.01,
        epsilon_decay: float = 0.995,
        batch_size: int = 64,
        buffer_size: int = 100000,
        target_update_freq: int = 10,
        device: str | None = None,
    ):
        """
        Initialise l'agent DQN.

        Args:
            state_dim: Dimension de l'espace d'√©tat
            action_dim: Nombre d'actions possibles
            learning_rate: Taux d'apprentissage Adam
            gamma: Discount factor (0-1, importance du futur)
            epsilon_start: Epsilon initial (exploration)
            epsilon_end: Epsilon minimal (exploitation)
            epsilon_decay: Facteur de d√©croissance de epsilon
            batch_size: Taille du batch pour training
            buffer_size: Capacit√© du replay buffer
            target_update_freq: Fr√©quence de mise √† jour du target network (episodes)
            device: Device PyTorch ('cpu', 'cuda', ou None pour auto)
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq

        # Device (CPU ou GPU)
        if device is None:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = torch.device(device)

        print(f"üñ•Ô∏è  DQN Agent using device: {self.device}")

        # Cr√©er les deux r√©seaux (Q-network et Target network)
        self.q_network = QNetwork(state_dim, action_dim).to(self.device)
        self.target_network = QNetwork(state_dim, action_dim).to(self.device)

        # Copier les poids initiaux vers le target network
        self.target_network.load_state_dict(self.q_network.state_dict())
        self.target_network.eval()  # Toujours en mode √©valuation

        # Optimizer (Adam est un bon choix pour DQN)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)

        # Loss function (Huber Loss = robuste aux outliers)
        self.criterion = nn.SmoothL1Loss()

        # Replay buffer
        self.memory = ReplayBuffer(buffer_size)

        # Tracking des m√©triques
        self.training_step = 0
        self.episode_count = 0
        self.losses = []

        print("‚úÖ Agent DQN cr√©√©:")
        print(f"   State dim: {state_dim}")
        print(f"   Action dim: {action_dim}")
        print(f"   Param√®tres Q-Network: {self.q_network.count_parameters():,}")

    def select_action(self, state: np.ndarray, training: bool = True) -> int:
        """
        S√©lectionne une action avec epsilon-greedy.

        Strat√©gie:
            - Avec probabilit√© epsilon: action al√©atoire (exploration)
            - Avec probabilit√© 1-epsilon: meilleure action selon Q-network (exploitation)

        Args:
            state: √âtat actuel (numpy array)
            training: Si True, utilise epsilon-greedy, sinon greedy pur

        Returns:
            action: Index de l'action s√©lectionn√©e
        """
        if training and np.random.random() < self.epsilon:
            # Exploration: action al√©atoire
            return np.random.randint(self.action_dim)
        else:
            # Exploitation: meilleure action selon Q-network
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.q_network(state_tensor)
                return q_values.argmax().item()

    def store_transition(
        self,
        state: np.ndarray,
        action: int,
        next_state: np.ndarray,
        reward: float,
        done: bool
    ):
        """
        Stocke une transition dans le replay buffer.

        Args:
            state: √âtat actuel
            action: Action prise
            next_state: √âtat suivant
            reward: R√©compense re√ßue
            done: Episode termin√© ou non
        """
        self.memory.push(state, action, next_state, reward, done)

    def train_step(self) -> float:
        """
        Effectue un pas d'entra√Ænement (backpropagation).

        Algorithme Double DQN:
            1. Sample batch al√©atoire du replay buffer
            2. Calculer Q(s, a) actuelles avec q_network
            3. S√©lectionner meilleures actions avec q_network
            4. √âvaluer ces actions avec target_network (Double DQN)
            5. Calculer target: r + Œ≥ * Q_target(s', a_best)
            6. Minimiser loss = (Q_current - target)¬≤
            7. Backpropagation et mise √† jour des poids

        Returns:
            loss: Valeur de la loss (pour monitoring)
        """
        # V√©rifier qu'il y a assez de donn√©es
        if len(self.memory) < self.batch_size:
            return 0.0

        # 1. Sample batch al√©atoire
        transitions = self.memory.sample(self.batch_size)
        batch = Transition(*zip(*transitions, strict=False))

        # 2. Convertir en tensors PyTorch
        state_batch = torch.FloatTensor(np.array(batch.state)).to(self.device)
        action_batch = torch.LongTensor(batch.action).to(self.device)
        reward_batch = torch.FloatTensor(batch.reward).to(self.device)
        next_state_batch = torch.FloatTensor(np.array(batch.next_state)).to(self.device)
        done_batch = torch.FloatTensor(batch.done).to(self.device)

        # 3. Calculer Q-values actuelles pour les actions prises
        current_q_values = self.q_network(state_batch).gather(
            1, action_batch.unsqueeze(1)
        )

        # 4. Calculer Q-values cibles avec Double DQN
        with torch.no_grad():
            # Double DQN: S√©lectionner action avec q_network
            next_actions = self.q_network(next_state_batch).argmax(1, keepdim=True)

            # √âvaluer cette action avec target_network
            next_q_values = self.target_network(next_state_batch).gather(1, next_actions)

            # Target: r + Œ≥ * Q_target(s', a_best) * (1 - done)
            target_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values.squeeze()

        # 5. Calculer la loss (Huber Loss = robuste)
        loss = self.criterion(current_q_values.squeeze(), target_q_values)

        # 6. Backpropagation
        self.optimizer.zero_grad()
        loss.backward()

        # 7. Gradient clipping (√©vite explosions de gradients)
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=10)

        # 8. Mise √† jour des poids
        self.optimizer.step()

        # Tracking
        self.training_step += 1
        self.losses.append(loss.item())

        return loss.item()

    def update_target_network(self):
        """
        Met √† jour le target network en copiant les poids du q_network.

        Appel√© p√©riodiquement (tous les N episodes) pour stabilit√©.
        """
        self.target_network.load_state_dict(self.q_network.state_dict())

    def decay_epsilon(self):
        """
        D√©cro√Æt epsilon pour r√©duire progressivement l'exploration.

        Formule: epsilon = max(epsilon_end, epsilon * epsilon_decay)

        Exemple:
            epsilon_start = 1.0, epsilon_decay = 0.995, epsilon_end = 0.01
            Episode 100: epsilon ‚âà 0.60
            Episode 500: epsilon ‚âà 0.08
            Episode 1000: epsilon ‚âà 0.01
        """
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

    def save(self, path: str):
        """
        Sauvegarde le mod√®le complet (r√©seaux + optimizer + config).

        Args:
            path: Chemin du fichier .pth
        """
        # Cr√©er le r√©pertoire si n√©cessaire
        Path(path).parent.mkdir(parents=True, exist_ok=True)

        torch.save({
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon,
            'training_step': self.training_step,
            'episode_count': self.episode_count,
            'losses': self.losses[-1000:],  # Garde les 1000 derni√®res losses
            'config': {
                'state_dim': self.state_dim,
                'action_dim': self.action_dim,
                'learning_rate': self.optimizer.param_groups[0]['lr'],
                'gamma': self.gamma,
                'epsilon_end': self.epsilon_end,
                'epsilon_decay': self.epsilon_decay,
                'batch_size': self.batch_size,
                'target_update_freq': self.target_update_freq
            }
        }, path)

    def load(self, path: str):
        """
        Charge un mod√®le sauvegard√©.

        Args:
            path: Chemin du fichier .pth

        Raises:
            FileNotFoundError: Si le fichier n'existe pas
        """
        if not os.path.exists(path):
            raise FileNotFoundError(f"Model file not found: {path}")

        checkpoint = torch.load(path, map_location=self.device, weights_only=False)

        # Charger les √©tats des r√©seaux
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])

        # Charger l'optimizer
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        # Charger les m√©triques
        self.epsilon = checkpoint['epsilon']
        self.training_step = checkpoint['training_step']
        self.episode_count = checkpoint['episode_count']
        self.losses = checkpoint.get('losses', [])

        print(f"‚úÖ Mod√®le charg√© depuis: {path}")
        print(f"   Episode: {self.episode_count}")
        print(f"   Training steps: {self.training_step}")
        print(f"   Epsilon: {self.epsilon:.4f}")

    def save_checkpoint(
        self,
        episode: int,
        avg_reward: float,
        path_prefix: str = "data/rl/models"
    ) -> str:
        """
        Sauvegarde un checkpoint avec m√©tadonn√©es dans le nom.

        Args:
            episode: Num√©ro de l'√©pisode
            avg_reward: Reward moyen r√©cent
            path_prefix: Pr√©fixe du chemin

        Returns:
            Chemin du fichier sauvegard√©
        """
        os.makedirs(path_prefix, exist_ok=True)

        filename = f"{path_prefix}/dqn_ep{episode:04d}_r{avg_reward:.0f}.pth"
        self.save(filename)

        return filename

    def get_q_values(self, state: np.ndarray) -> np.ndarray:
        """
        Retourne les Q-values pour toutes les actions (pour debugging/analyse).

        Args:
            state: √âtat (numpy array)

        Returns:
            Q-values (numpy array de taille action_dim)
        """
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            q_values = self.q_network(state_tensor)
            return q_values.cpu().numpy().squeeze()

    def get_training_info(self) -> dict:
        """
        Retourne des informations sur l'√©tat du training.

        Returns:
            Dictionnaire avec m√©triques
        """
        avg_loss_100 = np.mean(self.losses[-100:]) if self.losses else 0.0

        return {
            "training_step": self.training_step,
            "episode_count": self.episode_count,
            "epsilon": self.epsilon,
            "buffer_size": len(self.memory),
            "avg_loss_100": avg_loss_100,
            "total_losses_tracked": len(self.losses)
        }

