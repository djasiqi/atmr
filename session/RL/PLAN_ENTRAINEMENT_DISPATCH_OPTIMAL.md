# üß† Plan d'Entra√Ænement RL pour Dispatch Optimal

**Date** : 21 octobre 2025  
**Objectif** : Utiliser le Reinforcement Learning pour apprendre le meilleur dispatching possible  
**M√©thode** : Entra√Ænement offline sur donn√©es historiques + simulations

---

## üéØ VOTRE ID√âE (Excellente !)

> "Lancer un entra√Ænement qui permettrait de d√©finir le meilleur r√©sultat possible"

**Concept** :

- **INPUT** : Heure d√©part, distance, temps transport, lieux, chauffeurs disponibles
- **OUTPUT** : Assignation optimale (√©quit√© + distance + temps)
- **M√âTHODE** : Entra√Æner un agent RL sur 1000+ dispatches historiques

---

## üìä ARCHITECTURE EXISTANTE (Bonne base !)

Vous avez **d√©j√†** :

‚úÖ **Environnement Gym** : `backend/services/rl/dispatch_env.py`

```python
class DispatchEnv(gym.Env):
    """
    √âtat (observation_space):
        - Positions chauffeurs (N √ó 2)
        - Disponibilit√© chauffeurs (N)
        - Charge de travail (N)
        - Positions bookings (M √ó 2)
        - Priorit√©s bookings (M)
        - Temps restant fen√™tre (M)
        - Heure actuelle + trafic

    Actions (action_space):
        - Action 0: Attendre
        - Actions 1 √† N√óM: Assigner booking[i] √† driver[j]

    R√©compense (reward):
        +100 * assignments_r√©ussis
        -50 * retards_pickup
        -60 * bookings_annul√©s
        +10 * distance_optimale
        +20 * workload_√©quilibr√©  ‚¨ÖÔ∏è √âQUIT√â !
        -5 * temps_inaction
    """
```

‚úÖ **Agent DQN** : `backend/services/rl/dqn_agent.py`

- Deep Q-Network (r√©seau de neurones)
- Experience Replay (m√©moire d'entra√Ænement)
- Target Network (stabilit√©)

‚úÖ **G√©n√©rateur de suggestions** : `backend/services/rl/suggestion_generator.py`

- Utilise le DQN entra√Æn√©
- Propose des r√©assignations

---

## üöÄ IMPL√âMENTATION : ENTRA√éNEMENT OFFLINE

### Phase 1Ô∏è‚É£ : Collecte des Donn√©es Historiques

**Objectif** : Extraire 1000+ dispatches pass√©s pour l'entra√Ænement

```python
# backend/scripts/rl_export_historical_data.py

from models import DispatchRun, Assignment, Booking, Driver
from datetime import datetime, timedelta
import json

def export_historical_dispatches(
    company_id: int,
    start_date: str,  # "2025-01-01"
    end_date: str,    # "2025-10-21"
    output_file: str = "data/rl/historical_dispatches.json"
):
    """
    Exporte les dispatches historiques au format JSON pour entra√Ænement RL.
    """
    dispatches = []

    # R√©cup√©rer tous les dispatch_runs de la p√©riode
    runs = DispatchRun.query.filter(
        DispatchRun.company_id == company_id,
        DispatchRun.day >= start_date,
        DispatchRun.day <= end_date,
        DispatchRun.status == DispatchStatus.COMPLETED
    ).all()

    print(f"üìä R√©cup√©ration de {len(runs)} dispatch runs...")

    for run in runs:
        # R√©cup√©rer les bookings et assignments
        assignments = Assignment.query.filter_by(dispatch_run_id=run.id).all()

        if len(assignments) == 0:
            continue  # Skip runs sans assignments

        # Calculer les m√©triques
        driver_loads = {}
        total_distance = 0
        retards = 0

        for a in assignments:
            driver_id = a.driver_id
            driver_loads[driver_id] = driver_loads.get(driver_id, 0) + 1

            # Calculer distance (si disponible)
            booking = a.booking
            if booking.pickup_lat and booking.dropoff_lat:
                dist = haversine_distance(
                    (booking.pickup_lat, booking.pickup_lon),
                    (booking.dropoff_lat, booking.dropoff_lon)
                )
                total_distance += dist

            # D√©tecter retards (si disponible)
            if hasattr(a, 'actual_pickup_time') and a.actual_pickup_time:
                delay = (a.actual_pickup_time - booking.scheduled_time).total_seconds() / 60
                if delay > 5:
                    retards += 1

        # Calculer √©cart de charge (√©quit√©)
        if driver_loads:
            max_load = max(driver_loads.values())
            min_load = min(driver_loads.values())
            load_gap = max_load - min_load
        else:
            load_gap = 0

        # Calculer score global
        quality_score = (
            100 - (load_gap * 10) -      # P√©nalit√© √©quit√©
            (total_distance * 0.5) -     # P√©nalit√© distance
            (retards * 5)                # P√©nalit√© retards
        )

        # Export
        dispatch_data = {
            "id": run.id,
            "date": run.day.isoformat(),
            "num_bookings": len(assignments),
            "num_drivers": len(driver_loads),
            "driver_loads": driver_loads,
            "load_gap": load_gap,
            "total_distance_km": round(total_distance, 2),
            "retards_count": retards,
            "quality_score": round(quality_score, 2),
            "bookings": [
                {
                    "id": a.booking_id,
                    "scheduled_time": a.booking.scheduled_time.isoformat(),
                    "pickup_lat": a.booking.pickup_lat,
                    "pickup_lon": a.booking.pickup_lon,
                    "dropoff_lat": a.booking.dropoff_lat,
                    "dropoff_lon": a.booking.dropoff_lon,
                    "assigned_driver": a.driver_id,
                }
                for a in assignments
            ]
        }

        dispatches.append(dispatch_data)

    # Sauvegarder
    with open(output_file, 'w') as f:
        json.dump({
            "company_id": company_id,
            "period": f"{start_date} to {end_date}",
            "total_dispatches": len(dispatches),
            "dispatches": dispatches
        }, f, indent=2)

    print(f"‚úÖ {len(dispatches)} dispatches export√©s vers {output_file}")
    print(f"üìä Statistiques:")
    print(f"   - √âcart moyen: {sum(d['load_gap'] for d in dispatches) / len(dispatches):.1f}")
    print(f"   - Score moyen: {sum(d['quality_score'] for d in dispatches) / len(dispatches):.1f}")

# Utilisation
if __name__ == "__main__":
    export_historical_dispatches(
        company_id=1,
        start_date="2025-01-01",
        end_date="2025-10-21"
    )
```

**Commande** :

```bash
docker exec atmr-api python backend/scripts/rl_export_historical_data.py
```

---

### Phase 2Ô∏è‚É£ : Entra√Ænement Offline (Batch Learning)

**Objectif** : Entra√Æner l'agent DQN sur des simulations bas√©es sur les donn√©es historiques

```python
# backend/scripts/rl_train_offline.py

import json
import numpy as np
from services.rl.dispatch_env import DispatchEnv
from services.rl.dqn_agent import DQNAgent
import torch

def train_offline(
    historical_data_file: str = "data/rl/historical_dispatches.json",
    num_episodes: int = 5000,
    save_path: str = "data/rl/models/dispatch_optimized.pth"
):
    """
    Entra√Æne l'agent DQN offline sur des donn√©es historiques.

    M√©thode :
    1. Charger les dispatches historiques
    2. Pour chaque episode :
        - S√©lectionner un dispatch historique al√©atoire
        - Recr√©er l'√©tat initial (bookings + drivers)
        - Simuler l'assignation avec l'agent
        - Calculer la r√©compense (√©quit√© + distance + retards)
        - Mettre √† jour le mod√®le
    3. Sauvegarder le mod√®le optimis√©
    """
    print("üß† D√©marrage entra√Ænement offline...")

    # Charger donn√©es historiques
    with open(historical_data_file, 'r') as f:
        data = json.load(f)

    dispatches = data['dispatches']
    print(f"üìä {len(dispatches)} dispatches charg√©s")

    # Initialiser environnement et agent
    env = DispatchEnv(
        num_drivers=5,      # Ajuster selon votre flotte
        max_bookings=20,    # Ajuster selon vos donn√©es
        simulation_hours=12
    )

    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    agent = DQNAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        learning_rate=0.0001,  # LR r√©duit pour offline
        gamma=0.99,            # Discount factor
        epsilon_start=0.5,     # Exploration r√©duite (on a d√©j√† des bonnes donn√©es)
        epsilon_end=0.01,
        epsilon_decay=0.995
    )

    # M√©triques d'entra√Ænement
    episode_rewards = []
    episode_load_gaps = []
    best_avg_reward = -float('inf')

    for episode in range(num_episodes):
        # S√©lectionner un dispatch historique al√©atoire
        dispatch = np.random.choice(dispatches)

        # Recr√©er l'√©tat initial
        state = _create_state_from_dispatch(env, dispatch)

        total_reward = 0
        done = False
        step = 0
        driver_loads = {i: 0 for i in range(env.num_drivers)}

        while not done and step < len(dispatch['bookings']):
            # Agent choisit une action (assigner booking √† driver)
            action = agent.select_action(state)

            # Simuler l'assignation
            next_state, reward, done, info = env.step(action)

            # Calculer r√©compense r√©elle bas√©e sur √©quit√©
            if action > 0:  # Action != "wait"
                driver_id = (action - 1) // env.max_bookings
                driver_loads[driver_id] += 1

                # R√©compense √©quit√©
                max_load = max(driver_loads.values())
                min_load = min(driver_loads.values())
                load_gap = max_load - min_load

                equity_reward = -10 * load_gap  # P√©nalit√© exponentielle
                reward += equity_reward

            # Stocker transition dans la m√©moire
            agent.memory.push(state, action, next_state, reward, done)

            # Entra√Æner
            loss = agent.train_step()

            total_reward += reward
            state = next_state
            step += 1

        # Calculer √©cart final
        max_load = max(driver_loads.values())
        min_load = min(driver_loads.values())
        load_gap = max_load - min_load

        episode_rewards.append(total_reward)
        episode_load_gaps.append(load_gap)

        # Logs tous les 100 episodes
        if (episode + 1) % 100 == 0:
            avg_reward = np.mean(episode_rewards[-100:])
            avg_gap = np.mean(episode_load_gaps[-100:])

            print(f"Episode {episode + 1}/{num_episodes}")
            print(f"  Avg Reward: {avg_reward:.2f}")
            print(f"  Avg Load Gap: {avg_gap:.2f}")
            print(f"  Epsilon: {agent.epsilon:.3f}")
            print(f"  Memory Size: {len(agent.memory)}")

            # Sauvegarder si meilleur mod√®le
            if avg_reward > best_avg_reward:
                best_avg_reward = avg_reward
                agent.save(save_path)
                print(f"  ‚úÖ Meilleur mod√®le sauvegard√©!")

        # Decay epsilon
        agent.update_epsilon()

    print(f"\nüéâ Entra√Ænement termin√©!")
    print(f"üìä Statistiques finales:")
    print(f"   - R√©compense moyenne (100 derniers): {np.mean(episode_rewards[-100:]):.2f}")
    print(f"   - √âcart moyen (100 derniers): {np.mean(episode_load_gaps[-100:]):.2f}")
    print(f"   - Mod√®le sauvegard√©: {save_path}")

def _create_state_from_dispatch(env, dispatch):
    """Recr√©e l'√©tat initial √† partir d'un dispatch historique."""
    # Reset environnement
    env.reset()

    # Charger les bookings du dispatch
    for booking_data in dispatch['bookings']:
        env.bookings.append({
            'id': booking_data['id'],
            'pickup_lat': booking_data['pickup_lat'],
            'pickup_lon': booking_data['pickup_lon'],
            'dropoff_lat': booking_data['dropoff_lat'],
            'dropoff_lon': booking_data['dropoff_lon'],
            'scheduled_time': booking_data['scheduled_time'],
            'assigned': False
        })

    # Retourner √©tat observ√©
    return env._get_observation()

# Utilisation
if __name__ == "__main__":
    train_offline(
        num_episodes=5000,  # Plus = meilleur, mais plus long (5000 ep ‚âà 2-3h)
        save_path="data/rl/models/dispatch_optimized_v1.pth"
    )
```

**Commande** :

```bash
docker exec atmr-api python backend/scripts/rl_train_offline.py
```

---

### Phase 3Ô∏è‚É£ : Int√©gration dans le Dispatch

**Objectif** : Utiliser l'agent entra√Æn√© pour **am√©liorer** le dispatch initial

```python
# backend/services/unified_dispatch/rl_optimizer.py

from services.rl.dqn_agent import DQNAgent
from services.rl.dispatch_env import DispatchEnv
import numpy as np

class RLDispatchOptimizer:
    """
    Optimiseur RL qui am√©liore le dispatch heuristique.
    """

    def __init__(self, model_path: str = "data/rl/models/dispatch_optimized_v1.pth"):
        self.agent = DQNAgent.load(model_path)
        self.agent.epsilon = 0.0  # Mode exploitation (pas d'exploration)

    def optimize_assignments(
        self,
        initial_assignments: List[Dict],
        bookings: List[Booking],
        drivers: List[Driver]
    ) -> List[Dict]:
        """
        Optimise les assignations initiales avec l'agent RL.

        Args:
            initial_assignments: Assignations de l'heuristique
            bookings: Liste des bookings
            drivers: Liste des chauffeurs

        Returns:
            Assignations optimis√©es (meilleur √©quilibre)
        """
        # Cr√©er environnement
        env = DispatchEnv(
            num_drivers=len(drivers),
            max_bookings=len(bookings)
        )

        # Charger √©tat initial
        state = self._create_state(bookings, drivers, initial_assignments)

        # Simuler des r√©assignations
        optimized = initial_assignments.copy()

        for _ in range(10):  # Max 10 swaps
            # Agent sugg√®re une r√©assignation
            action = self.agent.select_action(state)

            if action == 0:  # Wait (no change)
                break

            # D√©coder l'action (booking_id, driver_id)
            booking_idx = (action - 1) // len(drivers)
            driver_idx = (action - 1) % len(drivers)

            if booking_idx >= len(bookings):
                break

            # Appliquer la r√©assignation
            booking_id = bookings[booking_idx].id
            driver_id = drivers[driver_idx].id

            # Mettre √† jour
            for assignment in optimized:
                if assignment['booking_id'] == booking_id:
                    old_driver = assignment['driver_id']
                    assignment['driver_id'] = driver_id

                    # Calculer nouvelle r√©compense
                    driver_loads = self._calculate_loads(optimized, drivers)
                    max_load = max(driver_loads.values())
                    min_load = min(driver_loads.values())
                    new_gap = max_load - min_load

                    # Si moins bon, annuler
                    old_gap = self._calculate_gap(initial_assignments, drivers)
                    if new_gap > old_gap:
                        assignment['driver_id'] = old_driver  # Rollback
                    else:
                        print(f"‚úÖ R√©assignation : Booking {booking_id} ‚Üí Driver {driver_id}")
                        print(f"   √âcart r√©duit : {old_gap} ‚Üí {new_gap}")

            # Mettre √† jour √©tat
            state = self._create_state(bookings, drivers, optimized)

        return optimized

    def _calculate_gap(self, assignments, drivers):
        """Calcule l'√©cart de charge max-min."""
        loads = self._calculate_loads(assignments, drivers)
        return max(loads.values()) - min(loads.values())

    def _calculate_loads(self, assignments, drivers):
        """Compte le nombre d'assignations par chauffeur."""
        loads = {d.id: 0 for d in drivers}
        for a in assignments:
            loads[a['driver_id']] += 1
        return loads
```

---

### Phase 4Ô∏è‚É£ : Modification de l'Engine

**Objectif** : Int√©grer l'optimiseur RL dans le pipeline de dispatch

```python
# backend/services/unified_dispatch/engine.py

from services.unified_dispatch.rl_optimizer import RLDispatchOptimizer

# Dans la fonction run(), apr√®s l'heuristique:

# ... Heuristique a assign√© toutes les courses ...

# üÜï Optimisation RL (si activ√©e)
if mode == "auto" and getattr(s.features, "enable_rl_optimization", True):
    try:
        logger.info("[Engine] üß† Optimisation RL des assignations...")

        optimizer = RLDispatchOptimizer()

        # Convertir assignments en format optimisable
        initial = [
            {
                'booking_id': a.booking_id,
                'driver_id': a.driver_id,
            }
            for a in final_assignments
        ]

        # Optimiser
        optimized = optimizer.optimize_assignments(
            initial_assignments=initial,
            bookings=problem["bookings"],
            drivers=regs
        )

        # Appliquer les changements
        for i, a in enumerate(final_assignments):
            a.driver_id = optimized[i]['driver_id']

        logger.info("[Engine] ‚úÖ Optimisation RL termin√©e")

    except Exception as e:
        logger.warning("[Engine] ‚ö†Ô∏è Optimisation RL √©chou√©e: %s", e)
        # Continuer avec l'heuristique seule
```

---

## üìä R√âSULTATS ATTENDUS

### Avant (Heuristique seule)

```
Giuseppe : 5 courses
Dris     : 3 courses
Yannis   : 2 courses
√âCART    : 3 ‚ùå
```

### Apr√®s (Heuristique + RL Optimizer)

```
Giuseppe : 3-4 courses
Dris     : 3-4 courses
Yannis   : 3-4 courses
√âCART    : 0-1 ‚úÖ
```

**Am√©lioration** : **√âcart r√©duit de 66-100%** (3 ‚Üí 0-1) üéâ

---

## ‚è±Ô∏è PLANNING D'IMPL√âMENTATION

### Semaine 1 : Collecte des Donn√©es

- [ ] Cr√©er script `rl_export_historical_data.py`
- [ ] Exporter 1000+ dispatches historiques
- [ ] Analyser les donn√©es (√©carts moyens, patterns)

**Effort** : 1-2 jours

---

### Semaine 2 : Entra√Ænement Offline

- [ ] Cr√©er script `rl_train_offline.py`
- [ ] Entra√Æner agent DQN (5000 episodes ‚âà 2-3h GPU)
- [ ] √âvaluer le mod√®le sur donn√©es de test

**Effort** : 2-3 jours (dont 2-3h calcul)

---

### Semaine 3 : Int√©gration

- [ ] Cr√©er `RLDispatchOptimizer`
- [ ] Int√©grer dans `engine.py`
- [ ] Tester sur dispatches r√©els
- [ ] Mesurer am√©lioration (√©cart avant/apr√®s)

**Effort** : 2-3 jours

---

### Semaine 4 : Validation & Production

- [ ] A/B testing (heuristique vs RL)
- [ ] Monitoring des m√©triques
- [ ] Ajustements si n√©cessaire
- [ ] D√©ploiement en production

**Effort** : 2-3 jours

---

## üéØ AVANTAGES DE CETTE APPROCHE

‚úÖ **Apprentissage sur donn√©es r√©elles**  
‚Üí L'agent apprend de VOS dispatches pass√©s, pas de simulations th√©oriques

‚úÖ **Am√©lioration continue**  
‚Üí R√©entra√Æner tous les mois avec nouvelles donn√©es = am√©lioration constante

‚úÖ **Pas de r√®gles manuelles**  
‚Üí L'agent d√©couvre les patterns optimaux automatiquement

‚úÖ **Adaptable**  
‚Üí S'adapte aux changements (nouveaux chauffeurs, nouvelles zones, etc.)

‚úÖ **Transparence**  
‚Üí Peut expliquer pourquoi une r√©assignation est sugg√©r√©e

---

## üìà M√âTRIQUES DE SUCC√àS

| M√©trique                | Avant | Objectif | Impact         |
| ----------------------- | ----- | -------- | -------------- |
| **√âcart max courses**   | 3     | ‚â§1       | **-66%**       |
| **Satisfaction √©quit√©** | 66%   | 90%      | **+24%**       |
| **Temps dispatch**      | 9s    | <12s     | +3s acceptable |
| **Retards**             | X     | -10%     | Bonus          |

---

## üî¨ EXP√âRIMENTATIONS FUTURES

Une fois le syst√®me en place, vous pourrez :

1. **Entra√Æner sur diff√©rents objectifs** :

   - Minimiser distance totale
   - Maximiser satisfaction client
   - R√©duire co√ªts carburant

2. **Ajouter des features contextuelles** :

   - M√©t√©o (pluie ‚Üí trafic)
   - Jour de la semaine (lundi vs vendredi)
   - √âv√©nements (match de foot ‚Üí trafic)

3. **Multi-agent RL** :

   - Plusieurs agents coop√©ratifs (1 par chauffeur)
   - Optimisation d√©centralis√©e

4. **Transfer Learning** :
   - Entra√Æner sur Geneva, appliquer √† Lausanne
   - Partager apprentissage entre filiales

---

## üìù DOCUMENTS ASSOCI√âS

- `backend/services/rl/dispatch_env.py` : Environnement Gym existant
- `backend/services/rl/dqn_agent.py` : Agent DQN existant
- `SYNTHESE_PROBLEME_EQUILIBRE_FINAL.md` : Analyse du probl√®me actuel

---

## üöÄ PROCHAINE √âTAPE IMM√âDIATE

**Cr√©er le script d'export des donn√©es** :

```bash
docker exec atmr-api bash -c "mkdir -p backend/data/rl/models"
docker exec atmr-api python backend/scripts/rl_export_historical_data.py
```

**R√©sultat attendu** :

```
üìä R√©cup√©ration de 1247 dispatch runs...
‚úÖ 1247 dispatches export√©s vers data/rl/historical_dispatches.json
üìä Statistiques:
   - √âcart moyen: 2.8
   - Score moyen: 72.3
```

---

**Voulez-vous que je cr√©e les scripts d'export et d'entra√Ænement maintenant ?** üöÄ
