# ğŸ§  EntraÃ®nement RL en Cours

**Date** : 21 octobre 2025, 23h20  
**Statut** : âœ… **EN COURS** (2% complÃ©tÃ©)

---

## ğŸ“Š Ã‰tat Actuel

```
Progression : 100/5000 Ã©pisodes (2.0%)
Ã‰cart moyen : 4.96 courses
DurÃ©e estimÃ©e : ~2-3 heures
```

**Processus en arriÃ¨re-plan** âœ…  
L'entraÃ®nement tourne dans le conteneur Docker `atmr-api-1`

---

## ğŸ“ˆ Suivre la Progression

### MÃ©thode 1 : Script de Monitoring

```bash
docker exec atmr-api-1 python backend/scripts/monitor_rl_training.py
```

### MÃ©thode 2 : Logs en Temps RÃ©el

```bash
docker exec atmr-api-1 tail -f data/rl/training_output.log
```

### MÃ©thode 3 : DerniÃ¨res Lignes

```bash
docker exec atmr-api-1 tail -20 data/rl/training_output.log
```

---

## ğŸ¯ Objectif de l'EntraÃ®nement

L'agent DQN apprend Ã  **minimiser l'Ã©cart de charge** entre chauffeurs :

### Ã‰tat Actuel (Heuristique)

```
Giuseppe : 5 courses  âŒ
Dris     : 3 courses
Yannis   : 2 courses
Ã‰CART    : 3
```

### Objectif (AprÃ¨s RL)

```
Giuseppe : 3-4 courses  âœ…
Dris     : 3-4 courses  âœ…
Yannis   : 3-4 courses  âœ…
Ã‰CART    : 0-1
```

---

## âš™ï¸ Configuration de l'EntraÃ®nement

| ParamÃ¨tre            | Valeur     | Description                                |
| -------------------- | ---------- | ------------------------------------------ |
| **Ã‰pisodes**         | 5000       | Nombre d'itÃ©rations d'apprentissage        |
| **Ã‰tat (dimension)** | 94         | Positions chauffeurs + bookings + contexte |
| **Actions**          | 61         | Assigner booking[i] â†’ driver[j]            |
| **Learning Rate**    | 0.0001     | Taux d'apprentissage                       |
| **Batch Size**       | 64         | Taille des batchs d'entraÃ®nement           |
| **Buffer Size**      | 10,000     | MÃ©moire des expÃ©riences                    |
| **Epsilon**          | 0.5 â†’ 0.01 | Exploration â†’ Exploitation                 |

---

## ğŸ“¦ RÃ©seau de Neurones

```
Q-Network (DQN)
â”œâ”€â”€ Input Layer    : 94 neurones (Ã©tat)
â”œâ”€â”€ Hidden Layer 1 : 256 neurones + ReLU
â”œâ”€â”€ Hidden Layer 2 : 256 neurones + ReLU
â””â”€â”€ Output Layer   : 61 neurones (Q-values)

Total : 220,733 paramÃ¨tres entraÃ®nables
```

---

## ğŸ”„ Sauvegardes Automatiques

Le modÃ¨le est **sauvegardÃ© tous les 100 Ã©pisodes** si amÃ©lioration dÃ©tectÃ©e :

```
ğŸ“‚ data/rl/models/dispatch_optimized_v1.pth
```

CritÃ¨res de sauvegarde :

1. **PrioritÃ©** : Ã‰cart de charge rÃ©duit (gap < meilleur_prÃ©cÃ©dent)
2. **Secondaire** : RÃ©compense amÃ©liorÃ©e (Ã  gap Ã©gal)

---

## â±ï¸ Timeline EstimÃ©e

| Temps    | Ã‰pisodes | Progression |
| -------- | -------- | ----------- |
| 0 min    | 0        | DÃ©marrage   |
| 15 min   | 500      | 10%         |
| 30 min   | 1000     | 20%         |
| 1h       | 2000     | 40%         |
| 1h30     | 3000     | 60%         |
| 2h       | 4000     | 80%         |
| **2h30** | **5000** | **100% âœ…** |

---

## ğŸ“Š MÃ©triques Suivies

### 1. Ã‰cart de Charge (Load Gap)

- **Actuel** : 4.96 courses
- **Objectif** : â‰¤1 course
- **Poids dans la rÃ©compense** : Critique (-20 Ã— gapÂ²)

### 2. RÃ©compense Totale

- **Actuelle** : -2369.80
- **Objectif** : > -500
- **Composition** :
  - Ã‰quitÃ© : -20 Ã— (Ã©cart)Â²
  - Bonus Ã©cart â‰¤1 : +100
  - Distance : -0.5 Ã— km

### 3. Distance Totale

- **Actuelle** : 0.0 km (donnÃ©es manquantes)
- **Objectif** : Minimiser
- **PrioritÃ©** : Secondaire (aprÃ¨s Ã©quitÃ©)

---

## ğŸš€ Prochaines Ã‰tapes

### Pendant l'EntraÃ®nement (maintenant)

- [x] Export des donnÃ©es historiques
- [x] Lancement de l'entraÃ®nement
- [ ] PrÃ©paration de l'intÃ©grateur RL
- [ ] Tests sur donnÃ©es de validation

### AprÃ¨s l'EntraÃ®nement (dans ~2-3h)

1. **Ã‰valuation du modÃ¨le** :

   - Charger `dispatch_optimized_v1.pth`
   - Tester sur dispatch du 22 octobre
   - Comparer : heuristique vs RL

2. **IntÃ©gration dans le dispatch** :

   - CrÃ©er `RLDispatchOptimizer`
   - Modifier `engine.py`
   - Activer en mode "auto"

3. **Validation** :
   - A/B testing (avec/sans RL)
   - Monitoring des mÃ©triques
   - Ajustements si nÃ©cessaire

---

## ğŸ›‘ ArrÃªter l'EntraÃ®nement

Si nÃ©cessaire (problÃ¨me, erreur, etc.) :

```bash
# Trouver le PID du processus Python
docker exec atmr-api-1 ps aux | grep "rl_train_offline"

# Tuer le processus
docker exec atmr-api-1 kill <PID>

# Ou redÃ©marrer le conteneur
docker restart atmr-api-1
```

---

## ğŸ“ Notes Importantes

### Limitations Actuelles

- **1 seul dispatch historique** exportÃ© (22 oct.)
  - IdÃ©al : 50-100 dispatches
  - Impact : GÃ©nÃ©ralisation limitÃ©e
- **CoordonnÃ©es GPS manquantes** pour certaines courses
  - Distance = 0 km dans les mÃ©triques
  - L'agent optimise donc principalement l'Ã©quitÃ©

### AmÃ©liorations Futures

1. **Exporter plus de dispatches** (semaine entiÃ¨re)
2. **Ajouter contexte temporel** (heure, jour de la semaine)
3. **IntÃ©grer donnÃ©es OSRM** (temps rÃ©el de trajet)
4. **Multi-objectif** (Ã©quitÃ© + distance + satisfaction client)

---

## ğŸ“ Ce Que l'Agent Apprend

L'agent DQN dÃ©couvre automatiquement :

1. **Patterns de charge** :

   - Ã‰viter d'assigner trop de courses au mÃªme chauffeur
   - Ã‰quilibrer la charge dÃ¨s le dÃ©but du dispatch

2. **Contraintes temporelles** :

   - Respecter les fenÃªtres de temps
   - Prioriser les urgences

3. **StratÃ©gies optimales** :
   - Quand attendre (action 0)
   - Quel chauffeur choisir pour chaque course
   - Comment minimiser l'Ã©cart final

---

## ğŸ“ Contact

En cas de question ou problÃ¨me :

- VÃ©rifier les logs : `data/rl/training_output.log`
- Monitoring : `monitor_rl_training.py`
- Documentation : `session/RL/PLAN_ENTRAINEMENT_DISPATCH_OPTIMAL.md`

---

**DerniÃ¨re mise Ã  jour** : 21 octobre 2025, 23:20  
**Prochaine vÃ©rification recommandÃ©e** : Dans 30 minutes (â‰ˆ20% complÃ©tÃ©)
