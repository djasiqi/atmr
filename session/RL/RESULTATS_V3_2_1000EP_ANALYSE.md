# ‚ö†Ô∏è R√©sultats Entra√Ænement V3.2 (1000 Episodes) - Analyse Compl√®te

**Date** : 21 octobre 2025, 13:50  
**Dur√©e** : ~40 minutes  
**Configuration** : 4 drivers (3R+1E), 25 bookings, retard RETOUR ‚â§ 20 min  
**Status** : ‚ùå **EFFONDREMENT APR√àS EPISODE 200**

---

## üìä **R√âSULTATS FINAUX**

| M√©trique            | R√©sultat             | Target              | √âcart          |
| ------------------- | -------------------- | ------------------- | -------------- |
| **Reward moyen**    | **-8,436.7**         | +2,000 √† +3,500     | **-524%** ‚ùå   |
| **Assignments**     | **7.7 / 25** (30.8%) | 23-24 / 25 (92-96%) | **-67.7%** ‚ùå  |
| **Late pickups**    | 2.5                  | < 3                 | ‚úÖ OK          |
| **Epsilon final**   | 0.055                | 0.055               | ‚úÖ OK          |
| **Meilleur mod√®le** | **Episode 200**      | Episode 1000        | ‚ùå D√©gradation |

---

## üìà **COURBE D'APPRENTISSAGE - EFFONDREMENT VISIBLE**

### **√âvolution du Reward (Evaluations)** :

| Episode  | Reward (Eval)  | Assignments          | Status            |
| -------- | -------------- | -------------------- | ----------------- |
| **50**   | -4,211         | 16.4 / 25 (65.6%)    | Apprentissage     |
| **100**  | **-3,099** ‚úÖ  | 18.3 / 25 (73.2%)    | Am√©lioration      |
| **150**  | -3,549         | 17.4 / 25 (69.6%)    | Stable            |
| **200**  | **-2,201** üèÜ  | 18.0 / 25 (72.0%)    | **MEILLEUR !**    |
| **250**  | -3,953 ‚ö†Ô∏è      | N/A                  | D√©but d√©gradation |
| **300**  | -3,846 ‚ö†Ô∏è      | N/A                  | D√©gradation       |
| **400**  | -4,880 ‚ö†Ô∏è      | N/A                  | D√©gradation       |
| **550**  | -4,224 ‚ö†Ô∏è      | 17.9 / 25            | D√©gradation       |
| **600**  | -4,802 ‚ö†Ô∏è      | 14.6 / 25 (58.4%)    | D√©gradation forte |
| **650**  | -6,039 ‚ùå      | 11.8 / 25 (47.2%)    | Effondrement      |
| **700**  | -5,506 ‚ùå      | 13.3 / 25 (53.2%)    | Effondrement      |
| **800**  | -5,670 ‚ùå      | 13.9 / 25 (55.6%)    | Effondrement      |
| **850**  | -6,901 ‚ùå      | 9.7 / 25 (38.8%)     | Effondrement      |
| **900**  | **-10,353** ‚ùå | **4.5 / 25 (18%)**   | **CATASTROPHE**   |
| **1000** | **-9,518** ‚ùå  | **7.7 / 25 (30.8%)** | Catastrophe       |

### **Graphique ASCII** :

```
Reward
    0 ‚î§
-2000 ‚î§         ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ ‚Üê MEILLEUR (Ep 200)
-4000 ‚î§    ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ     ‚ï∞‚îÄ‚îÄ‚îÄ‚ïÆ
-6000 ‚î§‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ                ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
-8000 ‚î§                          ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
-10000‚î§                                 ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ>
      Ep 50  150  250  350  450  550  650  750  900
```

**‚Üí PIC √† Episode 200, puis D√âGRADATION PROGRESSIVE ‚Üí EFFONDREMENT** ‚ùå

---

## üîç **DIAGNOSTIC : POURQUOI L'EFFONDREMENT ?**

### **Observation Critique : LOSS EXPLOSE** ‚ö°

| Episode  | Loss        | Status                     |
| -------- | ----------- | -------------------------- |
| **10**   | 61.6        | ‚úÖ Normal                  |
| **100**  | 194.4       | ‚úÖ Normal                  |
| **200**  | 240.9       | ‚úÖ Acceptable              |
| **400**  | 617.5       | ‚ö†Ô∏è √âlev√©                   |
| **600**  | 12,921      | ‚ùå **EXPLOSION !**         |
| **800**  | 271,513     | ‚ùå **CATASTROPHE !**       |
| **1000** | **850,044** | ‚ùå **DIVERGENCE TOTALE !** |

**‚Üí Le Q-Network DIVERGE apr√®s Episode 200-300 !** ‚ö°

---

## üö® **CAUSES IDENTIFI√âES**

### **1. REWARD FUNCTION ENCORE TROP PUNITIVE** ‚ö†Ô∏è

```python
Probl√®me:
‚îú‚îÄ Agent assigne 7.7 / 25 courses (30.8%)
‚îú‚îÄ 17.3 courses annul√©es
‚îî‚îÄ P√©nalit√©: 17.3 √ó -150 = -2,595

Agent apprend:
‚îî‚îÄ "Ne PAS assigner = √©viter p√©nalit√©s retards"
   ‚Üí Mais cause ANNULATIONS massives
   ‚Üí Reward encore plus n√©gatif
   ‚Üí Cercle vicieux ‚ùå
```

### **2. LEARNING RATE TROP √âLEV√â POUR 4 DRIVERS** ‚ö†Ô∏è

```
Learning rate: 0.00674
‚îú‚îÄ Optimal pour 3 drivers (V3.1) ‚úÖ
‚îú‚îÄ TROP √âLEV√â pour 4 drivers ‚ùå
‚îî‚îÄ Cause: State space 118 dim (vs 94)

√âtat plus complexe = LR doit √™tre PLUS BAS
‚Üí LR 0.00674 cause instabilit√©/divergence
```

### **3. P√âNALIT√â RETOUR 20 MIN TROP STRICTE** ‚ö†Ô∏è

```python
Retard RETOUR 25 min:
‚îú‚îÄ V3.1 (30 min max): P√©nalit√© -12.5 (tol√©r√©)
‚îî‚îÄ V3.2 (20 min max): P√©nalit√© -100 (hors tol√©rance ‚ùå)

Agent apprend:
‚îî‚îÄ "Risque retard RETOUR > 20 min = NE PAS ASSIGNER"
   ‚Üí Pr√©f√®re ne rien faire
   ‚Üí Annulations massives
```

---

## üìä **COMPARAISON V3.1 vs V3.2**

| Config   | Drivers | Bookings | Best Reward (Ep)    | Assignments       | R√©sultat        |
| -------- | ------- | -------- | ------------------- | ----------------- | --------------- |
| **V3.1** | 3       | 20       | **-233** (Ep 150)   | 12.7 / 20 (63.5%) | ‚ö†Ô∏è D√©gradation  |
| **V3.2** | 4       | 25       | **-2,201** (Ep 200) | 7.7 / 25 (30.8%)  | ‚ùå Effondrement |

**‚Üí V3.2 PIRE que V3.1 malgr√© plus de drivers !** ‚ùå

---

## üéØ **CAUSES RACINES**

### **1. Reward Function Inadapt√©e** üéØ

```
P√©nalit√©s actuelles TROP FORTES:
‚îú‚îÄ Annulation: -150 (imm√©diat) + -100 (fin √©pisode) = -250
‚îú‚îÄ Retard RETOUR > 20 min: -120 (vs -100 en V3.1)
‚îú‚îÄ Retard ALLER > 30 min: -150

Agent calcule:
‚îú‚îÄ Assigner avec risque retard 25 min RETOUR = -100 √† -120
‚îú‚îÄ Ne pas assigner = -150 (annulation)
‚îî‚îÄ Diff√©rence: seulement -30 √† -50

‚Üí Agent pr√©f√®re ATTENDRE plut√¥t qu'assigner ! ‚ùå
```

### **2. Learning Rate Non Adapt√©** ‚ö†Ô∏è

```
Optuna optimis√© pour:
‚îú‚îÄ 11 drivers, 10 bookings (State dim 90)
‚îî‚îÄ LR 0.00674 optimal

Production V3.2:
‚îú‚îÄ 4 drivers, 25 bookings (State dim 118)
‚îî‚îÄ LR 0.00674 TROP √âLEV√â (+31% state complexity)

Impact:
‚îî‚îÄ Gradients trop forts ‚Üí Oubli catastrophique ‚Üí Divergence
```

### **3. Complexit√© Augment√©e** üß†

```
V3.1 (3 drivers):
‚îú‚îÄ State dim: 94
‚îú‚îÄ Actions: 61
‚îî‚îÄ Q-Network: 220k params

V3.2 (4 drivers):
‚îú‚îÄ State dim: 118 (+25.5%)
‚îú‚îÄ Actions: 101 (+65.6%)
‚îî‚îÄ Q-Network: 238k params (+7.9%)

‚Üí Plus complexe = Besoin de:
   - LR plus bas
   - Reward function plus tol√©rante
   - Plus d'episodes (1500-2000)
```

---

## üí° **SOLUTIONS POSSIBLES**

### **SOLUTION A : Ajuster Reward Function (RECOMMAND√â)** ‚≠ê

**Changements √† faire** :

```python
1. R√âDUIRE p√©nalit√© annulation: -150 ‚Üí -100
2. AUGMENTER tol√©rance RETOUR: 20 min ‚Üí 25 min
3. R√âDUIRE p√©nalit√© retard RETOUR: -0.75 ‚Üí -0.4 par minute
4. AUGMENTER reward assignment: +300 ‚Üí +400

Objectif: Encourager agent √† ASSIGNER m√™me avec risque petit retard
```

### **SOLUTION B : R√©duire Learning Rate** üéì

**Nouveau LR optimal** :

```
LR actuel: 0.00674 (trop √©lev√© pour 4 drivers)
LR sugg√©r√©: 0.004-0.005 (40% reduction)

Relancer Optuna pour 4 drivers, 25 bookings
‚Üí Trouve LR optimal pour cette config
```

### **SOLUTION C : Retour √† 3 Drivers + Reward Ajust√©e** üîÑ

**Configuration** :

```
Drivers: 3 (2 REGULAR + 1 EMERGENCY)
Bookings: 25
Retard RETOUR: 25 min (compromis vs 20 min)
LR: 0.00674 (conserv√©)
Reward: Ajust√©e (plus tol√©rante)

Avantage: M√™me complexit√© que V3.1, mais plus de courses
```

---

## üöÄ **MA RECOMMANDATION**

### **SOLUTION A + C COMBIN√âES** ‚≠ê

**Configuration Propos√©e** :

| Param√®tre               | V3.2 (√©chec) | **V3.3 (propos√©)** | Justification           |
| ----------------------- | ------------ | ------------------ | ----------------------- |
| **Drivers**             | 4            | **3**              | R√©duire complexit√©      |
| **REGULAR**             | 3            | **2**              | Revenir √† config stable |
| **EMERGENCY**           | 1            | **1**              | Conserv√©                |
| **Bookings**            | 25           | **25**             | Garder volume r√©el      |
| **Retour tol√©rance**    | 20 min       | **25 min**         | Plus r√©aliste           |
| **P√©nalit√© annulation** | -150         | **-100**           | Moins punitive          |
| **Reward assignment**   | +300         | **+400**           | Plus incitatif          |
| **LR**                  | 0.00674      | **0.00674**        | Conserv√©                |

---

## üìã **PROCHAINES √âTAPES RECOMMAND√âES**

### **Option 1 : Ajuster Reward V3.3 (RECOMMAND√â)** ‚≠ê

```
1. Modifier dispatch_env.py:
   ‚îú‚îÄ Retard RETOUR: 25 min (vs 20 min)
   ‚îú‚îÄ P√©nalit√© annulation: -100 (vs -150)
   ‚îú‚îÄ Reward assignment: +400 (vs +300)
   ‚îî‚îÄ P√©nalit√© RETOUR: -0.4 (vs -0.75) par minute

2. Retour √† 3 drivers (2R+1E):
   ‚îî‚îÄ Config stable et prouv√©e

3. Tester 100 episodes:
   ‚îî‚îÄ Valider que reward function fonctionne

4. Si OK ‚Üí 1000 episodes
```

### **Option 2 : Relancer Optuna pour 4 Drivers** üîß

```
Trouver LR optimal pour:
‚îú‚îÄ 4 drivers
‚îú‚îÄ 25 bookings
‚îî‚îÄ Reward V3.2

Dur√©e: 10-15 minutes (50 trials)
```

### **Option 3 : Utiliser Meilleur Mod√®le (Episode 200)** üì¶

```
Charger le checkpoint Episode 200:
‚îî‚îÄ dqn_ep0200_r-3977.pth
   Reward: -2,200.6
   Assignments: 18.0 / 25 (72%)

√âvaluer si utilisable en production:
‚îî‚îÄ 72% assignments = acceptable pour semi-auto mode
```

---

## üî¨ **ANALYSE D√âTAILL√âE**

### **Point de Pic : Episode 200** üèÜ

```
Episode 200:
‚îú‚îÄ Reward (eval): -2,200.6 ‚úÖ MEILLEUR
‚îú‚îÄ Assignments: 18.0 / 25 (72%)
‚îú‚îÄ Epsilon: 0.559 (exploration/exploitation)
‚îî‚îÄ Loss: 240.9 (acceptable)

‚Üí Agent avait trouv√© un bon √©quilibre !
```

### **D√©gradation Progressive (Ep 200-600)** ‚ö†Ô∏è

```
Episode 200 ‚Üí 600:
‚îú‚îÄ Reward: -2,201 ‚Üí -4,802 (-118%)
‚îú‚îÄ Assignments: 18.0 ‚Üí 14.6 (-18.9%)
‚îú‚îÄ Loss: 240 ‚Üí 12,921 (+5,283%)
‚îî‚îÄ Epsilon: 0.559 ‚Üí 0.175 (moins d'exploration)

Cause: Learning rate trop √©lev√© + Epsilon trop bas
‚Üí Agent oublie strat√©gies apprises (catastrophic forgetting)
```

### **Effondrement Total (Ep 600-1000)** ‚ùå

```
Episode 600 ‚Üí 1000:
‚îú‚îÄ Reward: -4,802 ‚Üí -9,518 (-98%)
‚îú‚îÄ Assignments: 14.6 ‚Üí 7.7 (-47.3%)
‚îú‚îÄ Loss: 12,921 ‚Üí 850,044 (+6,477%)
‚îî‚îÄ Epsilon: 0.175 ‚Üí 0.055 (pure exploitation)

Agent compl√®tement diverg√©:
‚îî‚îÄ Choisit strat√©gie "Ne rien faire"
   ‚Üí Assignments chutent √† 30%
   ‚Üí Reward catastrophique
```

---

## üí° **LE√áON APPRISE**

### **Reward Function V3.2 EST TROP PUNITIVE** ‚ö†Ô∏è

```
Probl√®me fondamental:
‚îú‚îÄ P√©nalit√© annulation: -150 (trop forte)
‚îú‚îÄ P√©nalit√© retard RETOUR > 20 min: -120 (trop stricte)
‚îî‚îÄ Retard RETOUR 20 min: Trop stricte pour 25 courses

Agent apprend:
‚îî‚îÄ "Ne pas assigner = moins de p√©nalit√©s que assigner avec retard"
   ‚Üí Strat√©gie d'√©vitement
   ‚Üí Annulations massives
   ‚Üí Reward n√©gatif
```

---

## üéØ **PROPOSITION : REWARD FUNCTION V3.3**

### **Changements Propos√©s** :

| Aspect                       | V3.2 (√©chec)              | **V3.3 (propos√©)**              | Justification    |
| ---------------------------- | ------------------------- | ------------------------------- | ---------------- |
| **Reward assignment**        | +300                      | **+500**                        | Incitation FORTE |
| **P√©nalit√© annulation**      | -150 imm√©diat<br>-100 fin | **-80 imm√©diat**<br>**-70 fin** | Moins punitive   |
| **Retard RETOUR tol√©r√©**     | 20 min                    | **25 min**                      | Plus r√©aliste    |
| **P√©nalit√© RETOUR ‚â§ 25 min** | -0.75/min                 | **-0.3/min**                    | Moins stricte    |
| **P√©nalit√© RETOUR > 25 min** | -4/min (max -120)         | **-2/min (max -80)**            | Moins stricte    |

### **Ratios Nouveaux** :

```
V3.2 (√©chec):
‚îú‚îÄ Assignment: +300
‚îú‚îÄ Annulation: -250 (total)
‚îî‚îÄ Ratio: 1.2:1 (p√©nalit√© trop proche de reward)

V3.3 (propos√©):
‚îú‚îÄ Assignment: +500 ‚¨ÜÔ∏è
‚îú‚îÄ Annulation: -150 (total) ‚¨áÔ∏è
‚îî‚îÄ Ratio: 3.3:1 (reward CLAIREMENT sup√©rieur √† p√©nalit√©)

‚Üí Agent VEUT assigner ! ‚úÖ
```

---

## üìã **COMMANDE PROPOS√âE V3.3**

### **Test Rapide (100 episodes)** :

```bash
# Apr√®s ajustement reward function:
docker exec atmr-api-1 python scripts/rl/train_dqn.py \
  --episodes 100 \
  --num-drivers 3 \
  --max-bookings 25 \
  --simulation-hours 8 \
  --learning-rate 0.00674 \
  --gamma 0.9392 \
  --batch-size 64 \
  --epsilon-decay 0.9971 2>&1 | Tee-Object -FilePath "training_v3_3_test_100ep.txt"
```

**R√©sultats attendus (100 episodes)** :

- Reward : -1,000 √† +500
- Assignments : 20-22 / 25 (80-88%)

---

## ‚úÖ **R√âSUM√â EX√âCUTIF**

### **‚ùå √âCHEC V3.2**

| Aspect              | R√©sultat             | Cause                          |
| ------------------- | -------------------- | ------------------------------ |
| **Reward final**    | -8,436.7             | Reward function trop punitive  |
| **Assignments**     | 7.7 / 25 (30.8%)     | Agent √©vite d'assigner         |
| **Loss divergence** | 850,044              | LR trop √©lev√© + complexit√©     |
| **Point de pic**    | Episode 200 (-2,201) | Apr√®s, effondrement progressif |

### **‚úÖ ACTIONS RECOMMAND√âES**

1. **Ajuster reward function** (V3.3) :

   - +500 assignment (vs +300)
   - -150 annulation totale (vs -250)
   - 25 min tol√©rance RETOUR (vs 20 min)
   - P√©nalit√©s plus l√©g√®res

2. **Retour √† 3 drivers** :

   - R√©duire complexit√©
   - LR 0.00674 valid√© pour 3 drivers

3. **Test 100 episodes** avant final

---

## üöÄ **PROCHAINE √âTAPE**

**Voulez-vous que j'impl√©mente la Reward Function V3.3 ?** üéØ

**Changements** :

- ‚úÖ 3 drivers (2 REGULAR + 1 EMERGENCY)
- ‚úÖ 25 bookings (volume r√©el conserv√©)
- ‚úÖ Retard RETOUR 25 min (plus r√©aliste)
- ‚úÖ P√©nalit√©s r√©duites (encourager assignments)
- ‚úÖ Reward +500 (incitation forte)

---

**G√©n√©r√© le** : 21 octobre 2025, 13:50  
**Status** : ‚ùå V3.2 √©chou√© (effondrement Episode 200-1000)  
**Meilleur mod√®le** : Episode 200 (-2,201 reward, 72% assignments)  
**Cause** : Reward function trop punitive + LR trop √©lev√©  
**Solution** : Reward Function V3.3 (plus tol√©rante)
