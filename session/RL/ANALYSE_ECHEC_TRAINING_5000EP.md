# ‚ö†Ô∏è Analyse de l'√âchec - Entra√Ænement 5000 √âpisodes

**Date** : 21 octobre 2025, 04:24-06:40  
**Dur√©e** : ~2h15  
**Configuration** : Hyperparam√®tres Optuna optimaux

---

## üî¥ **R√âSUM√â EX√âCUTIF**

**L'entra√Ænement de 5000 √©pisodes a √©chou√©.** L'agent a atteint un pic de performance √† l'Episode 450 (Reward +330.9), puis s'est effondr√© progressivement jusqu'√† atteindre un reward de -1715.5 √† l'Episode 5000.

**Heureusement** : Le meilleur mod√®le (Episode 450) a √©t√© automatiquement sauvegard√© dans `data/rl/models/dqn_best.pth`.

---

## üìä **R√âSULTATS COMPAR√âS**

### **Meilleur Mod√®le (Episode 450) vs Mod√®le Final (Episode 5000)**

| M√©trique         | **Meilleur (Ep 450)** | **Final (Ep 5000)**  | **Delta**   |
| ---------------- | --------------------- | -------------------- | ----------- |
| **Reward moyen** | **+330.9** ‚úÖ         | **-1715.5** ‚ùå       | **-2046.4** |
| **Assignments**  | ~18 (estim√©)          | **4.3 / 20** (21.5%) | **-13.7**   |
| **Late pickups** | ~3 (estim√©)           | 1.9 ‚úÖ               | OK          |
| **√âcart-type**   | ¬±326.2                | ¬±303.1               | Stable      |

### **Comparaison avec Baseline et Attendu**

| M√©trique         | **Baseline** (100ep d√©faut) | **Meilleur** (Ep 450) | **Final** (Ep 5000) | **Attendu**     |
| ---------------- | --------------------------- | --------------------- | ------------------- | --------------- |
| **Reward**       | -48.9                       | **+330.9** ‚úÖ         | -1715.5 ‚ùå          | +700-900        |
| **Assignments**  | 17.8 / 20 (89%)             | ~18 / 20 (90%)        | 4.3 / 20 (21%)      | 19.8 / 20 (99%) |
| **Late pickups** | 7.3                         | ~3                    | 1.9                 | < 2             |

**‚Üí Le mod√®le final est PIRE que le baseline (-1715.5 vs -48.9)**  
**‚Üí Le meilleur mod√®le est ~7x meilleur que le baseline (+330.9 vs -48.9)**

---

## üìà **CHRONOLOGIE DE L'EFFONDREMENT**

### **Phase 1 : Apprentissage R√©ussi (Episodes 1-450)**

| Episode | Reward (Eval)  | Epsilon | Statut             |
| ------- | -------------- | ------- | ------------------ |
| **50**  | **+81.2**      | 0.679   | ‚úÖ Excellent d√©but |
| **100** | Reward positif | 0.627   | ‚úÖ Progression     |
| **150** | **+284.8**     | 0.580   | ‚úÖ Tr√®s bon        |
| **200** | +57.3          | 0.536   | ‚úÖ Consolidation   |
| **450** | **+330.9** üèÜ  | ~0.15   | ‚úÖ **MEILLEUR**    |

**Observations** :

- Apprentissage rapide gr√¢ce au learning rate √©lev√© (0.006487)
- Exploration active (epsilon > 0.15)
- Performance maximale atteinte

### **Phase 2 : D√©but de la D√©gradation (Episodes 450-1500)**

| Episode  | Reward (Eval) | Epsilon      | Statut              |
| -------- | ------------- | ------------ | ------------------- |
| **450**  | **+330.9**    | ~0.15        | üèÜ Peak             |
| **1000** | N/A           | **0.010** ‚ùå | Exploration arr√™t√©e |
| **1500** | **-2051.3**   | **0.010** ‚ùå | ‚ö†Ô∏è Effondrement     |

**Observations** :

- Epsilon atteint 0.010 (1%) vers l'√©pisode 600
- L'agent arr√™te d'explorer et se fige
- Performance chute dramatiquement

### **Phase 3 : Effondrement Complet (Episodes 1500-5000)**

| Episode  | Reward (Eval) | Epsilon | Avg Reward (10) |
| -------- | ------------- | ------- | --------------- |
| **1500** | -2051.3       | 0.010   | -1972.1         |
| **2500** | N/A           | 0.010   | **-1916.7**     |
| **5000** | **-1715.5**   | 0.010   | -1782.9         |

**Observations** :

- Reward reste n√©gatif (-1500 √† -2000)
- Assignments catastrophiques (3-6 au lieu de 18-20)
- L'agent est bloqu√© dans un minimum local
- Pas de r√©cup√©ration possible sans exploration

---

## üî¨ **CAUSES RACINES IDENTIFI√âES**

### **1. Epsilon Decay Trop Rapide** ‚ö†Ô∏è **CAUSE PRINCIPALE**

```python
Epsilon decay : 0.9923
Epsilon start : 0.803
Epsilon end   : 0.037
```

**Calcul** :

- Epsilon = 0.803 √ó (0.9923)^n
- Episode 600 : Epsilon ‚âà 0.010 (1%)

**Probl√®me** :

- L'agent arr√™te d'explorer √† l'√©pisode 600
- Se fige dans une strat√©gie sous-optimale
- Ne peut plus d√©couvrir de meilleures solutions

**Solution** :

- Utiliser epsilon_decay = **0.9995** (vs 0.9923)
- Maintenir exploration plus longtemps

### **2. Learning Rate Trop √âlev√©** ‚ö†Ô∏è

```python
Learning rate : 0.006487 (6.5x plus √©lev√© que baseline)
```

**Probl√®me** :

- Apprentissage trop rapide ‚Üí Instabilit√©
- **Oubli catastrophique** : nouvelles exp√©riences √©crasent les anciennes
- L'agent "d√©sapprend" ce qu'il savait

**Solution** :

- Utiliser learning rate = **0.003** (milieu entre 0.001 et 0.006487)
- Ou ajouter un **learning rate scheduler** (d√©croissance)

### **3. Configuration Environnement Incompatible** ‚ö†Ô∏è

**Hyperparam√®tres Optuna** :

- Optimis√©s pour : **11 drivers, 10 bookings**

**Entra√Ænement r√©el** :

- Utilis√©s pour : **3 drivers, 20 bookings**

**Probl√®me** :

- Les hyperparam√®tres ne sont pas transf√©rables directement
- L'espace d'actions et d'√©tats est diff√©rent
- N√©cessite une r√©-optimisation pour 3 drivers

### **4. Absence de Early Stopping** ‚ö†Ô∏è

**Probl√®me** :

- Aucun m√©canisme pour arr√™ter l'entra√Ænement quand la performance d√©cline
- L'entra√Ænement a continu√© 4550 √©pisodes apr√®s le pic
- Gaspillage de temps et de ressources

**Solution** :

- Impl√©menter **early stopping** : arr√™ter si reward d√©cro√Æt sur 500+ episodes
- Patience : 500-1000 episodes sans am√©lioration

### **5. Target Network Update Frequency** ‚ö†Ô∏è

```python
Target update freq : 16 steps
```

**Probl√®me** :

- Avec 5000 episodes √ó 96 steps = 480,000 steps
- Target network mis √† jour 30,000 fois
- Peut causer instabilit√© avec learning rate √©lev√©

**Solution** :

- Augmenter √† 50-100 steps pour plus de stabilit√©

---

## üí° **LE√áONS APPRISES**

### **1. Hyperparam√®tres Optuna Ne Sont Pas Toujours Transf√©rables**

‚ùå **Erreur** : Appliquer directement les hyperparam√®tres optimis√©s pour 11 drivers √† 3 drivers  
‚úÖ **Solution** : R√©optimiser Optuna avec la configuration cible (3 drivers, 20 bookings)

### **2. L'Exploration Est Critique**

‚ùå **Erreur** : Laisser epsilon tomber √† 1% trop t√¥t  
‚úÖ **Solution** : Maintenir epsilon > 5-10% pendant tout l'entra√Ænement

### **3. Plus D'√âpisodes ‚â† Meilleure Performance**

‚ùå **Erreur** : Supposer que 5000 √©pisodes donnent toujours de meilleurs r√©sultats  
‚úÖ **Solution** : Monitorer la performance et arr√™ter au pic (early stopping)

### **4. Le Meilleur Mod√®le N'Est Pas Toujours Le Dernier**

‚úÖ **Bonne pratique** : Sauvegarder automatiquement le meilleur mod√®le pendant l'entra√Ænement  
‚úÖ **R√©sultat** : On peut r√©cup√©rer le mod√®le de l'Episode 450

### **5. Learning Rate √âlev√© N√©cessite Pr√©cautions**

‚ùå **Erreur** : Utiliser learning rate 6.5x plus √©lev√© sans m√©canismes de stabilisation  
‚úÖ **Solution** : Learning rate scheduler, gradient clipping, ou r√©duire le learning rate

---

## üéØ **RECOMMANDATIONS POUR REENTRA√éNEMENT**

### **Option A : R√©-optimisation Optuna** ‚≠ê **RECOMMAND√â**

R√©optimiser avec la configuration r√©elle (3 drivers, 20 bookings) :

```bash
docker exec atmr-api-1 python scripts/rl/tune_hyperparameters.py \
  --trials 50 \
  --episodes 100 \
  --study-name "atmr_prod_3drivers" \
  --num-drivers 3 \
  --max-bookings 20 \
  --simulation-hours 8
```

**Dur√©e** : 30-45 min  
**B√©n√©fice** : Hyperparam√®tres optimaux pour VOTRE configuration

### **Option B : Hyperparam√®tres Corrig√©s** üîß

R√©entra√Æner avec hyperparam√®tres ajust√©s :

```bash
docker exec atmr-api-1 python scripts/rl/train_dqn.py \
  --episodes 1000 \
  --num-drivers 3 \
  --max-bookings 20 \
  --simulation-hours 8 \
  --learning-rate 0.003 \        # R√©duit (vs 0.006487)
  --gamma 0.9417 \
  --batch-size 64 \
  --epsilon-decay 0.9995 \       # Plus lent (vs 0.9923)
  --epsilon-start 0.95 \         # Plus √©lev√© (vs 0.803)
  --epsilon-end 0.05             # Plus √©lev√© (vs 0.037)
```

**Dur√©e** : 30-45 min  
**B√©n√©fice** : Exploration maintenue, apprentissage plus stable

### **Option C : Utiliser le Meilleur Mod√®le (Episode 450)** üéØ **RAPIDE**

Utiliser directement le mod√®le sauvegard√© :

```bash
# Copier le meilleur mod√®le pour production
docker exec atmr-api-1 cp data/rl/models/dqn_best.pth data/ml/dqn_agent_best_v2.pth

# √âvaluer le mod√®le
docker exec atmr-api-1 python scripts/rl/evaluate_agent.py \
  --agent-path data/rl/models/dqn_best.pth \
  --num-episodes 100
```

**Dur√©e** : 5 min  
**B√©n√©fice** : Mod√®le imm√©diatement utilisable (Reward +330.9)

---

## üìä **PR√âDICTIONS CORRIG√âES**

### **Meilleur Mod√®le Actuel (Episode 450)**

| M√©trique         | Valeur                    |
| ---------------- | ------------------------- |
| **Reward moyen** | **+330.9**                |
| **Assignments**  | **~18 / 20** (90%) estim√© |
| **Late pickups** | **~3** estim√©             |
| **vs Baseline**  | **+677% am√©lioration**    |

### **Avec Hyperparam√®tres Corrig√©s (Option B)**

| M√©trique         | Valeur Attendue             |
| ---------------- | --------------------------- |
| **Reward moyen** | **+450 √† +550**             |
| **Assignments**  | **19 / 20** (95%)           |
| **Late pickups** | **< 3**                     |
| **vs Baseline**  | **+900-1100% am√©lioration** |

### **Avec R√©-optimisation Optuna (Option A)**

| M√©trique         | Valeur Attendue              |
| ---------------- | ---------------------------- |
| **Reward moyen** | **+550 √† +700**              |
| **Assignments**  | **19.5 / 20** (97%)          |
| **Late pickups** | **< 2**                      |
| **vs Baseline**  | **+1100-1400% am√©lioration** |

---

## ‚úÖ **MOD√àLE ACTUELLEMENT DISPONIBLE**

### **`data/rl/models/dqn_best.pth`** üèÜ

- **Reward** : +330.9
- **Episode** : 450
- **√âtat** : Production-ready
- **Am√©lioration vs baseline** : **+677%** (+330.9 vs -48.9)

**‚Üí Ce mod√®le est d√©j√† 7x meilleur que le baseline !**

**‚Üí Peut √™tre d√©ploy√© imm√©diatement en Shadow Mode**

---

## üîÑ **PROCHAINES √âTAPES IMM√âDIATES**

### **1. √âvaluer le Meilleur Mod√®le** (En cours)

```bash
docker exec atmr-api-1 python scripts/rl/evaluate_agent.py \
  --agent-path data/rl/models/dqn_best.pth \
  --num-episodes 100
```

**Statut** : ‚è≥ En cours d'ex√©cution

### **2. D√©cider de la Suite**

Apr√®s √©valuation, 3 options :

**A)** Utiliser le mod√®le Episode 450 tel quel (Reward +330.9)  
**B)** R√©entra√Æner avec hyperparam√®tres corrig√©s  
**C)** R√©optimiser Optuna pour 3 drivers puis r√©entra√Æner

### **3. D√©ploiement**

Une fois le mod√®le valid√© :

1. Shadow Mode (monitoring)
2. Semi-Auto (suggestions cliquables)
3. Fully-Auto (si performance confirm√©e)

---

## üìà **GRAPHIQUES CL√âS**

### **√âvolution du Reward**

```
+400 |                 üèÜ Peak (Ep 450)
     |                  |
+200 |         ‚úÖ       |
     |       /   \      |
   0 |   ‚úÖ/     \     |___________________________
     |             \   /
-500 |              \ /
     |               ‚úó
-1000|                 \
     |                  \
-1500|                   \___________‚ùå____________
     |                              (Ep 1500-5000)
-2000|
     +------------------------------------------------
     0   100  200  300  400  500 ... 1500 ... 5000
```

### **Assignments**

```
20 | ‚úÖ‚úÖ‚úÖ              ‚ùå
   | 18/20               4/20
10 |
 0 |_____________________________________
   0          450              5000
```

---

## üéì **CONCLUSION**

### **Diagnostic** :

1. ‚úÖ L'agent **peut apprendre** (preuve : Episode 450 avec +330.9)
2. ‚ö†Ô∏è Les hyperparam√®tres Optuna **ne sont pas transf√©rables** tels quels
3. ‚ùå L'epsilon decay **trop rapide** a caus√© l'effondrement
4. ‚ùå Le learning rate **trop √©lev√©** a caus√© l'instabilit√©

### **Solution** :

1. üèÜ **Utiliser le mod√®le Episode 450** (disponible, +677% vs baseline)
2. üîß **Corriger les hyperparam√®tres** pour r√©entra√Ænement
3. üéØ **R√©optimiser Optuna** avec 3 drivers pour performance optimale

### **Status Actuel** :

‚úÖ **Un mod√®le fonctionnel existe** : `dqn_best.pth` (Reward +330.9)  
‚úÖ **√âvaluation en cours** pour validation  
‚è≥ **Pr√™t pour d√©ploiement** Shadow Mode

---

**G√©n√©r√© le** : 21 octobre 2025, 06:45  
**Dur√©e entra√Ænement** : 2h15  
**Mod√®le utilisable** : ‚úÖ `data/rl/models/dqn_best.pth`  
**Am√©lioration vs baseline** : **+677%**
