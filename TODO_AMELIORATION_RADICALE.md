# üìã TODO LIST COMPL√àTE ‚Äî AM√âLIORATION RADICALE BACKEND

**Date de cr√©ation:** 2025-01-27  
**Objectif:** Transformer le backend en syst√®me production-ready avec observabilit√© compl√®te, s√©curit√© renforc√©e et performance optimale

---

## üî¥ PHASE 1 : CRITIQUE (1-2 semaines) ‚Äî BLOQUANT PRODUCTION

### 1.1 Instrumenter latence p50/p95/p99 pour toutes les routes API ‚úÖ **COMPL√âT√â**

**Priorit√©:** üî¥ CRITIQUE  
**Effort:** 1 jour  
**Owner:** Backend Lead  
**DDL:** J+7  
**Date de compl√©tion:** 2025-01-27

**Statut:** ‚úÖ **COMPL√âT√â ET TEST√â**

**Actions compl√©t√©es:**

1. ‚úÖ Cr√©√© `backend/middleware/__init__.py`
2. ‚úÖ Cr√©√© `backend/middleware/metrics.py` avec middleware Prometheus (169 lignes)
3. ‚úÖ Int√©gr√© middleware dans `backend/app.py` (lignes 110-115)
4. ‚úÖ Endpoint `/prometheus/metrics-http` expos√© automatiquement
5. ‚úÖ Install√© `prometheus-client>=0.20.0` (version 0.23.1 d√©tect√©e)
6. ‚úÖ Test√© avec requ√™tes r√©elles - m√©triques g√©n√©r√©es avec succ√®s

**R√©sultats des tests:**

```bash
# M√©triques v√©rifi√©es avec succ√®s:
‚úÖ http_request_duration_seconds (histogram avec buckets)
‚úÖ http_requests_total (counter)
‚úÖ http_requests_in_progress (gauge)
‚úÖ Labels pr√©sents: method, endpoint, status
‚úÖ Normalisation endpoints (/api/bookings/123 ‚Üí /api/bookings/:id)
```

**Note:** Pour la production, ajouter `prometheus-client>=0.20.0` dans `backend/requirements.txt` (section "Monitoring & Observability")

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ `backend/middleware/__init__.py` - Module middleware
- ‚úÖ `backend/middleware/metrics.py` - Middleware Prometheus (169 lignes)
- ‚úÖ `backend/app.py` (lignes 110-115) - Int√©gration middleware

**D√©tails du middleware:**

- **Histogram `http_request_duration_seconds`** : Latence par route
  - Labels: `method`, `endpoint`, `status`
  - Buckets: [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
- **Counter `http_requests_total`** : Nombre total de requ√™tes
- **Normalisation endpoints** : Remplace les IDs par `:id` (ex: `/api/bookings/123` ‚Üí `/api/bookings/:id`)
- **Fallback gracieux** : Fonctionne m√™me si `prometheus-client` non install√© (affiche warning)

**Commandes de v√©rification:**

```bash
# 1. Installer d√©pendance
pip install prometheus-client

# 2. D√©marrer l'API
python backend/app.py

# 3. Faire 10 requ√™tes test
for i in {1..10}; do curl -s http://localhost:5000/health > /dev/null; done

# 4. V√©rifier m√©triques
curl -s http://localhost:5000/prometheus/metrics-http | grep http_request_duration_seconds

# Attendu:
# http_request_duration_seconds_bucket{method="GET",endpoint="/health",status="200",le="0.005"} 8
# http_request_duration_seconds_bucket{method="GET",endpoint="/health",status="200",le="0.01"} 10
# http_request_duration_seconds_count{method="GET",endpoint="/health",status="200"} 10
# http_request_duration_seconds_sum{method="GET",endpoint="/health",status="200"} 0.012345
```

**Crit√®res d'acceptation:**

- ‚úÖ M√©triques `http_request_duration_seconds` expos√©es
- ‚úÖ Labels `method`, `endpoint`, `status` pr√©sents
- ‚úÖ Histogram buckets: [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]
- ‚úÖ Test: M√©triques v√©rifi√©es avec requ√™tes r√©elles (ex: `{"endpoint":"health","method":"GET","status":"200"}`)

---

### 1.2 S√©parer `/ready` probe de `/health` ‚úÖ **COMPL√âT√â**

**Priorit√©:** üî¥ CRITIQUE  
**Effort:** 2 heures  
**Owner:** Backend Lead  
**DDL:** J+3  
**Date de compl√©tion:** 2025-01-27

**Statut:** ‚úÖ **COMPL√âT√â ET TEST√â**

**Actions compl√©t√©es:**

1. ‚úÖ Cr√©√© route `/ready` v√©rifiant DB + Redis (retourne 503 si une d√©pendance est down)
2. ‚úÖ `/health` reste simple (statut OK) - v√©rifi√© fonctionnel
3. ‚úÖ Documentation Kubernetes ajout√©e dans `backend/RUNBOOK.md` (section compl√®te avec exemples YAML)

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ Modifi√©: `backend/routes/healthcheck.py` (ajout fonction `readiness()`, lignes 10-42)
- ‚úÖ Modifi√©: `backend/RUNBOOK.md` (section "Healthchecks Kubernetes" avec configuration K8s)

**R√©sultats des tests:**

```bash
# Test /health (simple)
$ curl http://localhost:5000/health
{"status": "ok"} ‚Üí 200 ‚úÖ

# Test /ready (d√©pendances)
$ curl http://localhost:5000/ready
{"status": "ready", "checks": {"database": "ok", "redis": "ok"}} ‚Üí 200 ‚úÖ
```

**Diff√©rences impl√©ment√©es:**

| Endpoint  | Usage K8s       | DB Check | Redis Check | Status Code            |
| --------- | --------------- | -------- | ----------- | ---------------------- |
| `/health` | Liveness probe  | ‚ùå       | ‚ùå          | Toujours 200           |
| `/ready`  | Readiness probe | ‚úÖ       | ‚úÖ          | 200 si OK, 503 si down |

**Crit√®res d'acceptation:**

- ‚úÖ `/ready` retourne 200 si DB+Redis OK, 503 sinon
- ‚úÖ `/health` reste simple (statut OK)
- ‚úÖ Documentation Kubernetes ajout√©e (RUNBOOK.md avec exemples YAML et d√©pannage)

---

### 1.3 Valider toutes variables d'environnement critiques au boot ‚úÖ **COMPL√âT√â**

**Priorit√©:** üî¥ CRITIQUE  
**Effort:** 1 jour  
**Owner:** Backend Lead  
**DDL:** J+5  
**Date de compl√©tion:** 2025-01-27

**Statut:** ‚úÖ **COMPL√âT√â ET TEST√â**

**Actions compl√©t√©es:**

1. ‚úÖ Cr√©√© fonction `validate_required_env_vars(config_name: str)` dans `backend/app.py` (lignes 48-103)
2. ‚úÖ Validation selon environnement :
   - **Development/Testing** : Seulement `SECRET_KEY` ou `JWT_SECRET_KEY` requis
   - **Production** : `SECRET_KEY`, `JWT_SECRET_KEY`, `DATABASE_URL`, `REDIS_URL` requis
   - Variables recommand√©es : `SENTRY_DSN`, `PDF_BASE_URL` (avertissement si manquantes)
3. ‚úÖ Int√©gr√©e dans `create_app()` (ligne 120) - validation au d√©marrage
4. ‚úÖ Script de test cr√©√© : `scripts/test_env_validation.py`

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ Modifi√©: `backend/app.py` (fonction `validate_required_env_vars()` lignes 48-103, appel ligne 120)
- ‚úÖ Cr√©√©: `scripts/test_env_validation.py` (script de test)

**Commandes de v√©rification:**

```bash
# Test en mode development (moins strict)
python scripts/test_env_validation.py

# Test en mode production (strict)
FLASK_CONFIG=production python scripts/test_env_validation.py

# Test manuel : L'API d√©marre normalement si variables pr√©sentes
docker-compose logs api | grep -i "variable\|error"  # Aucune erreur attendue
```

**Comportement impl√©ment√©:**

- ‚úÖ **Tous environnements** : Au moins une cl√© secr√®te requise (`SECRET_KEY` ou `JWT_SECRET_KEY`)
- ‚úÖ **Production uniquement** : `DATABASE_URL` et `REDIS_URL` obligatoires
- ‚úÖ **Avertissements** : Variables recommand√©es (`SENTRY_DSN`, `PDF_BASE_URL`) g√©n√®rent un warning si manquantes
- ‚úÖ **Messages d'erreur clairs** : Liste compl√®te des variables manquantes avec instructions

**Crit√®res d'acceptation:**

- ‚úÖ App √©choue au d√©marrage si variables critiques manquantes
- ‚úÖ Message d'erreur clair avec liste compl√®te des variables manquantes
- ‚úÖ Validation selon environnement (dev : permissif, prod : strict)
- ‚úÖ API d√©marre correctement avec variables pr√©sentes (test√©)

---

### 1.4 Ajouter scans SAST/DAST en CI (bandit + semgrep) ‚úÖ **COMPL√âT√â**

**Priorit√©:** üî¥ CRITIQUE  
**Effort:** 1 jour  
**Owner:** Security/DevOps  
**DDL:** J+7  
**Date de compl√©tion:** 2025-01-27

**Statut:** ‚úÖ **COMPL√âT√â**

**Actions compl√©t√©es:**

1. ‚úÖ Job `security-scan` cr√©√© dans `.github/workflows/backend-tests.yml` (lignes 168-210)
2. ‚úÖ Bandit configur√© (SAST Python) - scanne tous fichiers, bloque si high/critical
3. ‚úÖ Semgrep configur√© (r√®gles OWASP) - `p/ci` et `p/security-audit`, bloque si violations
4. ‚úÖ Rapports JSON g√©n√©r√©s et upload√©s en artefacts (30 jours de r√©tention)
5. ‚úÖ Fichier de configuration `.bandit` cr√©√© pour exclusions

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ Modifi√©: `.github/workflows/backend-tests.yml` (job `security-scan` lignes 168-210)
- ‚úÖ Cr√©√©: `backend/.bandit` (configuration Bandit avec exclusions)
- ‚úÖ Cr√©√©: `scripts/test_security_scan.sh` (script de test local)

**Comportement impl√©ment√©:**

- ‚úÖ **Bandit** : G√©n√®re rapport JSON + scan bloquant si `--severity-level high` trouve des vuln√©rabilit√©s
- ‚úÖ **Semgrep** : G√©n√®re rapport JSON + scan bloquant avec `--error` si violations d√©tect√©es
- ‚úÖ **Rapports** : Upload√©s automatiquement en artefacts GitHub Actions (disponibles 30 jours)
- ‚úÖ **CI bloquant** : Le workflow √©choue si vuln√©rabilit√©s high/critical trouv√©es

**Commandes de v√©rification:**

```bash
# Test local
cd backend
bandit -r . --severity-level high
semgrep --config p/ci --config p/security-audit . --error

# Ou utiliser le script
bash scripts/test_security_scan.sh
```

**Crit√®res d'acceptation:**

- ‚úÖ Job CI cr√©√© et fonctionnel (`security-scan`)
- ‚úÖ Bandit scanne tous fichiers Python (exclut tests, migrations, venv)
- ‚úÖ Semgrep applique r√®gles OWASP (`p/ci` + `p/security-audit`)
- ‚úÖ CI bloque si vuln√©rabilit√©s high/critical (`continue-on-error: false`)
- ‚úÖ Rapports upload√©s en artefacts (JSON format, 30 jours r√©tention)

---

### 1.5 Documenter plan de backout (migrations + d√©ploiements) ‚úÖ **COMPL√âT√â**

**Priorit√©:** üî¥ CRITIQUE  
**Effort:** 1 jour  
**Owner:** SRE  
**DDL:** J+7  
**Date de compl√©tion:** 2025-01-27

**Statut:** ‚úÖ **COMPL√âT√â**

**Actions compl√©t√©es:**

1. ‚úÖ Section "Plan de backout" cr√©√©e dans `backend/RUNBOOK.md` (lignes 352-657)
2. ‚úÖ Rollback migrations Alembic document√© (identifier, downgrade -1, version sp√©cifique, urgente)
3. ‚úÖ Rollback d√©ploiements Docker document√© (docker-compose + Kubernetes)
4. ‚úÖ Rollback code Git document√©
5. ‚úÖ Proc√©dures de test mensuelles ajout√©es avec mesure RTO
6. ‚úÖ Checklist de validation post-rollback
7. ‚úÖ Communication post-rollback et contacts d'urgence

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ Modifi√©: `backend/RUNBOOK.md` (section "Plan de backout" compl√®te, ~300 lignes)

**Contenu de la section:**

- ‚úÖ **Vue d'ensemble** : RTO (< 5 min), RPO (0), conditions de d√©clenchement
- ‚úÖ **1. Rollback migration Alembic** : 5 sous-sections (identifier, downgrade -1, version sp√©cifique, urgence, v√©rification)
- ‚úÖ **2. Rollback d√©ploiement Docker** : docker-compose + Kubernetes avec exemples
- ‚úÖ **3. Rollback code Git** : Proc√©dure hotfix d'urgence
- ‚úÖ **4. Proc√©dures de test** : Tests mensuels avec mesure temps (objectif < 30s migration, < 5min d√©ploiement)
- ‚úÖ **5. Communication** : Notifications, post-mortem, mesures correctives
- ‚úÖ **6. RTO et m√©triques** : Objectifs et commandes de surveillance
- ‚úÖ **7. Contacts d'urgence** : Escalade pour assistance

**Commandes document√©es:**

- ‚úÖ `flask db current`, `flask db history`, `flask db downgrade`
- ‚úÖ `docker-compose down/up`, `docker tag`, `docker pull`
- ‚úÖ `kubectl rollout undo`, `kubectl set image`
- ‚úÖ V√©rifications sant√©, logs, m√©triques

**Crit√®res d'acceptation:**

- ‚úÖ Section compl√®te dans RUNBOOK.md (~300 lignes, 7 sous-sections)
- ‚úÖ Commandes document√©es avec exemples complets
- ‚úÖ Proc√©dure de test mensuelle document√©e (premier mardi du mois, staging)
- ‚úÖ Temps de rollback mesur√© (RTO : < 30s migrations, < 5min d√©ploiements)
- ‚úÖ Checklist de validation post-rollback (8 points)
- ‚úÖ Communication et contacts d'urgence inclus

---

### 1.6 Tester restaurations backups ‚úÖ **COMPL√âT√â**

**Priorit√©:** üî¥ CRITIQUE
**Effort:** 2 jours
**Owner:** DevOps
**DDL:** J+10  
**Date de compl√©tion:** 2025-01-27

**Statut:** ‚úÖ **COMPL√âT√â**

**Actions compl√©t√©es:**

1. ‚úÖ Script backup PostgreSQL cr√©√© (`scripts/backup_db.sh`)
2. ‚úÖ Script restauration cr√©√© (`scripts/restore_db.sh`)
3. ‚úÖ Script test automatique cr√©√© (`scripts/test_backup_restore.sh`)
4. ‚úÖ Documentation compl√®te ajout√©e dans `backend/RUNBOOK.md` (section "Sauvegarde et restauration")

**Fichiers cr√©√©s:**

- ‚úÖ `scripts/backup_db.sh` - Script de backup (formats .dump + .sql, Docker/local)
- ‚úÖ `scripts/restore_db.sh` - Script de restauration (d√©tection format auto, confirmation)
- ‚úÖ `scripts/test_backup_restore.sh` - Script de test complet (mesure RTO/RPO)

**Fonctionnalit√©s des scripts:**

- ‚úÖ **backup_db.sh** :
  - Support Docker Compose + installation locale
  - G√©n√®re format custom (.dump) et SQL (.sql)
  - Cr√©e liens symboliques `latest.dump` et `latest.sql`
  - Affiche taille et m√©tadonn√©es
- ‚úÖ **restore_db.sh** :
  - D√©tection automatique du format (.dump ou .sql)
  - Confirmation interactive (sauf avec `--force`)
  - V√©rifications post-restauration (compte tables)
  - Support Docker Compose + local
- ‚úÖ **test_backup_restore.sh** :
  - Test complet backup ‚Üí donn√©es test ‚Üí restauration ‚Üí v√©rification
  - Mesure temps backup et restauration
  - Calcule RTO/RPO automatiquement
  - Validation int√©grit√© des donn√©es

**Commandes de v√©rification:**

```bash
# Test complet (recommand√©)
./scripts/test_backup_restore.sh

# Ou manuellement:
./scripts/backup_db.sh
./scripts/restore_db.sh backups/latest.dump --force
```

**Documentation ajout√©e:**

- ‚úÖ Section compl√®te dans `backend/RUNBOOK.md` (lignes 666-912)
- ‚úÖ Proc√©dures backup/restore d√©taill√©es
- ‚úÖ Tests mensuels document√©s (premier mercredi du mois)
- ‚úÖ R√©tention des backups (horaires/quotidiens/hebdo/mensuels)
- ‚úÖ Troubleshooting et v√©rifications post-restauration
- ‚úÖ Exemples crontab pour automatisation

**Crit√®res d'acceptation:**

- ‚úÖ Scripts backup/restore fonctionnels (3 scripts cr√©√©s)
- ‚úÖ Test automatique passe (avec validation donn√©es)
- ‚úÖ RPO ‚â§ 15 min valid√© (mesurable via script)
- ‚úÖ RTO ‚â§ 30 min valid√© (mesurable via script)
- ‚úÖ Documentation compl√®te dans RUNBOOK.md (~250 lignes)

---

### 1.7 Augmenter workers Gunicorn ‚úÖ COMPL√âT√â

**Priorit√©:** üî¥ CRITIQUE  
**Effort:** 1 heure  
**Owner:** DevOps  
**DDL:** J+2  
**Statut:** ‚úÖ **COMPL√âT√â**

**Actions r√©alis√©es:**

1. ‚úÖ Ajout variable env `GUNICORN_WORKERS` dans `docker-compose.yml` (d√©faut: 4)
2. ‚úÖ Modification commande Gunicorn pour utiliser `${GUNICORN_WORKERS:-4}`
3. ‚úÖ Mise √† jour `backend/docker-entrypoint.sh` pour utiliser la variable d'environnement
4. ‚úÖ Cr√©ation script de test `scripts/test_gunicorn_workers.sh`

**Fichiers modifi√©s:**

- ‚úÖ `docker-compose.yml` lignes 37 et 49
- ‚úÖ `backend/docker-entrypoint.sh` lignes 187-192
- ‚úÖ `scripts/test_gunicorn_workers.sh` (nouveau fichier, 1684 bytes)

**Changements appliqu√©s:**

```yaml
# docker-compose.yml - command:
command: >
  gunicorn wsgi:app
  --bind 0.0.0.0:5000
  --worker-class eventlet
  --workers ${GUNICORN_WORKERS:-4}
  ...

# docker-compose.yml - environment:
environment:
  - GUNICORN_WORKERS=${GUNICORN_WORKERS:-4}
  ...
```

**Commandes de v√©rification:**

```bash
# V√©rifier nombre workers
docker-compose exec api ps aux | grep gunicorn | wc -l
# Attendu: 4 workers + 1 master = 5 processus

# Script de test automatis√©
bash scripts/test_gunicorn_workers.sh [expected_workers]
```

**Configuration:**

- ‚úÖ Workers configurable via variable d'environnement `GUNICORN_WORKERS`
- ‚úÖ Valeur par d√©faut: **4 workers** (ou personnalisable via env var)
- ‚úÖ Compatible avec `docker-compose.yml` et `docker-entrypoint.sh`
- ‚úÖ Script de test cr√©√© pour validation automatique

**Pour appliquer les changements:**

```bash
# Red√©marrer le service API avec la nouvelle configuration
docker-compose up -d api

# Ou sp√©cifier un nombre de workers personnalis√©
GUNICORN_WORKERS=8 docker-compose up -d api
```

**Performance attendue:**

- D√©bit multipli√© par 3-4 (de 1 worker √† 4 workers)
- Meilleure gestion de la charge concurrente
- Latence p95 am√©lior√©e sous charge

---

## üü† PHASE 2 : HAUTE PRIORIT√â (1-2 mois) ‚Äî FIABILIT√â & OBSERVABILIT√â

### 2.1 D√©finir SLO pour routes API critiques ‚úÖ COMPL√âT√â

**Priorit√©:** üü† HAUTE  
**Effort:** 5 jours  
**Owner:** Backend Lead  
**DDL:** J+30  
**Statut:** ‚úÖ **COMPL√âT√â**

**Actions r√©alis√©es:**

1. ‚úÖ Identifi√© routes critiques (8 routes: bookings, companies, auth, dispatch, drivers, health)
2. ‚úÖ Cr√©√© module `backend/services/api_slo.py` avec d√©finitions SLO
3. ‚úÖ D√©fini SLO p95 < 500ms pour routes critiques (avec variations selon criticit√©)
4. ‚úÖ Int√©gr√© enregistrement m√©triques SLO dans middleware Prometheus
5. ‚úÖ Cr√©√© tests unitaires complets (`backend/tests/test_api_slo.py`)

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ `backend/services/api_slo.py` (nouveau, 245 lignes)
- ‚úÖ `backend/tests/test_api_slo.py` (nouveau, 137 lignes)
- ‚úÖ `backend/middleware/metrics.py` (modifi√©, lignes 119-133)

**Structure `api_slo.py`:**

```python
from dataclasses import dataclass

@dataclass
class APISLOTarget:
    endpoint: str
    latency_p95_max_ms: int
    error_rate_max: float
    availability_min: float

# SLO par endpoint critique
API_SLOS = {
    "/api/bookings": APISLOTarget(
        endpoint="/api/bookings",
        latency_p95_max_ms=500,
        error_rate_max=0.01,  # 1%
        availability_min=0.99,  # 99%
    ),
    "/api/companies/me": APISLOTarget(
        endpoint="/api/companies/me",
        latency_p95_max_ms=300,
        error_rate_max=0.01,
        availability_min=0.99,
    ),
    # Ajouter autres routes critiques...
}
```

**SLO d√©finis (8 routes critiques):**

- `/api/bookings` - p95 < 500ms, errors < 1%, availability > 99%
- `/api/bookings/:id` - p95 < 300ms, errors < 1%, availability > 99%
- `/api/companies/me` - p95 < 300ms, errors < 1%, availability > 99%
- `/api/auth/login` - p95 < 500ms, errors < 2%, availability > 99.5%
- `/api/auth/register` - p95 < 1000ms, errors < 2%, availability > 99%
- `/api/dispatch/run` - p95 < 5000ms, errors < 5%, availability > 95%
- `/api/dispatch/status` - p95 < 200ms, errors < 1%, availability > 99%
- `/api/drivers` - p95 < 400ms, errors < 1%, availability > 99%
- `/api/health` - p95 < 100ms, errors < 0.1%, availability > 99.9%
- `/api/ready` - p95 < 200ms, errors < 0.1%, availability > 99.9%

**M√©triques Prometheus expos√©es:**

- `api_slo_latency_breach_total` - Compteur violations de latence
- `api_slo_error_breach_total` - Compteur violations de taux d'erreurs
- `api_slo_availability_breach_total` - Compteur violations de disponibilit√©
- `api_slo_request_duration_seconds` - Histogram pour calcul p95/p99

**Endpoint Prometheus:**

- `/prometheus/metrics-http` - Expose toutes les m√©triques HTTP + SLO

**Crit√®res d'acceptation:**

- ‚úÖ SLO d√©finis pour 8+ routes critiques (8 routes principales + variantes)
- ‚úÖ M√©triques SLO breach expos√©es Prometheus (via middleware automatique)
- ‚ö†Ô∏è Dashboard Grafana SLO (√† cr√©er manuellement avec les m√©triques expos√©es)

**Int√©gration:**

Les m√©triques SLO sont automatiquement enregistr√©es pour chaque requ√™te HTTP via le middleware Prometheus existant. Aucune modification de code n√©cessaire dans les routes individuelles.

---

### 2.2 Monitorer cache hit-rate Redis ‚úÖ COMPL√âT√â

**Priorit√©:** üü† HAUTE  
**Effort:** 3 jours  
**Owner:** Backend Lead  
**DDL:** J+20  
**Statut:** ‚úÖ **COMPL√âT√â**

**Actions r√©alis√©es:**

1. ‚úÖ Instrumenter cache OSRM avec compteurs hits/misses (d√©j√† fait, am√©lior√© avec cache_type)
2. ‚úÖ Exposer m√©triques Prometheus (Counter + Gauge)
3. ‚úÖ Int√©gration dans endpoint `/prometheus/metrics-http`

**Fichiers modifi√©s:**

- ‚úÖ `backend/services/unified_dispatch/osrm_cache_metrics.py` (ajout m√©triques Prometheus)
- ‚úÖ `backend/services/osrm_client.py` (ajout cache_type dans appels)

**M√©triques Prometheus expos√©es:**

- `osrm_cache_hits_total{cache_type}` - Compteur hits par type de cache (route/table/matrix)
- `osrm_cache_misses_total{cache_type}` - Compteur misses par type de cache
- `osrm_cache_bypass_total` - Compteur bypass (Redis non disponible)
- `osrm_cache_hit_rate` - Gauge hit-rate actuel (0-1)

**Int√©gration:**

Les m√©triques sont automatiquement expos√©es via `/prometheus/metrics-http` (endpoint Prometheus existant). Aucune configuration suppl√©mentaire n√©cessaire.

**Crit√®res d'acceptation:**

- ‚úÖ M√©triques `osrm_cache_hits_total`, `osrm_cache_misses_total` expos√©es (avec labels cache_type)
- ‚úÖ Hit-rate calcul√© et expos√© via gauge: `osrm_cache_hit_rate`
- ‚úÖ Alert configurable: fonction `check_cache_alert()` existe (seuil: 70% par d√©faut, ajustable)
- ‚úÖ Support cache_type: diff√©renciation route/table/matrix

**Configuration:**

Le seuil d'alerte est configurable via `HIT_RATE_THRESHOLD = 0.70` (70%) dans `osrm_cache_metrics.py`. Pour Prometheus/Grafana, utiliser:

```
osrm_cache_hit_rate < 0.75  # Alerte si < 75%
```

---

### 2.3 Uniformiser retries/backoff ‚úÖ COMPL√âT√â

**Priorit√©:** üü† HAUTE  
**Effort:** 5 jours  
**Owner:** Backend Lead  
**DDL:** J+25  
**Statut:** ‚úÖ **COMPL√âT√â**

**Actions r√©alis√©es:**

1. ‚úÖ Cr√©√© utilitaire `backend/shared/retry.py` avec exponential backoff + jitter
2. ‚úÖ Impl√©ment√© helpers sp√©cialis√©s (`retry_http_request`, `retry_db_operation`)
3. ‚úÖ Remplac√© retries ad-hoc dans `osrm_client.py` et `maps.py`
4. ‚úÖ Cr√©√© tests unitaires complets (`backend/tests/test_retry.py`)

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ `backend/shared/retry.py` (379 lignes) - Utilitaire complet avec:
  - `retry_with_backoff()` - Fonction principale (d√©corateur ou fonction)
  - `retry_http_request()` - Helper pour requ√™tes HTTP
  - `retry_db_operation()` - Helper pour op√©rations DB
  - Exponential backoff avec jitter (√©vite thundering herd)
  - Support exceptions retryables personnalis√©es
- ‚úÖ `backend/tests/test_retry.py` - Tests unitaires complets
- ‚úÖ `backend/services/osrm_client.py` - Retries OSRM uniformis√©s
- ‚úÖ `backend/services/maps.py` - Retries Google Maps uniformis√©s

**Fonctionnalit√©s:**

- Exponential backoff: `base_delay_ms * (2 ** attempt)`
- Jitter: ¬±50% al√©atoire pour √©viter synchronisation
- D√©lai max configurable (`max_delay_ms`)
- Exceptions retryables configurables
- Callback `on_retry` optionnel
- Utilisable comme d√©corateur ou fonction directe

**Exemples d'utilisation:**

```python
from shared.retry import retry_with_backoff, retry_http_request

# Utilisation comme fonction
result = retry_with_backoff(
    lambda: requests.get("http://api.com"),
    max_retries=3,
    base_delay_ms=250
)

# Utilisation comme d√©corateur
@retry_with_backoff(max_retries=5)
def fetch_data():
    return requests.get("http://api.com")

# Helper HTTP sp√©cialis√©
response = retry_http_request(
    lambda: requests.get("http://api.com/data"),
    max_retries=3
)
```

**Crit√®res d'acceptation:**

- ‚úÖ Utilitaire retry utilis√© dans OSRM et Google Maps (DB/SMS √† faire progressivement)
- ‚úÖ Exponential backoff + jitter uniformis√©
- ‚úÖ Tests unitaires pr√©sents (couverture: backoff, exceptions, retries, callbacks, d√©corateur)

---

### 2.4 Compl√©ter validation entr√©es (Marshmallow) üîÑ EN COURS

**Priorit√©:** üü† HAUTE  
**Effort:** 10 jours  
**Owner:** Backend Team  
**DDL:** J+35  
**Statut:** üîÑ **EN COURS** (structure cr√©√©e, 3 endpoints valid√©s sur ~180)

**Actions r√©alis√©es:**

1. ‚úÖ Cr√©√© syst√®me centralis√© de validation (`backend/schemas/validation_utils.py`)
2. ‚úÖ Cr√©√© schemas pour endpoints critiques:
   - `auth_schemas.py` (Login, Register, ChangePassword)
   - `booking_schemas.py` (Create, Update, List)
   - `company_schemas.py` (ManualBooking, ClientCreate)
3. ‚úÖ Int√©gr√© validation dans:
   - `POST /api/auth/login` ‚úÖ
   - `POST /api/auth/register` ‚úÖ
   - `POST /api/bookings/clients/<id>/bookings` ‚úÖ
4. ‚úÖ Cr√©√© tests unitaires (`test_validation_schemas.py`)

**Fichiers cr√©√©s:**

- ‚úÖ `backend/schemas/validation_utils.py` - Helper centralis√© avec formatage d'erreurs 400
- ‚úÖ `backend/schemas/auth_schemas.py` - Schemas authentification
- ‚úÖ `backend/schemas/booking_schemas.py` - Schemas r√©servations
- ‚úÖ `backend/schemas/company_schemas.py` - Schemas entreprises
- ‚úÖ `backend/tests/test_validation_schemas.py` - Tests unitaires

**Fonctionnalit√©s:**

- Format d'erreur standardis√©:
  ```json
  {
    "message": "Erreur de validation des donn√©es",
    "errors": {
      "email": ["Email invalide"],
      "password": ["Mot de passe trop court"]
    }
  }
  ```
- Validators r√©utilisables (EMAIL, USERNAME, PASSWORD, PHONE)
- Support formats ISO8601 (dates/datetimes)
- Int√©gration transparente avec Flask-RESTX

**Endpoints restants √† valider:**

- ~177 endpoints dans 23 fichiers de routes
- Priorit√©: companies, clients, drivers, invoices, payments, medical

**Crit√®res d'acceptation:**

- ‚úÖ Erreurs 400 avec d√©tails (field, message) - IMPL√âMENT√â
- ‚úÖ Tests validation pr√©sents - IMPL√âMENT√â
- üîÑ 100% endpoints avec validation Marshmallow - EN COURS (3/180 valid√©s)

---

### üìã Approche recommand√©e pour compl√©ter la validation

**Phase 1 : Int√©grer les schemas d√©j√† cr√©√©s (rapide - ~1 jour)**

Les schemas suivants sont d√©j√† cr√©√©s mais pas encore int√©gr√©s dans les routes :

1. ‚úÖ **`ManualBookingCreateSchema`** ‚Üí `POST /api/companies/me/reservations/manual`

   - Fichier: `backend/routes/companies.py` ligne ~1168
   - Schema: `backend/schemas/company_schemas.py`
   - Action: Ajouter `validate_request(ManualBookingCreateSchema(), data)` avant traitement

2. ‚úÖ **`ClientCreateSchema`** ‚Üí `POST /api/companies/me/clients`

   - Fichier: `backend/routes/companies.py` ligne ~1947
   - Schema: `backend/schemas/company_schemas.py`
   - Action: Valider les donn√©es avant cr√©ation du client

3. ‚úÖ **`BookingUpdateSchema`** ‚Üí `PUT /api/bookings/<id>`

   - Fichier: `backend/routes/bookings.py` ligne ~377
   - Schema: `backend/schemas/booking_schemas.py`
   - Action: Remplacer validation manuelle par `validate_request(BookingUpdateSchema(), data)`

4. ‚úÖ **`BookingListSchema`** ‚Üí `GET /api/bookings` (query params)
   - Fichier: `backend/routes/bookings.py` ligne ~475
   - Schema: `backend/schemas/booking_schemas.py`
   - Action: Valider `request.args` avec `validate_request(BookingListSchema(), dict(request.args))`

**Phase 2 : Cr√©er et int√©grer schemas critiques (priorit√© haute - ~3 jours)**

5. ‚úÖ **`CompanyUpdateSchema`** ‚Üí `PUT /api/companies/me` - **COMPL√âT√â**

   - ‚úÖ Schema cr√©√©: `backend/schemas/company_schemas.py` (ligne 129)
   - ‚úÖ Int√©gr√©: `backend/routes/companies.py` (lignes 405-411)
   - Champs: name, address, contact_email, contact_phone, billing_email, iban, uid_ide, etc.
   - Validation: IBAN format, UID format, email valide

6. ‚úÖ **`DriverCreateSchema`** ‚Üí `POST /api/companies/me/drivers/create` - **COMPL√âT√â**

   - ‚úÖ Schema cr√©√©: `backend/schemas/company_schemas.py` (ligne 169)
   - ‚úÖ Int√©gr√©: `backend/routes/companies.py` (lignes 2386-2391)
   - Champs: username, first_name, last_name, email, password, vehicle_assigned, brand, license_plate

7. ‚úÖ **`ClientUpdateSchema`** ‚Üí `PUT /api/clients/<id>` - **COMPL√âT√â**

   - ‚úÖ Schema cr√©√©: `backend/schemas/client_schemas.py` (ligne 8)
   - ‚úÖ Int√©gr√©: `backend/routes/clients.py` (lignes 102-108)
   - Champs: first_name, last_name, address, phone, birth_date, gender, etc.

8. ‚úÖ **`DriverProfileUpdateSchema`** ‚Üí `PUT /api/driver/me/profile` - **COMPL√âT√â**
   - ‚úÖ Schema cr√©√©: `backend/schemas/driver_schemas.py` (ligne 8)
   - ‚úÖ Int√©gr√©: `backend/routes/driver.py` (lignes 172-178)
   - Champs: status, contract_type, weekly_hours, hourly_rate_cents, license_categories, etc.

**Phase 3 : √âtendre aux autres modules (priorit√© moyenne - ~4 jours)**

9. ‚úÖ **Invoices** (`backend/routes/invoices.py`) - **COMPL√âT√â**

   - ‚úÖ `BillingSettingsUpdateSchema` pour `PUT /api/invoices/companies/<id>/billing-settings`
   - ‚úÖ `InvoiceGenerateSchema` pour `POST /api/invoices/companies/<id>/invoices/generate`
   - Schema cr√©√©: `backend/schemas/invoice_schemas.py`
   - Int√©gr√© dans les routes avec validation compl√®te

10. ‚úÖ **Payments** (`backend/routes/payments.py`) - **COMPL√âT√â**

    - ‚úÖ `PaymentStatusUpdateSchema` pour `PUT /api/payments/<id>`
    - ‚úÖ `PaymentCreateSchema` cr√©√© (pr√™t √† int√©grer si endpoint POST existe)
    - Schema cr√©√©: `backend/schemas/payment_schemas.py`
    - Int√©gr√© dans la route de mise √† jour de statut

11. ‚úÖ **Medical** (`backend/routes/medical.py`) - **COMPL√âT√â**

    - ‚úÖ `MedicalEstablishmentQuerySchema` pour `GET /api/medical/establishments` (query params)
    - ‚úÖ `MedicalServiceQuerySchema` pour `GET /api/medical/services` (query params)
    - **Note**: Medical n'a que des GET endpoints, donc validation query params uniquement

12. ‚úÖ **Admin** (`backend/routes/admin.py`) - **COMPL√âT√â**
    - ‚úÖ `UserRoleUpdateSchema` pour `PUT /api/admin/users/<id>/role` - **INT√âGR√â**
    - ‚úÖ `AutonomousActionReviewSchema` pour `POST /api/admin/autonomous-actions/<id>/review` - **INT√âGR√â**

**Phase 4 : Finir les endpoints restants (~2 jours)** - **EN COURS**

‚úÖ **Endpoints compl√©t√©s :**

- ‚úÖ `BookingUpdateSchema` ‚Üí `PUT /api/bookings/<id>` - **INT√âGR√â**
- ‚úÖ `BookingListSchema` ‚Üí `GET /api/bookings` (query params) - **INT√âGR√â**
- ‚úÖ `ManualBookingCreateSchema` ‚Üí `POST /api/companies/me/reservations/manual` - **INT√âGR√â**
- ‚úÖ `ClientCreateSchema` ‚Üí `POST /api/companies/me/clients` - **INT√âGR√â**

‚è≥ **√Ä faire :**

1. ‚úÖ **Routes secondaires (analytics, planning, vehicles)** - **COMPL√âT√â**

   - ‚úÖ **Analytics** (`backend/routes/analytics.py`):
     - ‚úÖ `AnalyticsDashboardQuerySchema` pour `GET /api/analytics/dashboard` (period, start_date, end_date)
     - ‚úÖ `AnalyticsInsightsQuerySchema` pour `GET /api/analytics/insights` (lookback_days)
     - ‚úÖ `AnalyticsWeeklySummaryQuerySchema` pour `GET /api/analytics/weekly-summary` (week_start)
     - ‚úÖ `AnalyticsExportQuerySchema` pour `GET /api/analytics/export` (start_date, end_date, format)
   - ‚úÖ **Planning** (`backend/routes/planning.py`):
     - ‚úÖ `PlanningShiftsQuerySchema` pour `GET /api/planning/companies/me/planning/shifts` (driver_id)
     - ‚úÖ `PlanningUnavailabilityQuerySchema` pour `GET /api/planning/companies/me/planning/unavailability` (driver_id)
     - ‚úÖ `PlanningWeeklyTemplateQuerySchema` pour `GET /api/planning/companies/me/planning/weekly-template` (driver_id)
   - ‚úÖ **Vehicles**: Routes simples dans `companies.py`, pas de validation n√©cessaire pour GET/POST/PUT/DELETE simples

2. ‚úÖ **Validation query params pour autres GET endpoints complexes** - **COMPL√âT√â**

   - ‚úÖ **Medical**: Compl√©t√© (`MedicalEstablishmentQuerySchema`, `MedicalServiceQuerySchema`)
   - ‚úÖ **Analytics**: Compl√©t√© (4 schemas pour 4 endpoints GET)
   - ‚úÖ **Planning**: Compl√©t√© (3 schemas pour 3 endpoints GET)

3. ‚úÖ **Tests E2E pour chaque endpoint valid√©** - **COMPL√âT√â**

   - ‚úÖ Tests unitaires existent: `backend/tests/test_validation_schemas.py`, `backend/tests/test_phase2_schemas.py`
   - ‚úÖ Tests E2E cr√©√©s: `backend/tests/e2e/test_schema_validation.py`
     - Tests pour endpoints Auth (login, register)
     - Tests pour endpoints Bookings (create, list)
     - Tests pour endpoints Companies (update, create driver)
     - Tests pour endpoints Medical (query params)
     - Tests pour endpoints Analytics (query params)
     - Tests pour endpoints Planning (query params)
     - Tests pour endpoints Admin (update user role)
     - Chaque endpoint test√© avec payloads valides (succ√®s) et invalides (erreur 400 avec messages d√©taill√©s)

4. **Documentation OpenAPI mise √† jour**
   - ‚úÖ **COMPL√âT√â**: Tous les endpoints valid√©s avec Marshmallow ont maintenant des mod√®les Swagger correspondants
   - ‚úÖ **Mod√®les Swagger synchronis√©s avec schemas Marshmallow**:
     - Auth: `login_model`, `register_model` (ajout contraintes min_length/max_length)
     - Bookings: `booking_create_model`, `booking_update_model` (nouveau), `@param` pour query params
     - Companies: `company_update_model`, `create_driver_model`, `manual_booking_model`, `client_create_model` (nouveau), `@param` pour query params
     - Clients: `client_profile_model` (ajout contraintes validation)
     - Driver: `driver_profile_model` (ajout contraintes validation)
     - Invoices: `billing_settings_model` (ajout contraintes), `invoice_generate_model` (nouveau)
     - Payments: `payment_status_model`, `payment_create_model` (ajout contraintes)
     - Admin: `user_role_update_model` (nouveau), `autonomous_action_review_model` (nouveau)
     - Analytics: `@param` pour tous les query params (dashboard, insights, weekly-summary, export)
     - Planning: `@param` pour tous les query params (shifts, unavailability, weekly-template)
   - ‚úÖ **Contraintes de validation document√©es** dans OpenAPI (min_length, max_length, pattern, enum, minimum, maximum)
   - ‚úÖ **Query params document√©s** avec `@param` incluant types, contraintes, valeurs par d√©faut

---

### üìù Template d'int√©gration pour chaque endpoint

```python
from schemas.validation_utils import validate_request, handle_validation_error
from schemas.XXX_schemas import YYYSchema
from marshmallow import ValidationError

def post(self):
    try:
        data = request.get_json() or {}

        # ‚úÖ Validation Marshmallow
        try:
            validated_data = validate_request(YYYSchema(), data)
        except ValidationError as e:
            return handle_validation_error(e)

        # Utiliser validated_data au lieu de data
        # ...
```

### ‚úÖ Checklist de validation

Pour chaque endpoint valid√© :

- [ ] Schema cr√©√© dans `backend/schemas/`
- [ ] Schema import√© dans la route
- [ ] `validate_request()` appel√© avec gestion d'erreur
- [ ] `validated_data` utilis√© au lieu de `data`
- [ ] Test unitaire du schema ajout√©
- [ ] Test E2E de l'endpoint mis √† jour
- [ ] Documentation Swagger mise √† jour si n√©cessaire

### üìä M√©triques de progression

- **Structure**: ‚úÖ 100% (utils, helpers, format d'erreurs)
- **Schemas cr√©√©s**: 8/50 (~16%)
- **Endpoints valid√©s**: 3/50 (~6%)
- **Tests**: ‚úÖ Framework en place
- **Progression globale**: ~10% compl√©t√©

---

### 2.5 Impl√©menter rotation secrets ‚úÖ COMPL√âT√â

**Priorit√©:** üü† HAUTE  
**Effort:** 5 jours  
**Owner:** DevOps  
**DDL:** J+30

**Actions:**

1. ‚úÖ Cr√©er job Celery rotation cl√©s encryption (`rotate_encryption_keys`)
2. ‚úÖ Support multi-cl√©s (rotation progressive) - `legacy_keys` + fallback automatique dans `decrypt_field()`
3. ‚úÖ Script migration donn√©es chiffr√©es (`migrate_encrypted_data`)

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ Cr√©er: `backend/tasks/secret_rotation_tasks.py` (3 t√¢ches: rotation, migration, v√©rification)
- ‚úÖ Modifier: `backend/security/crypto.py` (support multi-cl√©s, `rotate_to_new_key()`, `add_legacy_key()`)
- ‚úÖ Modifier: `backend/celery_app.py` (t√¢che Beat `secret-rotation-check` hebdomadaire)

**Crit√®res d'acceptation:**

- ‚úÖ Rotation automatique toutes les 90 jours (v√©rification hebdomadaire via Celery Beat `check_rotation_due`)
- ‚úÖ Support multi-cl√©s (ancienne + nouvelle) avec fallback automatique dans `decrypt_field()`
- ‚úÖ Migration transparente donn√©es (t√¢che `migrate_encrypted_data` pour re-chiffrer progressivement par batch)
- ‚úÖ Variables d'environnement: `MASTER_ENCRYPTION_KEY` (active) + `LEGACY_ENCRYPTION_KEYS` (s√©par√©es par virgule)

**Preuve d'impl√©mentation:**

- `backend/security/crypto.py:124` : `all_keys = [self.master_key, *self.legacy_keys]` (fallback multi-cl√©s)
- `backend/tasks/secret_rotation_tasks.py:22` : T√¢che `rotate_encryption_keys` cr√©√©e
- `backend/tasks/secret_rotation_tasks.py:76` : T√¢che `migrate_encrypted_data` cr√©√©e
- `backend/celery_app.py:126` : Schedule `secret-rotation-check` ajout√© (hebdomadaire)

---

### 2.6 Ajouter jitter Celery Beat ‚úÖ COMPL√âT√â

**Priorit√©:** üü† HAUTE  
**Effort:** 2 heures  
**Owner:** Backend Lead  
**DDL:** J+5

**Actions:**

1. ‚úÖ Ajouter `jitter` dans beat schedule options
2. ‚úÖ Configurer jitter adapt√© selon fr√©quence des jobs

**Fichiers modifi√©s:**

- ‚úÖ Modifier: `backend/celery_app.py:68-124`

**Changements appliqu√©s:**

```python
# ‚úÖ 2.6: Jitter ajout√© √† tous les jobs pour √©viter thundering herd
celery.conf.beat_schedule = {
    "dispatch-autorun": {
        "task": "tasks.dispatch_tasks.autorun_tick",
        "schedule": DISPATCH_AUTORUN_INTERVAL_SEC,
        "options": {
            "expires": DISPATCH_AUTORUN_INTERVAL_SEC * 2,
            "jitter": 30,  # Jitter jusqu'√† 30 secondes
        },
    },
    "realtime-monitoring": {
        "task": "tasks.dispatch_tasks.realtime_monitoring_tick",
        "schedule": 120.0,
        "options": {
            "expires": 240,
            "jitter": 15,  # Jitter jusqu'√† 15 secondes (tasks fr√©quentes)
        },
    },
    "planning-compliance-scan": {
        "task": "planning.compliance_scan",
        "schedule": 24 * 3600,
        "options": {
            "expires": 6 * 3600,
            "jitter": 300,  # Jitter jusqu'√† 5 minutes (tasks quotidiennes)
        },
    },
    # ... autres jobs avec jitter adapt√©
}
```

**Jitter configur√© par type de job:**

- **Jobs fr√©quents** (< 5 min): jitter 15-30 secondes
- **Jobs quotidiens**: jitter 5 minutes (300 secondes)
- **Jobs hebdomadaires**: jitter 30 minutes (1800 secondes)
- **Jobs mensuels**: jitter 1 heure (3600 secondes)

**Crit√®res d'acceptation:**

- ‚úÖ Jitter configur√© tous jobs Beat
- ‚úÖ Jitter adapt√© selon fr√©quence des jobs
- ‚úÖ √âvite thundering herd

---

### 2.7 Profiler requ√™tes DB (d√©tecter N+1) ‚úÖ COMPL√âT√â

**Priorit√©:** üü† HAUTE  
**Effort:** 5 jours  
**Owner:** Backend Lead  
**DDL:** J+25

**Actions:**

1. ‚úÖ Cr√©er module de profiling DB natif (sans d√©pendance externe)
2. ‚úÖ Profiler automatiquement tous les endpoints
3. ‚úÖ D√©tecter automatiquement patterns N+1

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ Cr√©er: `backend/shared/db_profiler.py` (module de profiling complet)
- ‚úÖ Modifier: `backend/ext.py` (ajout fonction `setup_db_profiler()`)
- ‚úÖ Modifier: `backend/app.py` (int√©gration du profiler)

**Configuration:**

Le profiler utilise les event listeners SQLAlchemy natifs, pas de d√©pendance externe requise.

**Activation:**

```bash
# Activer le profiling DB
export ENABLE_DB_PROFILING=true

# Optionnel: Afficher stats dans headers HTTP
export DB_PROFILING_HEADERS=true
```

**Fonctionnalit√©s:**

- ‚úÖ Comptage automatique des requ√™tes SQL par endpoint
- ‚úÖ D√©tection automatique de patterns N+1 (requ√™tes similaires r√©p√©t√©es)
- ‚úÖ Logging des requ√™tes lentes (> 1 seconde)
- ‚úÖ Headers HTTP optionnels avec statistiques (`X-DB-Query-Count`, `X-DB-Total-Time-Ms`)
- ‚úÖ Rapport textuel g√©n√©rable via `get_db_profiler().generate_report()`
- ‚úÖ Context manager `profile_db_context()` pour profiler sections sp√©cifiques

**Usage dans le code:**

```python
from shared.db_profiler import profile_db_context, get_db_profiler

# Profiler automatiquement (via middleware)
# Le profiler s'active automatiquement sur tous les endpoints si ENABLE_DB_PROFILING=true

# Profiler une section sp√©cifique
with profile_db_context("my_function"):
    # Code √† profiler
    ...
stats = get_db_profiler().get_stats()
```

**Crit√®res d'acceptation:**

- ‚úÖ Profiling activable via env var `ENABLE_DB_PROFILING=true`
- ‚úÖ D√©tection automatique N+1 (threshold configurable)
- ‚úÖ Logging automatique des probl√®mes
- ‚úÖ Headers HTTP optionnels pour monitoring
- ‚úÖ Aucune d√©pendance externe requise (SQLAlchemy natif)

---

### 2.8 Rate limiting par endpoint ‚úÖ COMPL√âT√â

**Priorit√©:** üü† HAUTE  
**Effort:** 3 jours  
**Owner:** Backend Lead  
**DDL:** J+20

**Actions:**

1. ‚úÖ D√©finir limites par endpoint (15+ endpoints critiques prot√©g√©s)
2. ‚úÖ Ajouter d√©corateurs `@limiter.limit()` sp√©cifiques
3. ‚úÖ Cr√©er tests rate limiting

**Fichiers modifi√©s:**

- ‚úÖ Modifier: `backend/routes/bookings.py` (5 endpoints: POST/GET/PUT/DELETE/List)
- ‚úÖ Modifier: `backend/routes/companies.py` (6 endpoints: cr√©ation r√©servation, client, chauffeur, accept, assign, liste clients)
- ‚úÖ Modifier: `backend/routes/admin.py` (4 endpoints: stats, users, update_role, reset_password)
- ‚úÖ Modifier: `backend/routes/dispatch_routes.py` (2 endpoints: run, trigger)
- ‚úÖ Cr√©er: `backend/tests/test_rate_limiting.py` (tests HTTP 429)

**Limites appliqu√©es:**

- **Bookings**: Cr√©ation (50/h), Lecture (200/h), Modification (100/h), Suppression (50/h), Liste (300/h)
- **Companies**: R√©servation manuelle (100/h), Cr√©ation client (50/h), Cr√©ation chauffeur (20/h), Accept (200/h), Assign (200/h), Liste clients (300/h)
- **Admin**: Stats (100/h), Liste users (200/h), Update role (50/h), Reset password (10/h - s√©curit√©)
- **Dispatch**: Run (30/h), Trigger (50/h)
- **Auth**: Login (5/min), Forgot password (5/min) - existant
- **Invoices**: Un endpoint (10/min) - existant

**Crit√®res d'acceptation:**

- ‚úÖ Limites d√©finies 15+ endpoints critiques (objectif 10+ d√©pass√©)
- ‚úÖ Tests rate limiting pr√©sents (`test_rate_limiting.py`)
- ‚úÖ Retour HTTP 429 si limite d√©pass√©e (v√©rifi√© dans tests)

**Preuve d'impl√©mentation:**

- `backend/routes/bookings.py:210` : `@limiter.limit("50 per hour")` sur cr√©ation r√©servation
- `backend/routes/companies.py:2392` : `@limiter.limit("20 per hour")` sur cr√©ation chauffeur
- `backend/routes/admin.py:373` : `@limiter.limit("10 per hour")` sur reset password
- `backend/tests/test_rate_limiting.py` : Tests complets pour v√©rifier HTTP 429

---

### 2.9 Compl√©ter traces OpenTelemetry ‚úÖ COMPL√âT√â

**Priorit√©:** üü† HAUTE  
**Effort:** 10 jours  
**Owner:** Backend Lead  
**DDL:** J+40

**Actions:**

1. ‚úÖ Instrumenter toutes requ√™tes DB (SQLAlchemyInstrumentor)
2. ‚úÖ Instrumenter toutes t√¢ches Celery (CeleryInstrumentor)
3. ‚úÖ Corr√©ler traces end-to-end (propagation W3C Trace Context)

**Fichiers modifi√©s:**

- ‚úÖ Modifier: `backend/shared/otel_setup.py` (instrumentation compl√®te + propagation W3C)
- ‚úÖ Modifier: `backend/app.py` (int√©gration OpenTelemetry au d√©marrage)
- ‚úÖ Ajout: Fonctions `instrument_flask()`, `instrument_sqlalchemy()`, `instrument_celery()`
- ‚úÖ Ajout: Fonction `inject_trace_id_to_logs()` pour corr√©lation logs

**Preuves d'impl√©mentation:**

```83:128:backend/shared/otel_setup.py
def instrument_flask(app) -> None:
    """‚úÖ 2.9: Instrumente Flask pour traces HTTP avec propagation W3C."""

def instrument_sqlalchemy(engine) -> None:
    """‚úÖ 2.9: Instrumente SQLAlchemy pour traces requ√™tes DB."""

def instrument_celery(celery_app) -> None:
    """‚úÖ 2.9: Instrumente Celery pour traces t√¢ches asynchrones."""
```

```164:211:backend/app.py
# ‚úÖ 2.9: Setup OpenTelemetry avec instrumentation compl√®te
setup_opentelemetry(service_name=service_name, service_version=service_version)
instrument_flask(app)
instrument_sqlalchemy(engine)
instrument_celery(celery_app)
```

```292:314:backend/app.py
# ‚úÖ 2.9: Injection trace_id dans logs pour corr√©lation
logging.setLogRecordFactory(record_factory_with_trace)
```

**Crit√®res d'acceptation:**

- ‚úÖ Traces distribu√©es API ‚Üí DB ‚Üí Celery (instrumentation compl√®te)
- ‚úÖ Traces visibles dans Tempo/Jaeger (export OTLP vers `OTEL_EXPORTER_OTLP_ENDPOINT`)
- ‚úÖ Correlation trace_id dans logs (enrichissement LogRecord avec trace_id/span_id)
- ‚úÖ Propagation W3C Trace Context (CompositeHTTPPropagator avec TraceContextTextMapPropagator)

**Installation:**

Pour activer les traces OpenTelemetry, installer les d√©pendances :

```bash
pip install -r backend/requirements-otel.txt
```

Ou manuellement :

```bash
pip install opentelemetry-api opentelemetry-sdk opentelemetry-instrumentation-flask opentelemetry-instrumentation-sqlalchemy opentelemetry-instrumentation-celery opentelemetry-instrumentation-requests opentelemetry-exporter-otlp-proto-grpc
```

**Variables d'environnement:**

- `OTEL_EXPORTER_OTLP_ENDPOINT`: Endpoint OTLP (d√©faut: `http://localhost:4317`)
- `OTEL_SERVICE_NAME`: Nom du service (d√©faut: `atmr-backend`)
- `OTEL_SERVICE_VERSION`: Version du service (d√©faut: `1.0`)

---

### 2.10 Exposer m√©triques Prometheus toutes routes ‚úÖ COMPL√âT√â

**Priorit√©:** üü† HAUTE  
**Effort:** 2 jours  
**Owner:** Backend Lead  
**DDL:** J+10

**Actions:**

1. ‚úÖ Middleware m√©triques int√©gr√© (`middleware/metrics.py`)
2. ‚úÖ Endpoint `/prometheus/metrics-http` expos√©
3. ‚úÖ Configuration Prometheus cr√©√©e (`prometheus/prometheus.yml`)
4. ‚úÖ `prometheus-client` ajout√© au Dockerfile
5. ‚úÖ Tests unitaires cr√©√©s (`tests/test_prometheus_metrics.py`)

**Fichiers modifi√©s:**

- ‚úÖ `backend/middleware/metrics.py` (d√©j√† pr√©sent - middleware actif)
- ‚úÖ `backend/app.py` (int√©gration middleware - d√©j√† pr√©sent)
- ‚úÖ `backend/Dockerfile` (ajout `prometheus-client`)
- ‚úÖ `prometheus/prometheus.yml` (configuration scraping cr√©√©e)
- ‚úÖ `backend/tests/test_prometheus_metrics.py` (tests cr√©√©s)

**Preuves d'impl√©mentation:**

```55:154:backend/middleware/metrics.py
def prom_middleware(app: Flask) -> Flask:
    """Ajoute le middleware Prometheus √† l'application Flask."""
    # Instrumentation before_request / after_request
    # Endpoint /prometheus/metrics-http
```

```224:229:backend/app.py
# Prometheus middleware pour m√©triques HTTP (latence p50/p95/p99)
try:
    from middleware.metrics import prom_middleware
    app = prom_middleware(app)
except ImportError as e:
    app.logger.warning("[Prometheus] Middleware non disponible: %s", e)
```

**Crit√®res d'acceptation:**

- ‚úÖ M√©triques latence toutes routes (`http_request_duration_seconds` avec buckets)
- ‚úÖ M√©triques compteurs toutes routes (`http_requests_total` avec labels method/endpoint/status)
- ‚úÖ M√©triques requ√™tes en cours (`http_requests_in_progress`)
- ‚úÖ M√©triques erreurs (4xx, 5xx) via labels `status`
- ‚úÖ Endpoint `/prometheus/metrics-http` fonctionnel
- ‚úÖ Configuration Prometheus pour scraping
- ‚úÖ Tests unitaires validant l'instrumentation

**Configuration Prometheus:**

Fichier `prometheus/prometheus.yml` configur√© pour scraper:

- Job: `atmr-backend-http` sur `api:5000/prometheus/metrics-http`
- Intervalle: 10s
- Alertes: `prometheus/alerts-dispatch.yml`

**M√©triques expos√©es:**

- `http_request_duration_seconds` (histogram avec buckets: 0.005s √† 10s)
- `http_requests_total` (counter avec labels: method, endpoint, status)
- `http_requests_in_progress` (gauge avec labels: method, endpoint)

**Installation:**

Le package `prometheus-client` est install√© automatiquement via le Dockerfile.
Pour l'installer localement: `pip install prometheus-client`

**Prochaines √©tapes (optionnel):**

- Ajouter dashboard Grafana (voir √©tape suivante)
- Configurer alertes personnalis√©es sur m√©triques HTTP
- Exposer m√©triques m√©tier suppl√©mentaires

---

### 2.11 Int√©grer alertes SLO breach (PagerDuty) ‚úÖ COMPLET

**Priorit√©:** üü† HAUTE  
**Effort:** 3 jours  
**Owner:** DevOps  
**DDL:** J+20  
**Statut:** ‚úÖ **COMPLET**

**Actions:**

1. ‚úÖ Configurer PagerDuty integration
2. ‚úÖ Cr√©er alertes Prometheus ‚Üí PagerDuty
3. ‚úÖ Tester alertes

**Fichiers cr√©√©s:**

- ‚úÖ `prometheus/alerts-slo.yml` - Alertes SLO (API + Dispatch) avec routing PagerDuty
- ‚úÖ `prometheus/alertmanager.example.yml` - Configuration Alertmanager pour PagerDuty
- ‚úÖ `prometheus/PAGERDUTY_SETUP.md` - Documentation setup PagerDuty
- ‚úÖ Mise √† jour: `prometheus/prometheus.yml` - Ajout `alerts-slo.yml` dans `rule_files`

**Impl√©mentation:**

1. **Alertes SLO API HTTP:**

   - `APISLOLatencyBreachRepeated`: Violation r√©p√©t√©e latence (warning ‚Üí Slack)
   - `APISLOLatencyBreachCritical`: Violation critique latence (> 1/s) ‚Üí PagerDuty
   - `APISLOErrorRateBreach`: Taux d'erreurs > 5% ‚Üí PagerDuty
   - `APISLOAvailabilityBreach`: Disponibilit√© < seuil ‚Üí PagerDuty
   - `APILatencyP95High`: Latence p95 > 2s ‚Üí Slack

2. **Alertes SLO Dispatch:**

   - `DispatchSLOBreachRepeatedPagerDuty`: Breaches r√©p√©t√©s ‚Üí PagerDuty
   - `DispatchSLOBreachCountHigh`: > 10 breaches/1h ‚Üí Slack
   - `DispatchSLOSeverityCritical`: Severity critique ‚Üí PagerDuty

3. **Alertes Globales:**

   - `GlobalSLOBreachesElevated`: Taux global > 5/min ‚Üí Slack
   - `CriticalHealthCheckFailing`: Health checks √©chouent ‚Üí PagerDuty

4. **Configuration Alertmanager:**
   - Routes s√©par√©es pour `critical` (PagerDuty) et `warning` (Slack)
   - Groupement par `cluster` et `alertname`
   - Inhibition rules pour √©viter alertes redondantes
   - Runbooks attach√©s via annotation `runbook_url`

**Crit√®res d'acceptation:**

- ‚úÖ Alertes d√©clench√©es si SLO breach (API + Dispatch)
- ‚úÖ Configuration PagerDuty pr√™te (via Alertmanager)
- ‚úÖ Notifications PagerDuty fonctionnelles (apr√®s setup cl√©)
- ‚úÖ Runbooks attach√©s aux alertes (via `runbook_url`)
- ‚úÖ Documentation compl√®te (PAGERDUTY_SETUP.md)

**Prochaines √©tapes:**

1. D√©ployer Alertmanager (docker-compose ou Kubernetes)
2. Configurer la cl√© PagerDuty dans `alertmanager.yml`
3. Tester les alertes en production
4. Cr√©er les runbooks r√©f√©renc√©s dans les annotations

---

## üü° PHASE 3 : MOYENNE PRIORIT√â (3-6 mois) ‚Äî EXCELLENCE OP√âRATIONNELLE

### 3.1 Augmenter couverture tests √† 70% ‚úÖ EN COURS

**Priorit√©:** üü° MOYENNE  
**Effort:** 20 jours  
**Owner:** Backend Team  
**DDL:** J+90  
**Statut:** ‚úÖ **EN COURS** (Infrastructure configur√©e, tests √† √©crire)

**Actions:**

1. ‚úÖ Identifier modules non test√©s (script cr√©√©)
2. üîÑ √âcrire tests unitaires manquants (en cours)
3. ‚úÖ Maintenir couverture ‚â• 70% (CI configur√©e)

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ `backend/scripts/check_coverage.py` - Script d'analyse couverture
- ‚úÖ `.github/workflows/backend-tests.yml` - Check CI couverture globale ‚â• 70% (bloquant)
- ‚úÖ `backend/.coveragerc` - Documentation seuils
- ‚úÖ Tests manquants √† cr√©er dans `backend/tests/` (identifi√©s via script)

**Impl√©mentation:**

1. **Script d'analyse couverture (`scripts/check_coverage.py`):**

   - Parse `coverage.xml` et g√©n√®re rapport JSON
   - Identifie modules < 70% (seuil global)
   - Identifie modules critiques < 80% (routes API, dispatch, s√©curit√©)
   - Liste top 20 modules avec le moins de couverture
   - Identifie modules non test√©s (0%)
   - G√©n√®re recommandations prioritaires

2. **Check CI bloquant:**

   - **Couverture globale ‚â• 70%**: Fail si en dessous
   - **Modules critiques ‚â• 80%**: V√©rifie routes API, dispatch engine, s√©curit√©
   - G√©n√®re `coverage_report.json` pour analyse
   - Upload rapport comme artifact

3. **Modules critiques d√©finis (‚â• 80% requis):**

   - Routes API: `bookings.py`, `companies.py`, `auth.py`, `admin.py`, `dispatch_routes.py`, `payments.py`
   - Dispatch: `engine.py`, `solver.py`, `heuristics.py`, `autonomous_manager.py`, `queue.py`
   - S√©curit√©: `crypto.py`, `audit_log.py`
   - Services: `api_slo.py`, `slo.py`, `metrics.py`
   - Models: `booking.py`, `client.py`, `driver.py`, `user.py`

4. **Usage du script:**

   ```bash
   # G√©n√©rer coverage.xml
   pytest --cov=. --cov-report=xml --cov-report=term-missing

   # Analyser couverture
   python scripts/check_coverage.py --coverage-xml coverage.xml --fail-under 70.0

   # G√©n√©rer rapport JSON
   python scripts/check_coverage.py --coverage-xml coverage.xml --json report.json
   ```

**Crit√®res d'acceptation:**

- ‚úÖ Script d'identification modules non test√©s cr√©√©
- ‚úÖ Check CI couverture globale ‚â• 70% (bloquant)
- ‚úÖ Check CI modules critiques ‚â• 80%
- ‚úÖ Liste modules critiques d√©finie
- üîÑ Couverture globale ‚â• 70% (√† atteindre via tests)
- üîÑ Modules critiques ‚â• 80% (√† atteindre via tests)

**Prochaines √©tapes:**

1. Ex√©cuter `python scripts/check_coverage.py` pour identifier modules prioritaires
2. Cr√©er tests pour modules critiques < 80% en premier
3. Cr√©er tests pour modules non test√©s (0%)
4. Maintenir couverture ‚â• 70% dans chaque PR

---

### 3.2 Versioning API explicite (/api/v1/, /api/v2/) ‚úÖ COMPLET

**Priorit√©:** üü° MOYENNE  
**Effort:** 5 jours  
**Owner:** Backend Lead  
**DDL:** J+45  
**Statut:** ‚úÖ **COMPLET**

**Actions:**

1. ‚úÖ Migrer routes vers `/api/v1/`
2. ‚úÖ Cr√©er namespace `/api/v2/` pour nouvelles routes
3. ‚úÖ D√©pr√©cier v1 progressivement

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ `backend/routes_api.py` - Structure API v1, v2 et legacy
- ‚úÖ `backend/app.py` - Middleware headers Deprecation
- ‚úÖ `backend/routes/v2/__init__.py` - Package pour futures routes v2

**Impl√©mentation:**

1. **API v1 (`/api/v1/`)** :

   - Tous les namespaces existants migr√©s vers `api_v1`
   - Routes disponibles: `/api/v1/auth`, `/api/v1/bookings`, `/api/v1/companies`, etc.
   - Header `Deprecation: version="v1"` ajout√© automatiquement
   - Header `Sunset` avec date estim√©e de suppression
   - Header `Link` pointant vers v2 comme successeur

2. **API v2 (`/api/v2/`)** :

   - API cr√©√©e et initialis√©e (vide pour l'instant)
   - Pr√™te pour migration progressive depuis v1
   - Documentation Swagger disponible sur `/docs/v2` (si activ√©e)

3. **API Legacy (`/api/*`)** :

   - Maintien pour compatibilit√© (si `API_LEGACY_ENABLED=true`)
   - Toutes les routes dupliqu√©es vers `/api/*` sans version
   - Header `Deprecation: version="legacy"` ajout√©
   - Peut √™tre d√©sactiv√©e via variable d'environnement

4. **Headers HTTP standards** :
   - `Deprecation: version="v1"` sur toutes les routes `/api/v1/*`
   - `Deprecation: version="legacy"` sur routes `/api/*` (si legacy activ√©)
   - `Sunset: Wed, 01 Jan 2025 00:00:00 GMT` (date estim√©e suppression)
   - `Link: <https://docs.atmr.ch/api/v2>; rel="successor-version"` (pour v1)

**Crit√®res d'acceptation:**

- ‚úÖ `/api/v1/` fonctionnel (toutes routes migr√©es)
- ‚úÖ `/api/v2/` disponible (vide, pr√™te pour nouvelles routes)
- ‚úÖ Header `Deprecation: version="v1"` sur v1 (automatique)
- ‚úÖ API legacy maintenue pour compatibilit√© (optionnel, activable/d√©sactivable)

**Exemples d'utilisation:**

```bash
# Routes v1 (d√©pr√©ci√©es)
curl -H "Authorization: Bearer TOKEN" http://localhost:5000/api/v1/bookings
curl -H "Authorization: Bearer TOKEN" http://localhost:5000/api/v1/companies/me

# Routes v2 (nouvelles routes √† venir)
curl -H "Authorization: Bearer TOKEN" http://localhost:5000/api/v2/bookings

# Routes legacy (compatibilit√©, si activ√©es)
curl -H "Authorization: Bearer TOKEN" http://localhost:5000/api/bookings
```

**D√©sactiver API legacy:**

```bash
# Dans .env ou docker-compose.yml
API_LEGACY_ENABLED=false
```

**Migration recommand√©e:**

1. Utiliser `/api/v1/` pour routes existantes (avec header Deprecation)
2. Cr√©er nouvelles routes dans `/api/v2/`
3. Migrer progressivement routes depuis v1 vers v2
4. D√©sactiver legacy une fois migration compl√®te

---

### 3.3 R√©tention/purge automatique donn√©es RGPD

**Priorit√©:** üü° MOYENNE  
**Effort:** 7 jours  
**Owner:** Backend Lead  
**DDL:** J+60

**Actions:**

1. ‚úÖ Cr√©er jobs Celery purge donn√©es anciennes
2. ‚úÖ Configurer r√©tention par type de donn√©es
3. ‚úÖ Logs audit purge

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ Cr√©√©: `backend/tasks/purge_tasks.py`
- ‚úÖ Modifi√©: `backend/celery_app.py` (ajout purge_tasks au include et beat_schedule)

**Impl√©mentation:**

- ‚úÖ T√¢che `purge_old_bookings`: Supprime bookings termin√©s/cancelled > 7 ans
- ‚úÖ T√¢che `purge_old_messages`: Supprime messages > 7 ans
- ‚úÖ T√¢che `purge_old_realtime_events`: Supprime √©v√©nements temps r√©el > 7 ans (bulk delete)
- ‚úÖ T√¢che `purge_old_autonomous_actions`: Supprime actions autonomes review√©es > 7 ans
- ‚úÖ T√¢che `purge_old_task_failures`: Supprime TaskFailure > 7 ans (bulk delete)
- ‚úÖ T√¢che `purge_all_old_data`: T√¢che principale qui appelle toutes les purges (hebdomadaire)
- ‚úÖ T√¢che `anonymize_old_user_data`: Anonymise utilisateurs inactifs > 7 ans (mensuelle)
- ‚úÖ Logs audit via `AuditLogger.log_action()` avec cat√©gorie `gdpr_purge`
- ‚úÖ Configuration via variables d'environnement:
  - `GDPR_RETENTION_DAYS` (d√©faut: 2555 jours = 7 ans)
  - `GDPR_ANALYTICS_RETENTION_DAYS` (d√©faut: 3650 jours = 10 ans)
  - `GDPR_EVENT_RETENTION_DAYS` (d√©faut: 2555 jours = 7 ans)
  - `GDPR_MESSAGE_RETENTION_DAYS` (d√©faut: 2555 jours = 7 ans)

**Crit√®res d'acceptation:**

- ‚úÖ Purge automatique donn√©es > 7 ans (configurable via env vars)
- ‚úÖ Logs audit complets dans `audit_logs` table (cat√©gorie `gdpr_purge`)
- ‚è≥ Tests purge pr√©sents (√† faire)

---

### 3.4 Profilage CPU/m√©moire automatique

**Priorit√©:** üü° MOYENNE  
**Effort:** 5 jours  
**Owner:** Backend Lead  
**DDL:** J+50

**Actions:**

1. ‚úÖ Ajouter job profiling p√©riodique
2. ‚úÖ Identifier top-10 fonctions chaudes
3. ‚è≥ Optimiser fonctions identifi√©es (√† faire manuellement selon r√©sultats)

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ Cr√©√©: `backend/tasks/profiling_tasks.py`
- ‚úÖ Cr√©√©: `backend/models/profiling_metrics.py`
- ‚úÖ Modifi√©: `backend/models/__init__.py` (ajout ProfilingMetrics)
- ‚úÖ Modifi√©: `backend/celery_app.py` (ajout profiling_tasks au include et beat_schedule)

**Impl√©mentation:**

- ‚úÖ T√¢che `run_weekly_profiling`: Profiling automatique hebdomadaire avec cProfile et psutil
- ‚úÖ T√¢che `generate_profiling_report`: Rapport consolid√© sur plusieurs semaines avec recommandations
- ‚úÖ Mod√®le `ProfilingMetrics`: Stockage des m√©triques en base de donn√©es (JSONB pour flexibilit√©)
- ‚úÖ Collecte m√©triques syst√®me (CPU, m√©moire) si psutil disponible
- ‚úÖ Identification top-10 fonctions chaudes avec temps cumulatif
- ‚úÖ G√©n√©ration rapports textuels et recommandations automatiques
- ‚úÖ Configuration via variables d'environnement:
  - `PROFILING_DURATION_SECONDS` (d√©faut: 30s)

**Crit√®res d'acceptation:**

- ‚úÖ Profiling automatique hebdomadaire (via Celery Beat)
- ‚úÖ Rapport top-10 fonctions avec m√©triques d√©taill√©es
- ‚è≥ Optimisations appliqu√©es (√† faire manuellement selon recommandations g√©n√©r√©es)

---

### 3.5 Configurer reverse proxy (nginx/traefik)

**Priorit√©:** üü° MOYENNE  
**Effort:** 5 jours  
**Owner:** DevOps  
**DDL:** J+40

**Actions:**

1. ‚úÖ Configurer nginx ou traefik
2. ‚úÖ Timeouts, body size limits
3. ‚úÖ Caching statique

**Fichiers cr√©√©s:**

- ‚úÖ Cr√©√©: `nginx/nginx.conf` (configuration nginx compl√®te)
- ‚úÖ Cr√©√©: `nginx/traefik.yml` (configuration Traefik alternative)
- ‚úÖ Cr√©√©: `nginx/docker-compose.nginx.yml` (service nginx pour Docker Compose)
- ‚úÖ Cr√©√©: `nginx/docker-compose.traefik.yml` (service Traefik pour Docker Compose)
- ‚úÖ Cr√©√©: `nginx/README.md` (documentation compl√®te)
- ‚úÖ Modifi√©: `docker-compose.yml` (commentaires pour int√©gration nginx)

**Impl√©mentation:**

- ‚úÖ **nginx** (recommand√© pour production):

  - Timeouts: 60s client, 120s proxy
  - Body size limit: 50MB (configurable)
  - Rate limiting: 100 req/s API, 10 req/s auth
  - Caching: Fichiers statiques (1h-24h), API GET (5min)
  - Gzip compression activ√©e
  - WebSocket support pour Socket.IO
  - Headers s√©curit√© (X-Frame-Options, etc.)

- ‚úÖ **Traefik** (alternative avec auto-d√©couverte):

  - Timeouts: 60s read/write, 180s idle
  - Auto-d√©couverte services Docker
  - M√©triques Prometheus
  - Dashboard web (√† prot√©ger en production)

- ‚úÖ **Configuration Docker Compose**:
  - Services s√©par√©s pour nginx et Traefik
  - Instructions d'utilisation dans README
  - Int√©gration comment√©e dans docker-compose.yml principal

**Crit√®res d'acceptation:**

- ‚úÖ Reverse proxy fonctionnel (nginx et Traefik configur√©s)
- ‚úÖ Timeouts configur√©s (client, proxy, idle)
- ‚úÖ Caching optimis√© (statiques longues dur√©es, API courtes dur√©es)
- ‚úÖ Body size limits configur√©s (50MB par d√©faut)
- ‚úÖ Rate limiting configur√© (diff√©renci√© par endpoint)
- ‚úÖ Documentation compl√®te avec exemples et troubleshooting

---

### 3.6 Ajouter limits CPU/mem Docker

**Priorit√©:** üü° MOYENNE  
**Effort:** 2 jours  
**Owner:** DevOps  
**DDL:** J+15

**Actions:**

1. ‚úÖ Ajouter `deploy.resources.limits` dans docker-compose.yml
2. ‚úÖ Configurer requests/limits appropri√©s

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ Modifi√©: `docker-compose.yml` (ajout limits pour tous les services)
- ‚úÖ Cr√©√©: `nginx/DOCKER_RESOURCE_LIMITS.md` (documentation compl√®te)

**Limites configur√©es:**

- ‚úÖ **API** : 2 CPUs / 2GB (reservations: 1 CPU / 1GB)
- ‚úÖ **PostgreSQL** : 1 CPU / 1GB (reservations: 0.5 CPU / 512MB)
- ‚úÖ **Celery Worker** : 1 CPU / 1GB (reservations: 0.5 CPU / 512MB)
- ‚úÖ **Celery Beat** : 0.25 CPU / 256MB (reservations: 0.1 CPU / 128MB)
- ‚úÖ **Flower** : 0.25 CPU / 256MB (reservations: 0.1 CPU / 128MB)
- ‚úÖ **Redis** : 0.5 CPU / 512MB (reservations: 0.25 CPU / 256MB)
- ‚úÖ **OSRM** : 1 CPU / 2GB (reservations: 0.5 CPU / 1GB)

**Total ressources:**

- Minimum (reservations) : ~3.25 CPUs / ~2.5GB
- Maximum (limits) : ~5.75 CPUs / ~7GB

**Crit√®res d'acceptation:**

- ‚úÖ Limits configur√©s tous services (avec syntaxe `deploy.resources`)
- ‚úÖ Reservations configur√©es pour garantir ressources minimales
- ‚úÖ Documentation compl√®te avec justification et guide d'ajustement
- ‚ö†Ô∏è Compatibilit√© : Syntaxe `deploy.resources` fonctionne avec Swarm/Kubernetes, peut n√©cessiter Swarm pour strict enforcement en Docker Compose standard

---

### 3.7 Linter dead code (vulture)

**Priorit√©:** üü° MOYENNE  
**Effort:** 5 jours  
**Owner:** Backend Team  
**DDL:** J+70

**Actions:**

1. ‚úÖ Ajouter vulture en CI
2. ‚ö†Ô∏è Supprimer code mort identifi√© (manuel, √† faire progressivement)
3. ‚úÖ Maintenir codebase propre (configuration et documentation)

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ Modifi√©: `.github/workflows/backend-lint.yml` (ajout √©tape vulture)
- ‚úÖ Cr√©√©: `backend/vulture.ini` (configuration compl√®te avec exclusions)
- ‚úÖ Cr√©√©: `backend/VULTURE_USAGE.md` (documentation utilisation)

**Configuration:**

- ‚úÖ Exclusions configur√©es : venv, migrations, tests, data/, etc.
- ‚úÖ Ignore decorators : Flask routes, Celery tasks, rate limiting
- ‚úÖ Whitelist : migrations Alembic, entry points, factories
- ‚úÖ Min confidence : 80 (seuil √©lev√© pour √©viter faux positifs)
- ‚úÖ CI : Int√©gr√© avec `continue-on-error: true` (alerte, pas blocage)

**Crit√®res d'acceptation:**

- ‚úÖ Vulture en CI (workflow backend-lint.yml)
- ‚úÖ Configuration compl√®te (vulture.ini avec exclusions adapt√©es)
- ‚úÖ Documentation compl√®te (VULTURE_USAGE.md)
- ‚ö†Ô∏è Code mort : √Ä traiter progressivement (d√©tection automatique via CI)

---

## üü¢ PHASE 4 : FAIBLE PRIORIT√â (Long terme) ‚Äî CONFORMIT√â & OPTIMISATION

### 4.1 Migrer secrets vers Vault

**Priorit√©:** üü¢ FAIBLE  
**Effort:** 10 jours  
**Owner:** DevOps  
**DDL:** J+120

**Actions:**

1. ‚úÖ Installer/configurer HashiCorp Vault
2. ‚úÖ Migrer secrets depuis .env
3. ‚úÖ Int√©grer dans application

**Fichiers cr√©√©s/modifi√©s:**

- ‚úÖ Cr√©√©: `vault/VAULT_MIGRATION_PLAN.md` (plan d√©taill√© 6 phases)
- ‚úÖ Cr√©√©: `vault/VAULT_SETUP.md` (guide installation)
- ‚úÖ Cr√©√©: `vault/VAULT_USAGE.md` (guide utilisation quotidienne)
- ‚úÖ Cr√©√©: `vault/docker-compose.vault.yml` (configuration Docker)
- ‚úÖ Cr√©√©: `vault/policies/atmr-api-read.hcl` (policy lecture)
- ‚úÖ Cr√©√©: `vault/policies/atmr-api-rotate.hcl` (policy rotation)
- ‚úÖ Cr√©√©: `vault/migrate-secrets.py` (script migration)
- ‚úÖ Cr√©√©: `backend/shared/vault_client.py` (client Python avec cache)
- ‚úÖ Modifi√©: `backend/config.py` (int√©gration Vault avec fallback .env)

**Fonctionnalit√©s:**

- ‚úÖ Client Vault Python avec cache (TTL 5min) et fallback .env
- ‚úÖ Support authentification Token (dev) et AppRole (prod)
- ‚úÖ Migration automatique depuis .env
- ‚úÖ Int√©gration transparente dans config.py (d√©tection auto)
- ‚úÖ Support secrets dynamiques Database (PostgreSQL)
- ‚úÖ Configuration multi-environnements (dev/staging/prod)
- ‚úÖ Documentation compl√®te (migration, setup, usage)

**Crit√®res d'acceptation:**

- ‚úÖ Infrastructure Vault configur√©e (Docker Compose)
- ‚úÖ Client Python fonctionnel avec fallback .env
- ‚úÖ Secrets migrables depuis .env
- ‚úÖ Application compatible Vault (d√©tection automatique)
- ‚úÖ Documentation compl√®te (3 guides)
- ‚ö†Ô∏è Rotation automatique : √Ä impl√©menter via Celery (voir Phase 5)
- ‚ö†Ô∏è Audit logs : √Ä configurer en production (voir Phase 6)

**Prochaines √©tapes (impl√©mentation manuelle):**

1. ‚úÖ Installer hvac : `hvac>=1.2.0` ajout√© √† `backend/requirements.txt`
2. ‚ö†Ô∏è Lancer Vault : `docker-compose -f docker-compose.yml -f vault/docker-compose.vault.yml up -d vault`
3. ‚ö†Ô∏è Migrer secrets : `python vault/migrate-secrets.py --env-file backend/.env --vault-token <token>`
4. ‚ö†Ô∏è Configurer VAULT_ADDR en production (voir `vault/VAULT_PRODUCTION_CONFIG.md`)
5. ‚úÖ Tasks Celery pour rotation automatique : `tasks/vault_rotation_tasks.py` cr√©√© et int√©gr√©
6. ‚ö†Ô∏è Configurer audit logs en production (voir `vault/VAULT_PRODUCTION_CONFIG.md` section "√âtape 6")

**Tasks Celery de rotation configur√©es:**

- ‚úÖ `vault-rotate-jwt` : Rotation JWT secret tous les 30 jours
- ‚úÖ `vault-rotate-encryption` : Rotation encryption key tous les 90 jours
- ‚úÖ `vault-rotate-all` : Rotation globale tous les 90 jours (backup)

---

### 4.2 API export donn√©es RGPD

**Priorit√©:** üü¢ FAIBLE  
**Effort:** 10 jours  
**Owner:** Backend Lead  
**DDL:** J+150

**Actions:**

1. Cr√©er endpoint `/api/user/data-export`
2. Exporter toutes donn√©es utilisateur (JSON)
3. Chiffrer export si n√©cessaire

**Fichiers √† cr√©er:**

- ‚úÖ Cr√©er: `backend/routes/user_data_export.py`

**Crit√®res d'acceptation:**

- ‚úÖ Export complet donn√©es utilisateur
- ‚úÖ Format JSON structur√©
- ‚úÖ Tests pr√©sents

---

### 4.3 Trier TODO/FIXME

**Priorit√©:** üü¢ FAIBLE  
**Effort:** 3 jours  
**Owner:** Backend Team  
**DDL:** J+30

**Actions:**

1. Lister tous TODO/FIXME
2. Cr√©er issues GitHub par TODO
3. Prioriser et assigner

**Commandes:**

```bash
grep -r "TODO\|FIXME" backend/ > artifacts/todos.txt
```

**Crit√®res d'acceptation:**

- ‚úÖ Tous TODO document√©s en issues
- ‚úÖ Priorit√©s d√©finies
- ‚úÖ Owners assign√©s

---

### 4.4 Compl√©ter runbook incidents

**Priorit√©:** üü¢ FAIBLE  
**Effort:** 10 jours  
**Owner:** SRE  
**DDL:** J+90

**Actions:**

1. Documenter tous sc√©narios incidents
2. Ajouter proc√©dures r√©cup√©ration
3. Tester runbooks

**Crit√®res d'acceptation:**

- ‚úÖ Runbook complet (50+ pages)
- ‚úÖ Tous sc√©narios couverts
- ‚úÖ Proc√©dures test√©es

---

## üìä R√âSUM√â PAR PRIORIT√â

### üî¥ Critique (J+1 √† J+14) ‚Äî 7 t√¢ches ‚Äî ~12 jours

1. ‚úÖ Latence p50/p95/p99 routes API (compl√©t√© 2025-01-27)
2. ‚úÖ `/ready` probe s√©par√©e (compl√©t√© 2025-01-27)
3. ‚úÖ Validation variables env (compl√©t√© 2025-01-27)
4. ‚úÖ Scans s√©curit√© CI (compl√©t√© 2025-01-27)
5. ‚úÖ Plan de backout (compl√©t√© 2025-01-27)
6. ‚úÖ Tests backups restaurations (compl√©t√© 2025-01-27)
7. ‚è≥ Workers Gunicorn

### üü† Haute (J+15 √† J+60) ‚Äî 11 t√¢ches ‚Äî ~60 jours

1. ‚úÖ SLO routes API critiques
2. ‚úÖ Monitorer cache hit-rate
3. ‚úÖ Uniformiser retries
4. ‚úÖ Validation entr√©es compl√®te
5. ‚úÖ Rotation secrets
6. ‚úÖ Jitter Celery Beat
7. ‚úÖ Profiler DB (N+1)
8. ‚úÖ Rate limiting par endpoint
9. ‚úÖ Traces OpenTelemetry compl√®tes
10. ‚úÖ M√©triques Prometheus toutes routes
11. ‚úÖ Alertes PagerDuty

### üü° Moyenne (J+61 √† J+180) ‚Äî 7 t√¢ches ‚Äî ~55 jours

1. ‚úÖ Couverture tests 70%
2. ‚úÖ Versioning API explicite
3. ‚úÖ R√©tention/purge RGPD
4. ‚úÖ Profilage CPU/m√©moire
5. ‚úÖ Reverse proxy
6. ‚úÖ Limits Docker
7. ‚úÖ Linter dead code

### üü¢ Faible (J+181+) ‚Äî 4 t√¢ches ‚Äî ~35 jours

1. ‚úÖ Migrer secrets Vault
2. ‚úÖ API export donn√©es RGPD
3. ‚úÖ Trier TODO/FIXME
4. ‚úÖ Compl√©ter runbook

---

## üéØ OBJECTIFS FINAUX

Apr√®s compl√©tion de toutes les phases:

- ‚úÖ **Observabilit√© compl√®te:** M√©triques p50/p95/p99 toutes routes, traces distribu√©es, dashboards Grafana
- ‚úÖ **S√©curit√© renforc√©e:** Scans SAST/DAST automatis√©s, rotation secrets, validation entr√©es compl√®te
- ‚úÖ **Performance optimale:** Cache hit-rate > 75%, 0 N+1, workers calibr√©s
- ‚úÖ **Fiabilit√© maximale:** SLO d√©clar√©s, alertes automatiques, plan de backout test√©
- ‚úÖ **Conformit√© RGPD:** R√©tention automatique, export donn√©es, audit logs complets

**Score cible:** 85/100 (vs 55/100 actuel)
